[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Christian L. Goueguel",
    "section": "",
    "text": "Dr. Christian L. Goueguel is a research scientist specializing in laser spectroscopy and chemometrics. His work spans industries such as environmental monitoring, agriculture, medical devices, and dairy production. He has authored 20+ peer-reviewed articles and presented his research at numerous international conferences.\nThroughout his career, Christian has consistently pushed the boundaries of analytical chemistry, delivering innovative, high-impact solutions that bridge research and practical applications.\nHe actively contributes to the academic and research community by serving as a referee for several scientific journals including J. Anal. At. Spectrom., ChemComm, Appl. Opt., Photonics Research, Opt. Lett., and Opt. Express. In this role, he evaluates and reviews submitted manuscripts, providing constructive feedback to ensure the quality, rigor, and integrity of the published research."
  },
  {
    "objectID": "bio.html",
    "href": "bio.html",
    "title": "Home",
    "section": "",
    "text": "Photo by Christian L. Goueguel.\n\n\n\n\nChristian earned his PhD in 2012 from the Institut National de la Recherche Scientifique (INRS)-University of Quebec, in Varennes, Canada under the supervision of Mohamed Chaker and François Vidal. Before pursuing his PhD in Canada, he completed both his Bachelor’s and Master’s degrees in Physics at the Grenoble Institute of Technology, France. His doctoral research, conducted at the National Research Council (NRC) in Boucherville, Canada, focused on improving the analytical performance of Laser-Induced Breakdown Spectroscopy (LIBS) for trace element analysis in metallic alloys.\nFollowing his PhD, Christian undertook a NETL-RAU postdoctoral role at Carnegie Mellon University (CMU) in Pittsburgh, Pennsylvania, and an ORISE postdoctoral fellowship at the U.S. Department of Energy’s National Energy Technology Laboratory (NETL). He conducted research on underwater LIBS for real-time environmental monitoring in carbon capture and storage (CCS). His efforts centered on developing innovative approaches to address analytical challenges associated with monitoring trace elements under extreme subsurface conditions.\nChristian has also applied his expertise in agricultural innovation as a research scientist in an agronomic services company. He developed LIBS-based systems for rapid, cost-effective soil and plant tissue analysis, optimizing measurement procedures, designing multivariate calibration models, and integrating machine learning algorithms to predict soil textural properties and nutrient content with high accuracy. Christian also worked as a senior optical engineer for a California-based tech company headquartered in Fremont, while contributing to operations at their Scottsdale office in Arizona. Additionally, he has worked as a chemometrics consultant, where he specialized in developing machine learning models for milk quality assessment using near-infrared spectroscopy (NIRS) data.\nChristian values quality time with his friends and family, finding joy in outdoor activities. A passionate amateur photographer, he also enjoys capturing the beauty of landscapes and cities through his lens, blending his love for optics with his artistic passion. Driven by his belief in learning through teaching, Christian launched his blog to share insights and ideas. Having written a few posts already, he looks forward to publishing many more in the future."
  },
  {
    "objectID": "bio.html#about",
    "href": "bio.html#about",
    "title": "Home",
    "section": "",
    "text": "Christian earned his PhD in 2012 from the Institut National de la Recherche Scientifique (INRS)-University of Quebec, in Varennes, Canada under the supervision of Mohamed Chaker and François Vidal. Before pursuing his PhD in Canada, he completed both his Bachelor’s and Master’s degrees in Physics at the Grenoble Institute of Technology, France. His doctoral research, conducted at the National Research Council (NRC) in Boucherville, Canada, focused on improving the analytical performance of Laser-Induced Breakdown Spectroscopy (LIBS) for trace element analysis in metallic alloys.\nFollowing his PhD, Christian undertook a NETL-RAU postdoctoral role at Carnegie Mellon University (CMU) in Pittsburgh, Pennsylvania, and an ORISE postdoctoral fellowship at the U.S. Department of Energy’s National Energy Technology Laboratory (NETL). He conducted research on underwater LIBS for real-time environmental monitoring in carbon capture and storage (CCS). His efforts centered on developing innovative approaches to address analytical challenges associated with monitoring trace elements under extreme subsurface conditions.\nChristian has also applied his expertise in agricultural innovation as a research scientist in an agronomic services company. He developed LIBS-based systems for rapid, cost-effective soil and plant tissue analysis, optimizing measurement procedures, designing multivariate calibration models, and integrating machine learning algorithms to predict soil textural properties and nutrient content with high accuracy. Christian also worked as a senior optical engineer for a California-based tech company headquartered in Fremont, while contributing to operations at their Scottsdale office in Arizona. Additionally, he has worked as a chemometrics consultant, where he specialized in developing machine learning models for milk quality assessment using near-infrared spectroscopy (NIRS) data.\nChristian values quality time with his friends and family, finding joy in outdoor activities. A passionate amateur photographer, he also enjoys capturing the beauty of landscapes and cities through his lens, blending his love for optics with his artistic passion. Driven by his belief in learning through teaching, Christian launched his blog to share insights and ideas. Having written a few posts already, he looks forward to publishing many more in the future."
  },
  {
    "objectID": "publications.html",
    "href": "publications.html",
    "title": "Christian L. Goueguel",
    "section": "",
    "text": "Photo by Sylvia Yang."
  },
  {
    "objectID": "publications.html#book-chapters",
    "href": "publications.html#book-chapters",
    "title": "Christian L. Goueguel",
    "section": "Book Chapters",
    "text": "Book Chapters\n\nC.R. Bhatt, C.L. Goueguel, J.C. Jain, D.L. McIntyre, J.P. Singh, LIBS application to liquid samples In: Laser-Induced Breakdown Spectroscopy (2nd Edition), Elsevier Science, 2020\nD.L. McIntyre, J.C. Jain, C.L. Goueguel, J.P. Singh, Application of Laser-Induced Breakdown Spectroscopy (LIBS) to Carbon Sequestration Research and Development In: Spectroscopic Techniques for Security, Forensic and Environmental Applications, Nova Science Publishers, 2014"
  },
  {
    "objectID": "publications.html#articles",
    "href": "publications.html#articles",
    "title": "Christian L. Goueguel",
    "section": "Articles",
    "text": "Articles\n\nC.L. Goueguel, A. Soumare, C. Nault, J. Nault, Direct determination of soil texture using laser-induced breakdown spectroscopy and multivariate linear regressions, Journal of Analytical Atomic Spectrometry 34, 2019\nC.L. Goueguel, C.R. Bhatt, J.C. Jain, C.L. Lopano, D.L. McIntyre, Quantification of dissolved metals in high-pressure CO2-water solutions by underwater laser-induced breakdown spectroscopy, Optics & Laser Technology 108, 2018\nC.R. Bhatt, J.C. Jain, C.L. Goueguel, D.L. McIntyre, J.P. Singh, Determination of rare earth elements in geological samples using laser-induced breakdown spectroscopy (LIBS), Applied spectroscopy 72, 2018\nC.R. Bhatt, J.C. Jain, C.L. Goueguel, D.L. McIntyre, J.P. Singh, Measurement of Eu and Yb in aqueous solutions by underwater laser-induced breakdown spectroscopy, Spectrochemical Acta Part B: Atomic Spectroscopy 137, 2017\nC.R. Bhatt, C.L. Goueguel, J.C. Jain, H.M. Edenborn, D.L. McIntyre, Analysis of charcoal blast furnace slags by laser-induced breakdown spectroscopy, Applied Optics 56, 2017\nJ.C. Jain, C.L. Goueguel, C.R. Bhatt, D.L. McIntyre, LIBS Sensor for Sub-surface CO2 Leak Detection in Carbon Sequestration, Sensors & Transducers Journal 214, 2017\nJ.C. Jain, D.L. McIntyre, C.L. Goueguel, Harsh environment low- cost LIBS sensor for sub-surface CO2 leak detection in carbon sequestration, Materials for Energy, Efficiency, and Sustainability: TechConnect Briefs 2017\nC.L. Goueguel, J.C. Jain, D.L. McIntyre, C.G. Carson, H.M. Edenborn, In situ measurements of calcium carbonate dissolution under rising CO2 pressure using underwater laser-induced breakdown spectroscopy, Journal of Analytical Atomic Spectrometry 31, 2016\nC.L. Goueguel, D.L. McIntyre, J.C. Jain, Matrix effect of sodium compounds on the determination of metal ions in aqueous solutions by underwater laser-induced breakdown spectroscopy, Optics letters 41, 2016\nC.G. Carson, C.L. Goueguel, H. Sanghapi, J.C. Jain, D.L. McIntyre, Evaluation of a commercially available passively Q-switched Nd: YAG laser with LiF: F2- saturable absorber for laser-induced breakdown spectroscopy, Optics & Laser Technology 79, 2016\nC.L. Goueguel, D.L. McIntyre, J.C. Jain, A.K. Karamalidis, C.G. Carson, Matrix effect of sodium compounds on the determination of metal ions in aqueous solutions by underwater laser-induced breakdown spectroscopy, Applied optics 54, 2015\nC.L. Goueguel, D.L. McIntyre, J.C. Jain, A.K. Karamalidis, Laser-Induced Breakdown Spectroscopy (LIBS) of a High-Pressure CO2–Water Mixture: Application to Carbon Sequestration, Applied spectroscopy 68, 2014\nJ.C. Jain, D.L. McIntyre, K. Ayyalasomayajula, V. Dikshit, C.L. Goueguel, F. Yu-Yueh, J.P. Singh, Application of laser-induced breakdown spectroscopy in carbon sequestration research and development, Pramana 83, 2014\nC.L. Goueguel, J.P. Singh, D.L. McIntyre, J.C. Jain, A.K. Karamalidis, Effect of sodium chloride concentration on elemental analysis of brines by laser-induced breakdown spectroscopy (LIBS), Applied spectroscopy 68, 2014\nC.L. Goueguel, S. Laville, F. Vidal, M. Chaker, M. Sabsabi, Resonant laser-induced breakdown spectroscopy for analysis of lead traces in copper alloys, Journal of Analytical Atomic Spectrometry 26, 2011\nF. Vidal, S. Laville, C.L. Goueguel, H. Loudyi, K. Rifai, M. Chaker, M. Sabsabi, A simple model of laser-induced fluorescence under arbitrary optical thickness conditions at the excitation wavelength, Journal of Quantitative Spectroscopy and Radiative Transfer 111, 2010\nC.L. Goueguel, S. Laville, F. Vidal, M. Sabsabi, M. Chaker, Investigation of resonance-enhanced laser-induced breakdown spectroscopy for analysis of aluminum alloys, Journal of Analytical Atomic Spectrometry 25, 2010\nS. Laville, C.L. Goueguel, H. Loudyi, F. Vidal, M. Chaker, M. Sabsabi, Laser-induced fluorescence detection of lead atoms in a laser-induced plasma: An experimental analytical optimization study, Spectrochimica Acta Part B: Atomic Spectroscopy 64, 2009"
  },
  {
    "objectID": "publications.html#conference-proceedings",
    "href": "publications.html#conference-proceedings",
    "title": "Christian L. Goueguel",
    "section": "Conference Proceedings",
    "text": "Conference Proceedings\n\nJ.C. Jain, H.M. Edenborn, C.L. Goueguel, C.R. Bhatt, D. Hartzler, D.L. McIntyre, A Novel Approach for Tracking CO2 Leakage Into Groundwater Using Carbonate Mineral Dissolution, Geological Society of America, 50, 2018\nJ.C. Jain, C.L. Goueguel, D.L. McIntyre, Development of LIBS Sensor for Sub-Surface CO2 Leak Detection in Carbon Sequestration, 2017 AIChE Annual Meeting\nJ.C. Jain, H.M. Edenborn, C.L. Goueguel, C.R. Bhatt, D.L. McIntyre, A Rapid Method for the Chemical Analysis of Charcoal Iron Furnace Slags, Geological Society of America, 49, 2017\nC.G. Carson, C.L. Goueguel, J.C. Jain, D.L. McIntyre, Development of laser-induced breakdown spectroscopy sensor to assess groundwater quality impacts resulting from geologic carbon sequestration, Micro-and Nanotechnology Sensors, Systems, and Applications VII 9467, 2015\nF. Vidal, M. Chaker, C.L. Goueguel, S. Laville, H. Loudyi, K. Rifai, M. Sabsabi, Enhanced Laser‐Induced Breakdown Spectroscopy by Second‐Pulse Selective Wavelength Excitation, AIP Conference Proceedings 1047, 2008\nC.L. Goueguel, S. Laville, H. Loudyi, M. Chaker, M. Sabsabi, F. Vidal, Detection of lead in brass by laser-induced breakdown spectroscopy combined with laser-induced fluorescence, Photonics North 2008, 709927, 2008"
  },
  {
    "objectID": "blog/posts/post3/index.html",
    "href": "blog/posts/post3/index.html",
    "title": "Chemometric Modeling with Tidymodels: A Tutorial for Spectroscopic Data",
    "section": "",
    "text": "Photo by Robert Lukeman.\n\n\n\n\nFor this tutorial, we use the beer dataset, publicly available and commonly used for spectroscopy-based regression problems. This dataset contains near-infrared spectroscopy (NIRS) data of beer samples alongside measurements of the original gravity (alcoholic beverage), which serves as the target variable. The tutorial emphasizes practical implementation and clear visualizations to empower users in deploying advanced regression models for spectroscopy datasets.\n\nSetup\nBelow, we use suppressPackageStartupMessages to suppress startup messages for clarity and load essential packages.\n\nssh = suppressPackageStartupMessages\n\n\ntidyverse for data manipulation and visualization.\ntidymodels for modeling workflows and machine learning.\ntidymodels_prefer() ensures consistency across conflicting tidymodels functions.\n\n\nssh(library(tidyverse))\nssh(library(tidymodels))\ntidymodels_prefer()\n\nAdditional libraries include:\n\nkknn: Implements k-nearest neighbors (KNN).\nglmnet: Used for elastic net regression.\nranger: Used for random forest.\nplsmod: Supports partial least squares (PLS) regression.\nmagrittr: Provides pipe operators (%&gt;%, %&lt;&gt;%).\npatchwork: Simplifies combining ggplot2 plots.\n\n\nlibrary(kknn)\nlibrary(plsmod)\nssh(library(glmnet))\nssh(library(ranger))\n\n\nssh(library(magrittr))\nlibrary(patchwork)\n\nWe set a custom theme with a clean white background and adjusted sizes for all ggplot2 plots.\n\nbase_size = 15 \nggplot2::theme_bw(\n  base_size = base_size,\n  base_line_size = base_size / 22,\n  base_rect_size = base_size / 15\n  ) %&gt;% \n  theme_set()\n\n\n\nDataset Overview\nWe begin by loading the beer dataset and identifying the spectral predictor columns, which correspond to the NIRS wavelength variables. Usually, I prefer storing spectral wavelengths as character strings in a variable named wavelength because it makes data manipulation easier. This approach enhances flexibility when selecting, filtering, or grouping columns, simplifies integration with tidyverse functions, and ensures compatibility with tidymodels preprocessing workflows.\n\nbeer_data &lt;- read_csv(\"beer.csv\", show_col_types = FALSE)\nwavelength &lt;- beer_data %&gt;% select(starts_with(\"xtrain\")) %&gt;% names()\n\nPreviewing the first rows of the dataset helps us ensure its integrity and understand its structure.\n\nbeer_data %&gt;% head()\n\n# A tibble: 6 × 577\n  originalGravity xtrain.1 xtrain.2 xtrain.3 xtrain.4 xtrain.5 xtrain.6 xtrain.7\n            &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n1            4.23    0.245    0.252    0.258    0.265    0.272    0.280    0.288\n2            6.02    0.243    0.249    0.256    0.262    0.269    0.277    0.285\n3            6.49    0.242    0.248    0.254    0.261    0.268    0.276    0.283\n4            8.92    0.240    0.246    0.252    0.259    0.266    0.273    0.281\n5            8.98    0.241    0.247    0.254    0.260    0.267    0.275    0.282\n6           10.2     0.240    0.246    0.253    0.259    0.266    0.274    0.281\n# ℹ 569 more variables: xtrain.8 &lt;dbl&gt;, xtrain.9 &lt;dbl&gt;, xtrain.10 &lt;dbl&gt;,\n#   xtrain.11 &lt;dbl&gt;, xtrain.12 &lt;dbl&gt;, xtrain.13 &lt;dbl&gt;, xtrain.14 &lt;dbl&gt;,\n#   xtrain.15 &lt;dbl&gt;, xtrain.16 &lt;dbl&gt;, xtrain.17 &lt;dbl&gt;, xtrain.18 &lt;dbl&gt;,\n#   xtrain.19 &lt;dbl&gt;, xtrain.20 &lt;dbl&gt;, xtrain.21 &lt;dbl&gt;, xtrain.22 &lt;dbl&gt;,\n#   xtrain.23 &lt;dbl&gt;, xtrain.24 &lt;dbl&gt;, xtrain.25 &lt;dbl&gt;, xtrain.26 &lt;dbl&gt;,\n#   xtrain.27 &lt;dbl&gt;, xtrain.28 &lt;dbl&gt;, xtrain.29 &lt;dbl&gt;, xtrain.30 &lt;dbl&gt;,\n#   xtrain.31 &lt;dbl&gt;, xtrain.32 &lt;dbl&gt;, xtrain.33 &lt;dbl&gt;, xtrain.34 &lt;dbl&gt;, …\n\n\n\np &lt;- beer_data %&gt;% mutate(spectra_id = paste0(\"s\", 1:80)) %&gt;%\n  pivot_longer(\n  cols = -c(originalGravity, spectra_id),\n  names_to = \"wavelength\",\n  values_to = \"intensity\"\n  ) %&gt;%\n  mutate(wavelength = rep(seq(1100, 2250, 2), times = 80)) %&gt;%\n  mutate(wavelength = as.numeric(wavelength)) %&gt;%\n  ggplot() +\n  aes(x = wavelength, y = intensity, colour = originalGravity, group = spectra_id) +\n  geom_line() +\n  scale_color_viridis_c(option = \"inferno\", direction = 1) +\n  labs(\n    x = \"Wavelength [nm]\", \n    y = \"Absorbance [arb. units]\", \n    title = \"NIRS Spectra of Beer Samples\", \n    subtitle = \"Contains 80 samples, measured from 1100 to 2250 nm\", \n    color = \"Original Gravity\") +\n  theme_minimal()\n\nplotly::ggplotly(p)\n\n\n\n\n\n\n\nSupervised Learning Techniques\nFor this analysis, we’ll evaluate and compare the performance of four supervised learning algorithms, categorized by their linearity or modeling approach (parametric vs. non-parametric):\n\n\n\n\n\n\n\n\nAlgorithm\nAcronym\nApproach\n\n\n\n\nsparse Partial Least Squares\nsPLS\nLinear\n\n\nElastic Net\nENet\nLinear\n\n\nk-Nearest Neighbors\nKNN\nNon-linear\n\n\nRandom Forests\nRF\nNon-linear\n\n\n\n\n\nStep 1: Data Splitting\nTo ensure unbiased model evaluation, we partition the data into training (80%) and testing (20%) sets, employing stratified sampling based on the target variable’s distribution.\n\nset.seed(123)\nsplit_data &lt;- initial_split(beer_data, prop = 0.8, strata = originalGravity)\ntrain_data &lt;- training(split_data)\ntest_data &lt;- testing(split_data)\n\n\n\nStep 2: Cross-Validation\nWe use a 5-fold repeated cross-validation strategy for hyperparameters tuning and performance evaluation, minimizing the risk of overfitting.\n\ncv_folds &lt;- vfold_cv(train_data, v = 5, repeats = 3)\n\nInterestingly, the vfold_cv function provides a powerful way to visualize the distribution of data across folds, allowing us to confirm that the stratification and splits are evenly balanced. This ensures that each fold accurately represents the overall dataset, enhancing the reliability of cross-validation results.\n\ncv_folds %&gt;%\n  tidy() %&gt;%\n  ggplot(aes(x = Fold, y = Row, fill = Data)) +\n  geom_tile() + \n  facet_wrap(~Repeat) + \n  scale_fill_brewer(palette = \"Paired\")\n\n\n\n\n\n\n\n\n\n\nStep 3: Preprocessing\nPreprocessing spectral data is a vast and intricate topic, deserving its own dedicated discussion, which we will explore in a future post. For this tutorial, we remove zero-variance predictors, and center the spectra intensity to ensure the data is well-suited for modeling.\ntidymodels provides a wide array of preprocessing steps through its versatile step_* functions. These functions allow for comprehensive data transformations, including centering, scaling, feature selection, and more, to be seamlessly integrated into the modeling workflow. Additionally, tidymodels offers the flexibility to create custom recipe steps, enabling you to design and implement tailored data transformations that meet your specific needs.\n\nbase_recipe &lt;- recipe(originalGravity ~ ., data = train_data) %&gt;%\n  update_role(originalGravity, new_role = \"outcome\") %&gt;%\n  update_role(all_of(wavelength), new_role = \"predictor\") %&gt;%\n  step_zv(all_predictors()) %&gt;%\n  step_center(all_predictors())\n\nFor comparison with the base preprocessing step, we introduce an additional step that applies Principal Component Analysis (PCA) to the predictor variables. This transformation reduces the dimensionality of the data while retaining the most significant variance.\n\npca_recipe &lt;- base_recipe %&gt;%\n  step_pca(all_predictors())\n\n\n\nStep 4: Model Specifications\nWe now define model specifications for each algorithm, incorporating hyperparameter tuning within the tidymodels framework. Notably, the tune function is used to specify hyperparameters that require optimization during the tuning process. For parameters with predefined values, these can be directly assigned within their allowable range.\n\n# k-Nearest Neighbors (KNN)\nknn_spec &lt;- nearest_neighbor() %&gt;%\n  set_args(neighbors = tune(), weight_func = tune(), dist_power = tune()) %&gt;%\n  set_engine('kknn') %&gt;%\n  set_mode('regression')\n\n# Partial Least Squares Regression (PLSR)\nspls_spec &lt;- pls() %&gt;%\n  set_args(predictor_prop = tune(), num_comp = tune()) %&gt;%\n  set_engine('mixOmics') %&gt;%\n  set_mode('regression')\n\n# Elastic Net (ENet)\nenet_spec &lt;-linear_reg() %&gt;%\n  set_args(penalty = tune(), mixture = tune()) %&gt;%\n  set_engine(\"glmnet\") %&gt;%\n  set_mode(\"regression\")\n\n# Random Forest (RF)\nrf_spec &lt;- rand_forest() %&gt;%\n  set_args(mtry = tune(), trees = tune(), min_n = tune()) %&gt;%\n  set_engine(\"ranger\") %&gt;% \n  set_mode(\"regression\")\n\n\n\nStep 5: Model Training and Tuning\nUsing the workflow_set function, we define a workflow for each model and tune hyperparameters using a grid search, and enable parallel processing using 4 CPU cores, accelerating computation during tuning.\n\nwflowSet &lt;- workflow_set(\n  preproc = list(base = base_recipe, pca = pca_recipe), \n  models = list(\n    knn = knn_spec, \n    spls = spls_spec, \n    enet = enet_spec, \n    rf = rf_spec), \n  cross = TRUE\n  )\n\nAs mentionned earlier, we opted to train each model using both the base preprocessing approach and the PCA-transformed data. This strategy results in a total of eight unique training models as follows:\n\nwflowSet \n\n# A workflow set/tibble: 8 × 4\n  wflow_id  info             option    result    \n  &lt;chr&gt;     &lt;list&gt;           &lt;list&gt;    &lt;list&gt;    \n1 base_knn  &lt;tibble [1 × 4]&gt; &lt;opts[0]&gt; &lt;list [0]&gt;\n2 base_spls &lt;tibble [1 × 4]&gt; &lt;opts[0]&gt; &lt;list [0]&gt;\n3 base_enet &lt;tibble [1 × 4]&gt; &lt;opts[0]&gt; &lt;list [0]&gt;\n4 base_rf   &lt;tibble [1 × 4]&gt; &lt;opts[0]&gt; &lt;list [0]&gt;\n5 pca_knn   &lt;tibble [1 × 4]&gt; &lt;opts[0]&gt; &lt;list [0]&gt;\n6 pca_spls  &lt;tibble [1 × 4]&gt; &lt;opts[0]&gt; &lt;list [0]&gt;\n7 pca_enet  &lt;tibble [1 × 4]&gt; &lt;opts[0]&gt; &lt;list [0]&gt;\n8 pca_rf    &lt;tibble [1 × 4]&gt; &lt;opts[0]&gt; &lt;list [0]&gt;\n\n\n\nctrl_grid &lt;- control_grid(\n  verbose = TRUE,\n  allow_par = TRUE,\n  extract = NULL,\n  save_pred = TRUE,\n  pkgs = c(\"doParallel\", \"doFuture\"),\n  save_workflow = FALSE,\n  event_level = \"first\",\n  parallel_over = \"resamples\"\n  )\n\n\ncl &lt;- parallel::makePSOCKcluster(4)\ndoParallel::registerDoParallel(cl)\n\n\nwflowSet %&lt;&gt;%\n  workflow_map(\n    fn = \"tune_grid\",\n    resamples = cv_folds,\n    grid = 10,\n    metrics = metric_set(rmse, mae, rsq),\n    control = ctrl_grid,\n    seed = 3L,\n    verbose = TRUE\n  )\n\n\nparallel::stopCluster(cl)\n\nNext, we utilize the rank_results function to systematically rank the training models based on key performance metrics such as the root mean squared error (RMSE), the mean absolute error (MAE), and the coefficient of determination \\((R^2)\\). Following this, we visualize the model performance with error bars, providing a clear and insightful comparison of their predictive capabilities.\n\nwflowSet %&gt;%\n  rank_results(rank_metric = \"rmse\") %&gt;%\n  relocate(c(rank, mean, std_err, model), .before = wflow_id)\n\n# A tibble: 240 × 9\n    rank  mean std_err model      wflow_id  .config   .metric     n preprocessor\n   &lt;int&gt; &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;     &lt;chr&gt;     &lt;chr&gt;   &lt;int&gt; &lt;chr&gt;       \n 1     1 0.146 0.00634 pls        base_spls Preproce… mae        15 recipe      \n 2     1 0.186 0.0106  pls        base_spls Preproce… rmse       15 recipe      \n 3     1 0.994 0.00102 pls        base_spls Preproce… rsq        15 recipe      \n 4     2 0.160 0.0139  linear_reg base_enet Preproce… mae        15 recipe      \n 5     2 0.226 0.0250  linear_reg base_enet Preproce… rmse       15 recipe      \n 6     2 0.990 0.00307 linear_reg base_enet Preproce… rsq        15 recipe      \n 7     3 0.160 0.0139  linear_reg base_enet Preproce… mae        15 recipe      \n 8     3 0.226 0.0249  linear_reg base_enet Preproce… rmse       15 recipe      \n 9     3 0.990 0.00305 linear_reg base_enet Preproce… rsq        15 recipe      \n10     4 0.160 0.0140  linear_reg base_enet Preproce… mae        15 recipe      \n# ℹ 230 more rows\n\n\n\nwflowSet %&gt;% autoplot(std_errs = qnorm(0.95), type = \"wflow_id\") +\n  ggsci::scale_color_lancet() +\n  theme(legend.position = \"bottom\") +\n  ylab(\"\")\n\n\n\n\n\n\n\n\n\n\nStep 6: Model Selection\nBy configuring the autoplot function with the argument select_best = TRUE, we rank the training models while visually emphasizing the best-performing model, making it easy to identify the optimal choice for further evaluation.\n\nwflowSet %&gt;% autoplot(select_best = TRUE, std_errs = qnorm(0.95), type = \"wflow_id\") +\n  geom_point(size = 3) +\n  ggsci::scale_color_lancet() +\n  theme(legend.position = \"bottom\") +\n  ylab(\"\")\n\n\n\n\n\n\n\n\nThe extract_workflow_set_result function retrieves the optimized hyperparameter values for the best-performing model, as determined by the lowest RMSE. In this analysis, it determines the optimal settings for base_spls, specifically identifying the optimized number of latent variables (num_comp) and the proportion of predictors (predictor_prop) allowed to have non-zero coefficients.\n\nbest_model &lt;- wflowSet %&gt;% \n  extract_workflow_set_result(\"base_spls\") %&gt;% \n  select_best(metric = \"rmse\") %&gt;%\n  print()\n\n# A tibble: 1 × 3\n  predictor_prop num_comp .config              \n           &lt;dbl&gt;    &lt;int&gt; &lt;chr&gt;                \n1         0.0633        3 Preprocessor1_Model01\n\n\nWe use the collect_predictions function to gather the best-performing model’s training data and visualize the relationship between the actual and predicted values. This allows us to assess the model’s predictive accuracy. Additionally, we perform a residual analysis using standardized residuals to further evaluate the model’s performance and identify any potential areas for improvement.\n\ntrain_results &lt;- wflowSet %&gt;% \n  collect_predictions() %&gt;% \n  filter(wflow_id == \"base_spls\" & .config == best_model %&gt;% pull(.config)) %&gt;%\n  select(-.row) %&gt;%\n  print()\n\n# A tibble: 62 × 6\n   wflow_id  .config               preproc model .pred originalGravity\n   &lt;chr&gt;     &lt;chr&gt;                 &lt;chr&gt;   &lt;chr&gt; &lt;dbl&gt;           &lt;dbl&gt;\n 1 base_spls Preprocessor1_Model01 recipe  pls    3.74            4.23\n 2 base_spls Preprocessor1_Model01 recipe  pls    6.39            6.02\n 3 base_spls Preprocessor1_Model01 recipe  pls    6.83            6.49\n 4 base_spls Preprocessor1_Model01 recipe  pls    9.11            8.92\n 5 base_spls Preprocessor1_Model01 recipe  pls    9.29            8.98\n 6 base_spls Preprocessor1_Model01 recipe  pls   10.3            10.2 \n 7 base_spls Preprocessor1_Model01 recipe  pls   10.3            10.4 \n 8 base_spls Preprocessor1_Model01 recipe  pls   10.6            10.4 \n 9 base_spls Preprocessor1_Model01 recipe  pls   10.4            10.5 \n10 base_spls Preprocessor1_Model01 recipe  pls   10.2            10.5 \n# ℹ 52 more rows\n\n\n\np1 &lt;- train_results %&gt;% \n  ggplot() +\n  aes(x = originalGravity, y = .pred) +\n  geom_point(alpha = .5) +\n  geom_abline(color = \"gray50\", lty = 2) +\n  coord_obs_pred() +\n  labs(x = \"Actual Original Gravity\", y = \"Predicted Original Gravity\")\n\n\np2 &lt;- train_results %&gt;% \n  ggplot() +\n  aes(x = .pred, y = (originalGravity - .pred)/sd((originalGravity - .pred))) +\n  geom_hline(yintercept = 0, color = \"gray50\", lty = 2) + \n  geom_point(alpha = .5) +\n  geom_smooth(method = \"loess\", se = FALSE, color = \"red\", lty = 1, linewidth = 0.5) +\n  labs(x = \"Predicted Original Gravity\", y = \"Standardized Residuals\")\n\n\np1 | p2\n\n\n\n\n\n\n\n\n\n\nStep 7: Model Testing\nFinally, we finalize the best-performing model (base_spls) by utilizing the extract_workflow and finalize_workflow functions. We then assess the model’s performance on the test set using last_fit(split = split_data), calculating key metrics to evaluate its accuracy. These metrics are retrieved using the collect_metrics() function.\nAs done previously, we visualize the results through actual vs. predicted plots, complemented by residual diagnostics, to provide a comprehensive evaluation of the model’s performance.\n\ntest_results &lt;- wflowSet %&gt;% \n  extract_workflow(\"base_spls\") %&gt;% \n  finalize_workflow(best_model) %&gt;% \n  last_fit(split = split_data)\n\n\ntest_results %&gt;% collect_metrics()\n\n# A tibble: 2 × 4\n  .metric .estimator .estimate .config             \n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 rmse    standard       0.102 Preprocessor1_Model1\n2 rsq     standard       0.999 Preprocessor1_Model1\n\n\n\np3 &lt;- test_results %&gt;% \n  collect_predictions() %&gt;% \n  ggplot() +\n  aes(x = originalGravity, y = .pred) +\n  geom_abline(color = \"gray50\", lty = 2) + \n  geom_point(alpha = .5) + \n  coord_obs_pred() +\n  labs(x = \"Actual Original Gravity\", y = \"Predicted Original Gravity\")\n\n\np4 &lt;- test_results %&gt;% \n  collect_predictions() %&gt;% \n  ggplot() +\n  aes(x = .pred, y = (originalGravity - .pred)/sd((originalGravity - .pred))) +\n  geom_hline(yintercept = 0, color = \"gray50\", lty = 2) + \n  geom_point(alpha = .5) +\n  geom_smooth(method = \"loess\", se = FALSE, color = \"red\", lty = 1, linewidth = 0.5) +\n  labs(x = \"Predicted Original Gravity\", y = \"Standardized Residuals\")\n\n\np3 | p4\n\n\n\n\n\n\n\n\n\n\nConclusion\nThis tutorial showcased the versatility of the Tidymodels framework for chemometric applications. By leveraging its modular and tidy design, you can implement robust spectroscopic models tailored to your dataset, ensuring both accuracy and reproducibility."
  },
  {
    "objectID": "blog/posts/post1/index.html",
    "href": "blog/posts/post1/index.html",
    "title": "An Overview of Orthogonal Partial Least Squares",
    "section": "",
    "text": "Photo by Simon Berger.\n\n\n\n\nFirst and foremost, let me briefly recall that Partial Least Squares (PLS) regression is, without doubt,one of the most, or maybe the most, multivariate regression methods commonly used in chemometrics. In fact, PLS was originally developed around 1975 by Herman Wold for use in the field of econometrics and was later embraced in the 1980’s by prominent chemometricians such as Svante Wold — Herman Wold’s son, who in 1971 coined the term chemometrics to describe this emerging field of using advanced statistical and machine learning methods in analytical chemistry applications — , Harald Martens, Bruce Kowalski, Tormod Naes, and Paul Geladi, just to name a few. Over the past few decades, PLS — whether as a regression or classification method — has become very popular among chemometrics practitioners for dealing with multicollinearity in applications such as multivariate calibration and process analytical technology, where there are usually fewer observations (samples) than variables (wavelengths).\nIn 2002, Johan Trygg and Svante Wold introduced a variant of PLS in a paper entitled “Orthogonal projections to latent structures (O‐PLS)”. The new approach is a supervised multivariate data projection method used to relate a set of predictor variables \\((X)\\) to one or more responses \\((Y)\\).Basically, like PLS, O-PLS attempts to extract the maximum information reflecting the variation in the data set, while assuming the existence of a small subset of hidden variables in the \\(X\\)-data to predict the response variables. These subsets are formally called latent variables (or LVs) because they are unmeasurable. Historically, this concept of hidden structure in a data set is a century old and comes from methods such as Principal Component Analysis (PCA). The O-PLS method, as named by the authors, uses orthogonal signal correction previously developed by S. Wold — Chemometrics Intell. Lab. Syst., 1998, 44, 175 — to maximize the explained covariance on the first LV, while the remaining LVs capture variance in the predictors which is orthogonal, i.e. statistically uncorrelated to the response variables. To put it simply, this means that unlike PLS — which handle random noise fairly well — , the new method also known as Orthogonal Partial Least-Squares (OPLS) enables to filter out the structured noise in the data set by modeling separately variations of the \\(X\\)-predictors correlated and uncorrelated to the \\(Y\\)-responses. Ultimately, this reduces the model’s complexity by lowing the number of LVs, in addition to allowing identification, analysis and investigation of the main source of orthogonal variation.\n\n\n\nLet’s take a quick look at the mathematics behind OPLS. Herein we consider \\(X\\) as a matrix of size \\(n \\times p\\) and \\(Y\\) as a matrix of size \\(n\\times m\\), where \\(p\\)isthe number of predictor variables, \\(m\\) isthe number of response variables and, \\(n\\) is the number of observations. Recall that PLS has been developed with the aim of searching the direction of a certain number of LV — with the constraint of being orthogonal to each other — that meet the following criteria: (1) capture maximum variance in the \\(X\\)-space, (2) capture maximum variance in the \\(Y\\)-space and, (3) maximize correlation between the \\(X\\)- and \\(Y\\)-space.\nA PLS model can be written as:\n\\[\n\\begin{align*}\nX =& TP^T + E \\\\\nY =& UC^T + F \\\\\nT =& U + H\n\\end{align*}\n\\]\nwhere \\(T\\) is the scores matrix summarizing variation in the \\(X\\)-predictors — i.e. similarities/differences between samples — , \\(P\\) is the loadings matrix — or often called the \\(X\\)‐loadings to distinguish it from the \\(Y\\)‐loadings — , \\(U\\) is the scores matrix that summarizes variation in the \\(Y\\)-responses, \\(C\\) is the \\(Y\\)‐loadings and, \\(E\\),\\(F\\),\\(H\\)arethe residual matrices. The superscript \\(\"T\"\\) denotes the matrix transpose. Note that \\(P\\) expresses the correlation between \\(X\\) and \\(U\\), whilst \\(C\\) expresses the correlation between \\(Y\\) and \\(T\\).\n\n\n\nFIG. 1: Geometrical illustration of a PLS model. Data (black dots) on the \\(X\\)-space are projected (orthogonal projection) onto the subspace defined by the first two latent variables (LVs).\n\n\nOPLS model operates similarly to PLS model but separates the systematic variation in \\(X\\)into three parts: (1) a predictive part that is correlated to \\(Y\\), (2) an orthogonal part, “orth”, that is uncorrelated to \\(Y\\), (3) a noise part — residual variation.\nThus, an OPLS model can be formulated as:\n\\[\n\\begin{align*}\nX =& TP^T + T_{\\text{orth}}P^T_{\\text{orth}} + E \\\\\nY =& UC^T + F\n\\end{align*}\n\\]\n\n\n\nFIG. 2: Geometrical illustration of the difference between PLS and OPLS models. Orthogonal variation (white dots) are filtered out by the second latent variable.\n\n\nIt should be mentioned that in fact two different OPLS algorithms have been developed. The first algorithm, most frequently referred to as O1-PLS or simply OPLS, as described above is unidirectional \\((X \\implies Y)\\), meaning that only orthogonal variations in the \\(X\\)-space are filtered out. The second algorithm, referred to as O2-PLS, is bi-directional \\((X \\iff Y)\\), meaning that orthogonal variations in both the \\(X\\)- and \\(Y\\)-space are filtered out.\nAn O2-PLS model can be written as:\n\\[\n\\begin{align*}\nX =& TP^T + T_{\\text{Y-orth}}P^T_{\\text{Y-orth}} + E \\\\\nY =& UC^T + U_{\\text{X-orth}}C^T_{\\text{X-orth}} + F\n\\end{align*}\n\\]\nO2-PLS separates \\(X\\) and \\(Y\\) into three parts: (1) a joint part (correlation between \\(X\\) and \\(Y\\)), (2) an orthogonal part — unrelated latent variation in \\(X\\) and \\(Y\\) separately— ,and (3) a noise part.\n\n\n\nFor comparison purposes, I used the LIBS spectra of plant materials to predict the concentration of potassium using the PLS and OPLS approaches. PLS was performed using the R package caret. Since OPLS is not available in caret, I used the package ropls instead.\n\n\n\nFIG. 3: LIBS spectra of five plant samples. A baseline offset has been applied for easier viewing of the spectra.\n\n\nFigures below show the \\(X\\)-scores scatter plots (projection of the \\(X\\)-space) for PLS (left panel) and OPLS (right panel) modeling. In both figures, the \\(x\\)-axis represents the first component (or latent variable), whereas the \\(y\\)-axis represents the second component for PLS and the first orthogonal component for OPLS. The rainbow colors represent the concentration of potassium (K) in each sample, from 1% (dark blue) to 7% (dark red). As we can see, OPLS only requires one component to correlate variation in the samples with K concentration (see the black arrow).\n\n\n\nFIG. 4: Comparison between the \\(X\\)-scores scatter plots for the PLS and OPLS models.\n\n\n\n\n\nSo in summary, the most important thing to remember is that OPLS (or O2-PLS) offers a valuable preprocessing tool, and will help to generate more efficient predictive models, especially in situations where structured noise dominates."
  },
  {
    "objectID": "blog/posts/post1/index.html#what-is-opls",
    "href": "blog/posts/post1/index.html#what-is-opls",
    "title": "An Overview of Orthogonal Partial Least Squares",
    "section": "",
    "text": "First and foremost, let me briefly recall that Partial Least Squares (PLS) regression is, without doubt,one of the most, or maybe the most, multivariate regression methods commonly used in chemometrics. In fact, PLS was originally developed around 1975 by Herman Wold for use in the field of econometrics and was later embraced in the 1980’s by prominent chemometricians such as Svante Wold — Herman Wold’s son, who in 1971 coined the term chemometrics to describe this emerging field of using advanced statistical and machine learning methods in analytical chemistry applications — , Harald Martens, Bruce Kowalski, Tormod Naes, and Paul Geladi, just to name a few. Over the past few decades, PLS — whether as a regression or classification method — has become very popular among chemometrics practitioners for dealing with multicollinearity in applications such as multivariate calibration and process analytical technology, where there are usually fewer observations (samples) than variables (wavelengths).\nIn 2002, Johan Trygg and Svante Wold introduced a variant of PLS in a paper entitled “Orthogonal projections to latent structures (O‐PLS)”. The new approach is a supervised multivariate data projection method used to relate a set of predictor variables \\((X)\\) to one or more responses \\((Y)\\).Basically, like PLS, O-PLS attempts to extract the maximum information reflecting the variation in the data set, while assuming the existence of a small subset of hidden variables in the \\(X\\)-data to predict the response variables. These subsets are formally called latent variables (or LVs) because they are unmeasurable. Historically, this concept of hidden structure in a data set is a century old and comes from methods such as Principal Component Analysis (PCA). The O-PLS method, as named by the authors, uses orthogonal signal correction previously developed by S. Wold — Chemometrics Intell. Lab. Syst., 1998, 44, 175 — to maximize the explained covariance on the first LV, while the remaining LVs capture variance in the predictors which is orthogonal, i.e. statistically uncorrelated to the response variables. To put it simply, this means that unlike PLS — which handle random noise fairly well — , the new method also known as Orthogonal Partial Least-Squares (OPLS) enables to filter out the structured noise in the data set by modeling separately variations of the \\(X\\)-predictors correlated and uncorrelated to the \\(Y\\)-responses. Ultimately, this reduces the model’s complexity by lowing the number of LVs, in addition to allowing identification, analysis and investigation of the main source of orthogonal variation."
  },
  {
    "objectID": "blog/posts/post1/index.html#mathematical-framework",
    "href": "blog/posts/post1/index.html#mathematical-framework",
    "title": "An Overview of Orthogonal Partial Least Squares",
    "section": "",
    "text": "Let’s take a quick look at the mathematics behind OPLS. Herein we consider \\(X\\) as a matrix of size \\(n \\times p\\) and \\(Y\\) as a matrix of size \\(n\\times m\\), where \\(p\\)isthe number of predictor variables, \\(m\\) isthe number of response variables and, \\(n\\) is the number of observations. Recall that PLS has been developed with the aim of searching the direction of a certain number of LV — with the constraint of being orthogonal to each other — that meet the following criteria: (1) capture maximum variance in the \\(X\\)-space, (2) capture maximum variance in the \\(Y\\)-space and, (3) maximize correlation between the \\(X\\)- and \\(Y\\)-space.\nA PLS model can be written as:\n\\[\n\\begin{align*}\nX =& TP^T + E \\\\\nY =& UC^T + F \\\\\nT =& U + H\n\\end{align*}\n\\]\nwhere \\(T\\) is the scores matrix summarizing variation in the \\(X\\)-predictors — i.e. similarities/differences between samples — , \\(P\\) is the loadings matrix — or often called the \\(X\\)‐loadings to distinguish it from the \\(Y\\)‐loadings — , \\(U\\) is the scores matrix that summarizes variation in the \\(Y\\)-responses, \\(C\\) is the \\(Y\\)‐loadings and, \\(E\\),\\(F\\),\\(H\\)arethe residual matrices. The superscript \\(\"T\"\\) denotes the matrix transpose. Note that \\(P\\) expresses the correlation between \\(X\\) and \\(U\\), whilst \\(C\\) expresses the correlation between \\(Y\\) and \\(T\\).\n\n\n\nFIG. 1: Geometrical illustration of a PLS model. Data (black dots) on the \\(X\\)-space are projected (orthogonal projection) onto the subspace defined by the first two latent variables (LVs).\n\n\nOPLS model operates similarly to PLS model but separates the systematic variation in \\(X\\)into three parts: (1) a predictive part that is correlated to \\(Y\\), (2) an orthogonal part, “orth”, that is uncorrelated to \\(Y\\), (3) a noise part — residual variation.\nThus, an OPLS model can be formulated as:\n\\[\n\\begin{align*}\nX =& TP^T + T_{\\text{orth}}P^T_{\\text{orth}} + E \\\\\nY =& UC^T + F\n\\end{align*}\n\\]\n\n\n\nFIG. 2: Geometrical illustration of the difference between PLS and OPLS models. Orthogonal variation (white dots) are filtered out by the second latent variable.\n\n\nIt should be mentioned that in fact two different OPLS algorithms have been developed. The first algorithm, most frequently referred to as O1-PLS or simply OPLS, as described above is unidirectional \\((X \\implies Y)\\), meaning that only orthogonal variations in the \\(X\\)-space are filtered out. The second algorithm, referred to as O2-PLS, is bi-directional \\((X \\iff Y)\\), meaning that orthogonal variations in both the \\(X\\)- and \\(Y\\)-space are filtered out.\nAn O2-PLS model can be written as:\n\\[\n\\begin{align*}\nX =& TP^T + T_{\\text{Y-orth}}P^T_{\\text{Y-orth}} + E \\\\\nY =& UC^T + U_{\\text{X-orth}}C^T_{\\text{X-orth}} + F\n\\end{align*}\n\\]\nO2-PLS separates \\(X\\) and \\(Y\\) into three parts: (1) a joint part (correlation between \\(X\\) and \\(Y\\)), (2) an orthogonal part — unrelated latent variation in \\(X\\) and \\(Y\\) separately— ,and (3) a noise part."
  },
  {
    "objectID": "blog/posts/post1/index.html#example-libs-spectra",
    "href": "blog/posts/post1/index.html#example-libs-spectra",
    "title": "An Overview of Orthogonal Partial Least Squares",
    "section": "",
    "text": "For comparison purposes, I used the LIBS spectra of plant materials to predict the concentration of potassium using the PLS and OPLS approaches. PLS was performed using the R package caret. Since OPLS is not available in caret, I used the package ropls instead.\n\n\n\nFIG. 3: LIBS spectra of five plant samples. A baseline offset has been applied for easier viewing of the spectra.\n\n\nFigures below show the \\(X\\)-scores scatter plots (projection of the \\(X\\)-space) for PLS (left panel) and OPLS (right panel) modeling. In both figures, the \\(x\\)-axis represents the first component (or latent variable), whereas the \\(y\\)-axis represents the second component for PLS and the first orthogonal component for OPLS. The rainbow colors represent the concentration of potassium (K) in each sample, from 1% (dark blue) to 7% (dark red). As we can see, OPLS only requires one component to correlate variation in the samples with K concentration (see the black arrow).\n\n\n\nFIG. 4: Comparison between the \\(X\\)-scores scatter plots for the PLS and OPLS models."
  },
  {
    "objectID": "blog/posts/post1/index.html#summary",
    "href": "blog/posts/post1/index.html#summary",
    "title": "An Overview of Orthogonal Partial Least Squares",
    "section": "",
    "text": "So in summary, the most important thing to remember is that OPLS (or O2-PLS) offers a valuable preprocessing tool, and will help to generate more efficient predictive models, especially in situations where structured noise dominates."
  },
  {
    "objectID": "photography.html",
    "href": "photography.html",
    "title": "Home",
    "section": "",
    "text": "Landscape Photography\n\n \n\n\n\n\n\n\nCity Photography"
  },
  {
    "objectID": "research.html",
    "href": "research.html",
    "title": "Christian L. Goueguel",
    "section": "",
    "text": "Welcome to an overview of my research projects, from current initiatives to notable past projects in laser spectroscopy (LIBS) and analytical instrumentation. Feel free to reach out if you’d like to collaborate or learn more about my work.\n\n\nSoil Analysis Using LIBS\nLeading research to enhance quantitative analysis of agricultural soils, with a focus on mitigating matrix interference through LIBS combined with advanced chemometric modeling. This work aims to support sustainable agricultural practices.\n\n\n\nDevelopment of a High-Throughput LIBS System\nDeveloped a fiber-optic LIBS system integrated with chemometrics for online and real-time monitoring of electrolyte solutes in clinical and point-of-care (POC) settings.\n\n\n\n\n\n\nPredicting Soil Texture\nDeveloped a LIBS-based benchtop instrument for the quantitative analysis of agricultural soils, leveraging machine learning and chemometrics to interpret complex spectral data. This research was highlighted in the Journal of Analytical Atomic Spectrometry on the back cover of Volume 34, Issue 8 (2019), for its contributions to soil texture prediction.\n\n\n\nAn example of LIBS spectra recorded from three soil types: sandy, silty, and clayey.\n\n\n\n\n\nPrincipal component analysis (PCA) biplot of spectral data. Sandy soils (grey) and clayey soils (blue) clusters.\n\n\n\nGroundwater Monitoring for Carbon Capture and Storage (CCS)\nDeveloped a compact, robust LIBS sensor for in-situ monitoring of CCS activities in extreme environments (high temperature, pressure, and salinity). One of our research projects earned the prestigious distinction of being featured on the front cover of the Journal of Analytical Atomic Spectrometry (Volume 31, Issue 7, 2016). This study explored the application of underwater LIBS for real-time, in situ monitoring of carbonate mineral dissolution under rising CO2 pressure.\n\n\n\nUnderwater LIBS experimental setup. Measurements were taken in a controlled, high-pressure environment, replicating up to 400 bar (≈ 6,000 psi) conditions in carbonated brine environments. Quantitative analysis of key elements (Mg, Ca, K, Ba, Mn, Sr, etc.) in brine.\n\n\n\n\n\nLIBS for CCS: Signal-to-noise ratio for key emission lines.\n\n\n\nWavelength-Selective Excitation Approaches in Laser-produced plasma\nResearch focused on improving the limit of detection (LOD) for trace impurities in metallic alloys using selective wavelength approaches in laser-produced plasma. Explored LA-LIF, resonance-enhance LIBS, and RLA-LIF approaches for enhanced LODs in impurity analysis. Proposed efficient excitation-fluorescence schemes for targeted element detection in RLA-LIF. Our research on the investigation of resonance-enhance LIBS was featured on the inside front cover of the Journal of Analytical Atomic Spectrometry, Volume 25, Issue 5 (2010).\n\n\n\nLA-LIF and resonance-enhance LIBS experimental setup. A first Nd:YAG pulse generates low-density vapor, followed by a second tunable pulse targeting a resonant transition of an analyte (LA-LIF), or of a matrix element (resonance-enhance LIBS).\n\n\n\n\n\n(upper panel) Confocal microscopy images of resonance-enhance LIBS sample damage at three ablation laser fluences. (lower panel) Resonance-enhance LIBS is especially advantageous when minimal surface damage is critical, as it ablates approximately 10 times less material per shot than standard LIBS.\n\n\n\n\n\nPartial Grotrian diagram illustrating the excitation and fluorescence emission processes of lead, as used in LA-LIF and RLA-LIF experiments.\n\n\n\n\n\n(left panel) RLA-LIF improves detection sensitivity by approximately 1 order of magnitude at a significantly lower laser fluence compared to standard LIBS. (right panel) Optical coherence tomography (OCT)-based 3D plots and scanning electron microscope (SEM) images of sample ablation craters in RLA-LIF."
  },
  {
    "objectID": "research.html#current-research",
    "href": "research.html#current-research",
    "title": "Christian L. Goueguel",
    "section": "",
    "text": "Soil Analysis Using LIBS\nLeading research to enhance quantitative analysis of agricultural soils, with a focus on mitigating matrix interference through LIBS combined with advanced chemometric modeling. This work aims to support sustainable agricultural practices."
  },
  {
    "objectID": "research.html#past-research",
    "href": "research.html#past-research",
    "title": "Christian L. Goueguel",
    "section": "",
    "text": "Development of a High-Throughput LIBS System\nDeveloped a fiber-optic LIBS system integrated with chemometrics for online and real-time monitoring of electrolyte solutes in clinical and point-of-care (POC) settings.\n\n\n\n\n\n\nPredicting Soil Texture\nDeveloped a LIBS-based benchtop instrument for the quantitative analysis of agricultural soils, leveraging machine learning and chemometrics to interpret complex spectral data. This research was highlighted in the Journal of Analytical Atomic Spectrometry on the back cover of Volume 34, Issue 8 (2019), for its contributions to soil texture prediction.\n\n\n\nAn example of LIBS spectra recorded from three soil types: sandy, silty, and clayey.\n\n\n\n\n\nPrincipal component analysis (PCA) biplot of spectral data. Sandy soils (grey) and clayey soils (blue) clusters.\n\n\n\nGroundwater Monitoring for Carbon Capture and Storage (CCS)\nDeveloped a compact, robust LIBS sensor for in-situ monitoring of CCS activities in extreme environments (high temperature, pressure, and salinity). One of our research projects earned the prestigious distinction of being featured on the front cover of the Journal of Analytical Atomic Spectrometry (Volume 31, Issue 7, 2016). This study explored the application of underwater LIBS for real-time, in situ monitoring of carbonate mineral dissolution under rising CO2 pressure.\n\n\n\nUnderwater LIBS experimental setup. Measurements were taken in a controlled, high-pressure environment, replicating up to 400 bar (≈ 6,000 psi) conditions in carbonated brine environments. Quantitative analysis of key elements (Mg, Ca, K, Ba, Mn, Sr, etc.) in brine.\n\n\n\n\n\nLIBS for CCS: Signal-to-noise ratio for key emission lines.\n\n\n\nWavelength-Selective Excitation Approaches in Laser-produced plasma\nResearch focused on improving the limit of detection (LOD) for trace impurities in metallic alloys using selective wavelength approaches in laser-produced plasma. Explored LA-LIF, resonance-enhance LIBS, and RLA-LIF approaches for enhanced LODs in impurity analysis. Proposed efficient excitation-fluorescence schemes for targeted element detection in RLA-LIF. Our research on the investigation of resonance-enhance LIBS was featured on the inside front cover of the Journal of Analytical Atomic Spectrometry, Volume 25, Issue 5 (2010).\n\n\n\nLA-LIF and resonance-enhance LIBS experimental setup. A first Nd:YAG pulse generates low-density vapor, followed by a second tunable pulse targeting a resonant transition of an analyte (LA-LIF), or of a matrix element (resonance-enhance LIBS).\n\n\n\n\n\n(upper panel) Confocal microscopy images of resonance-enhance LIBS sample damage at three ablation laser fluences. (lower panel) Resonance-enhance LIBS is especially advantageous when minimal surface damage is critical, as it ablates approximately 10 times less material per shot than standard LIBS.\n\n\n\n\n\nPartial Grotrian diagram illustrating the excitation and fluorescence emission processes of lead, as used in LA-LIF and RLA-LIF experiments.\n\n\n\n\n\n(left panel) RLA-LIF improves detection sensitivity by approximately 1 order of magnitude at a significantly lower laser fluence compared to standard LIBS. (right panel) Optical coherence tomography (OCT)-based 3D plots and scanning electron microscope (SEM) images of sample ablation craters in RLA-LIF."
  },
  {
    "objectID": "talks.html",
    "href": "talks.html",
    "title": "Christian L. Goueguel",
    "section": "",
    "text": "Photo by Susan Q Yin."
  },
  {
    "objectID": "talks.html#oralposter-presentations",
    "href": "talks.html#oralposter-presentations",
    "title": "Home",
    "section": "Oral/Poster Presentations",
    "text": "Oral/Poster Presentations\n\nC. L. Goueguel, Direct determination of soil texture using laser-induced breakdown spectroscopy and multivariate linear regressions. Annual meetings of the North American Society for Laser-Induced Breakdown Spectroscopy (NASLIBS) hosted by FACSS SciX Conference 2019 (Palm Springs, CA, USA)\nC. L. Goueguel, Using LIBS analyzer for laser-based soil analysis for precision agriculture. Invited speaker at the Canadian Society for Analytical Sciences and Spectroscopy, Jan. 1, 2018 (Toronto, ON, Canada)\nJ. C. Jain, C. L. Goueguel, D. L. McIntyre, H. M. Edenborn, Dissolution of mineral carbonates with increasing CO2 pressure and the implications for carbon sequestration leak detection. GSA annual meeting, 2016 (Denver, CO, USA)\nC. G. Carson, C. Goueguel, J. Jain, D. McIntyre, Development of laser induced breakdown spectroscopy sensor to assess groundwater quality impacts resulting from geologic carbon sequestration. SPIE Micro- and Nanotechnology Sensors, Systems, and Applications VII, 2015 (Baltimore, MD, SA)\nC. L. Goueguel, J. C. Jain, D. L. McIntyre, A. K. Karamalidis, Application of laser induced breakdown spectroscopy (LIBS) to analyze CO2-bearing solutions in high pressure high temperature environment. ACS 248th - National Meeting, 2014 (San Francisco, CA, USA)\nC. L. Goueguel, J. C. Jain, A. K. Karamalidis, D. L. McIntyre, J. P. Singh, Laser-induced breakdown spectroscopy of high-pressure carbonated brine solutions. 2014 PITTCON Conference & Expo, 2014 (Chicago, IL, USA)\nC. L. Goueguel, D. L. McIntyre, J. C. Jain, J. P. Singh, A. K. Karamalidis, Analysis of calcium in CO2-laden brine (NaCl-CaCl2) by Laser-induced breakdown spectroscopy (LIBS). Annual meetings of the North American Society for Laser-Induced Breakdown Spectroscopy (NASLIBS) hosted by FACSS SciX Conference 2013 (Milwaukee, WI, USA)\nJ. P. Singh, C L. Goueguel, D. L. McIntyre, J. C. Jain, A. K. Karamalidis, Laser-induced breakdown spectroscopy for elemental analysis of brines. 7th International Conference on Laser-Induced Breakdown Spectroscopy, 2012 (Luxor, Egypt)\nC. Goueguel, Laser Induced Breakdown Spectroscopy (LIBS): From Mars exploration to GCS monitoring. Postdoctoral Seminar at the Department of Civil and Environmental Engineering, Carnegie Mellon University, 2012 (Pittsburgh, PA, USA)\nC. Goueguel, S. Laville, F. Vidal, M. Sabsabi, M. Chaker, Investigation of resonance-enhanced laser-induced breakdown spectroscopy (RELIBS) for the analysis of aluminum alloys. 6th International Conference on Laser- Induced Breakdown Spectroscopy, 2010 (Memphis, TN, USA)\nC. Goueguel, S. Laville, F. Vidal, M. Sabsabi, Chaker, Investigation of resonance-enhanced laser-induced breakdown spectroscopy (RELIBS) for the analysis of aluminum alloys, PITTCON Conference & Expo, 2010 (Orlando, FL, USA)\nC. Goueguel, S. Laville, H. Loudyi, F. Vidal, M. Sabsabi, M. Chaker, Investigation of resonance-enhanced laser-induced breakdown spectroscopy for the analysis of aluminum alloys. 2nd North American Symposium on Laser-Induced Breakdown Spectroscopy, 2009 (New Orleans, LA, USA)\nC. Goueguel, H. Loudyi, S. Laville, M. Chaker, M. Sabsabi, F. Vidal, Excitation spectrale sélective d’un plasma d’aluminium par une seconde impulsion laser: Optimisation des paramètres expérimentaux. Colloque Plasma- Québec, 2009 (Montreal, QC, Canada)\nS. Laville, K. Rifai, H. Loudyi, M. Chaker, M. Sabsabi, F. Vidal, C. Goueguel, Analysis of trace elements in liquids using LIBS combined with laser-induced fluorescence. 5th International Conference on Laser-Induced Breakdown Spectroscopy, 2008 (Berlin, Germany)\nF. Vidal, M. Chaker, C. Goueguel, S. Laville, H. Loudyi, K. Rifai, M. Sabsabi, Enhanced laser-induced breakdown spectroscopy by second pulse selective wavelength excitation. Laser and plasma applications in materials science: 1st International Conference on Laser Plasma Applications in Materials Science, LAPAMS, 2008 (Alger, Algeria)\nC. Goueguel, S. Laville, H. Loudyi, M. Chaker, M. Sabsabi, F. Vidal. Detection of lead in brass by Laser-Induced Breakdown Spectroscopy combined with Laser-Induced Fluorescence. Photonics North, 2008 (Montreal, QC, Canada)\nH. Loudyi, C. Goueguel, K. Rifai, S. Laville, M. Sabsabi, M. Chaker, F. Vidal, Enhancing the sensitivity of the laser-Induced Breakdown Spectroscopy technique: spectrally selective excitation of specific elements in laser produced plasma. Canadian Association of Physicists Congress, 2008 (Laval, QC, Canada)\nC. Goueguel, H. Loudyi, S. Laville, M. Chaker, M. Sabsabi, F. Vidal, Analyse des matériaux solides par LIBS: Effet d’une deuxième impulsion laser basée sur une excitation spectrale sélective. Colloque Plasma-Québec, 2008 (Montreal, QC, Canada)\nF. Vidal, S. Laville, C. Goueguel, H. Loudyi, M. Chaker, M. Sabsabi, Investigation of laser-induced breakdown spectroscopy combined with laser- induced fluorescence. 4th Euro-Mediterranean Symposium on Laser-Induced Breakdown Spectroscopy, 2007 (Paris, France)"
  },
  {
    "objectID": "projects/index.html",
    "href": "projects/index.html",
    "title": "R Packages",
    "section": "",
    "text": "Hotelling’s T-Squared Statistic and Ellipse\n\n\n\nR\n\n\nMachine Learning\n\n\nChemometrics\n\n\nExploratory Data Analysis\n\n\n\nThe HotellingEllipse package facilitates the comparison of multivariate datasets based on their PCA or PLS scores by computing the Hotelling’s T-squared statistic. The package also provides the semi-minor and semi-major axes for drawing…\n\n\n\nMay 18, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nComputation of 2D and 3D Elliptical Joint Confidence Regions\n\n\n\nR\n\n\nMachine Learning\n\n\nChemometrics\n\n\nClustering\n\n\n\nThe ConfidenceEllipse package calculates the coordinates of elliptical joint regions at a specified confidence level. It provides the flexibility to estimate classical or robust confidence regions, which can be visualized as 2D or 3D plots.…\n\n\n\nApr 22, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPreprocessing Tools for Spectroscopy Data Analysis\n\n\n\nR\n\n\nVisualization\n\n\nChemometrics\n\n\nPreprocessing\n\n\nExploratory Data Analysis\n\n\n\nThe specProc package performs a wide range of preprocessing tasks essential for spectroscopic data analysis. Spectral preprocessing is essential in ensuring accurate and reliable results by minimizing the impact of various distortions and…\n\n\n\nNov 1, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCRAN Package Usage App\n\n\n\nR\n\n\nVisualization\n\n\nApp\n\n\n\nShiny app that provides an interactive way to explore CRAN package usage. It enables users to display detailed download trends and compare up to 20 packages simultaneously.\n\n\n\nDec 1, 2024\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blog/posts/post2/index.html",
    "href": "blog/posts/post2/index.html",
    "title": "Multivariate Outlier Detection in High-Dimensional Spectral Data",
    "section": "",
    "text": "Photo by Will Myers.\n\n\n\n\nIn the realm of laser spectroscopy, outliers are usually characterized by values that deviate significantly from the values of other observations, abnormally high or low — for example, humidity change, spatial heterogeneity on the samples surface, drift of the instrument parameters, human error, etc. Or by a subtle change in the characteristics of the spectra not taken into account before, since calibration models are usually trained on samples with relatively “similar” chemical and physical properties — what we call matrix effects.\n\n“An outlier is an observation which deviates so much from the other observations as to arouse suspicions that it was generated by a different mechanism.” D. M. Hawkins\n\nSolutions for the former case — referred to as gross outliers or extreme values — are often straightforward: we fix what was wrong and repeat the measurements if necessary. The latter case is usually more complex and requires advanced statistical methods. However, as discussed later, these subtle changes are often very difficult to detect with traditional methods in addition to being ineffective in high dimensions. The challenge is therefore to find reliable methods which are both:\n\nFast\nRobust to outliers or contamination\nApplicable to high-dimensional data\n\nIn high-dimensional space, the data points are very sparse, so that all points are almost equidistant from each other. In other words, the use of Euclidean distances become meaningless. The result is that the degree of outlyingness of data points is indistinguishable from each other. For this reason, outliers are best detected by using a lower-dimensional local subspace in which only a subset of features may be relevant.\n\n“Outliers are often hidden in the unusual local behavior of low-dimensional subspaces, and this deviant behavior is masked by full-dimensional analysis.” C. C. Aggarwal\n\n\n\n\nStandard and widely used distance-based methods consist of computing the Mahalanobis distance. This class of methods only uses distance space to flag outlier observations. The Mahalanobis distance (MD) for the \\(i\\)-th observation is given by:\n\\[\n\\text{MD}_i = \\sqrt{(x_i-\\bar{x})^T C^{-1} (x_i-\\bar{x})} \\quad \\text{with} \\quad C = \\frac{1}{n-1}X^TX\n\\]\n\\(X\\) is the data matrix of size \\(n \\times p\\), where \\(p\\)is the number of variables and n is the number of observations. \\(x_i\\) is an observation (a row of \\(X\\)), \\(\\bar{x}\\) is the mean vector, \\(C\\) is the sample covariance matrix which gives information about the covariance structure of the data — i.e. the shape of the ellipsoid specified by the covariance matrix.\n\n\n\nFIG. 1: Robust Mahalanobis distance versus the sample (observation) number.\n\n\nTo identify outlier candidates, MD² is computed and compared to a cut-off value equal to the 0.975 quantile of the Chi-Square distributionwith m degrees of freedom, m being the number of variables. This comes from the fact that MD² of multivariate normal data follows a Chi-Square distribution. Albeit, Hardin and Rocke (2005) have reported that a better approximation is found using an F-distribution, especially for small sample sizes. Nonetheless, Chi-Square distribution is still reasonably good for approximating the squared Mahalanobis distance. Hence, an observation is considered as an outlier candidate if:\n\\[\n\\text{MD}^2 = \\chi^2_{m,0.975}\n\\]\nHowever, this approach has two major issues: (1) the arithmetic mean and the sample covariance matrix are sensitive to outliers and (2) the covariance matrix \\(X^TX\\) must be invertible — more formally non singular. The former is solved by applying robust statistics, whilst the latter is obviously a severe limitation for high-dimensional data often encountered in chemometrics where \\(p \\ggg n\\). Indeed, the same limitation holds even when one chooses to use the robust version of MD, which is derived from the Minimum Covariance Determinant (MCD) estimator, instead of the classical MD.\n\n\n\nIn chemometrics, Principal Component Analysis (PCA) is widely used for exploratory analysis and for dimensionality reduction and can be used as outlier detection method.\nIndeed, PCA score is often used in conjunction with the Mahalanobis distance (or Hotelling’s \\(T^2\\) at 95% confidence level) to determine how far an observation is from the centroid of the elliptical region that encompasses most of the observations.\n\n\n\nFIG. 2: PCA score for the LIBS spectra of plant samples with the 95% confidence limit for Hotelling’s \\(T^2\\). The transparency level of the data points shows the contribution of each sample to the first component. The darkest samples are those that contribute most.\n\n\nHowever, this approach is not robust since classical PCA relies on the sample covariance matrix C, and therefore suffers from being extremely sensitive to outliers observations, as can be seen in the above figure. The consequences of this are twofold: on one hand, the variance explained by the principal components is inflated towards the outliers direction and therefore masking observations that deviate from the regular observations — the so-called masking effect. On the other hand, regular observations might be falsely flagged as outliers — the so-called swamping effect.\n\n\n\nFIG. 3: Same PCA score but with the addition of two colors: (red) with and (blue) without measurements error. The former (70 samples) are indistinguishable from error-free measurements (369 samples), although they seem to cluster in the upper right corner which may indicate a similar error.\n\n\nHence, to avoid these effects, a number of robustified versions of PCA based upon a robust estimate of the covariance matrix (each characterized by their breakdown point), by means of S-estimator, MM-estimator, (Fast)MCD-estimator or, the re-weighted MCD- (RMCD) estimator have been proposed over the past decades. However, these methods are limited to small or moderate dimensions since the robust estimates are only applicable when the number of observations is at least twice the number of variables \\((n﹥2p)\\).\n\n\n\nFIG. 4: Robust PCA (Fast-MCD) score for the plant samples. Prior to perform robust PCA, feature selection was carried out to reduce the dimensionality. Here, we can better differentiate the measurements with (red) and without (blue) error. It turns out that the former contribute the most (dark red) to the first component.\n\n\n\n\n\nFIG. 5: 3D score plot for the plant samples. Now we can see that the red samples form two separate clusters. The first is clearly separated from error-free measurements, while the second is still difficult to discern but clusters above the regular values.\n\n\nThere is another group of robust PCA methods that have been developed and that are better suited for handling high-dimensional data in the situation where the sample size is lower than the dimension \\((p \\ggg n)\\). These methods are:\n\nRobust PCA by projection-pursuit (PP-PCA)\nSpherical PCA (SPCA)\nRobust PCA (ROBPCA)\nRobust Sparse PCA (ROSPCA)\n\nThe projection-pursuit approach to robust PCA has been initially introduced by Li and Chen (1985) and is based on finding the directions that maximize a projection index. PP-PCA uses the median absolute deviation (MAD) or Qn-estimator as projection index instead of the variance. SPCA was derived by Locantore et al. (1999) and aims to project data onto the surface of a unit sphere to alleviate outliers effect. Like PP-PCA, SPCA uses MAD or Qn-estimator as a robust estimate of spread.\nMore recently, ROBPCA and ROSPCA have been introduced by Hubert et al. 2005 and by Hubert et al. 2015. ROBPCA combines the projection pursuit approach with the robust covariance estimation based on the RMCD in a low dimensional space. Interestingly, ROBPCA is much faster than the aforementioned methods and has the advantage to be applicable to both symmetrically distributed data and skewed data (Hubert et al. 2009). On the other hand, ROSPCA is derived from ROBPCA, but uses Sparse PCA (Zou et al. 2006) instead of PCA.\n\n\n\nAfter having briefly reviewed outlier detection methods based upon robust PCA, it is worth mentioning that outliers can be classified into two categories: Leverage points and Orthogonal outliers. As the figure below illustrates, the leverage points category can be split into Good leverage points and Bad leverage points. As their name suggests, one can have a positive effect while the other has a negative effect.\n\n\n\nFIG. 6: Illustration of the different types of outliers on the 2-dimensional PCA subspace. Image credit: Christian Goueguel.\n\n\nLeverage points and orthogonal outliers are differentiated by their respective scores and orthogonal distances. These distances tell us how far an observation is from the center of the ellipse defined by the regular observations (score = 0). The score distance (SD) is a measure of the distance between an observation belonging to the \\(k\\)-dimensional PCA subspace and the origin of that subspace. The orthogonal distance (OD) measures the deviation — i.e. lack of fit — of an observation from the \\(k\\)-dimensional PCA subspace.\n\n\n\nFIG. 7: Illustration of the score and orthogonal distances. Image credit: Christian Goueguel.\n\n\nThus, leverage points are characterized by a high score distance, while orthogonal outliers are characterized by a high orthogonal distance. Likewise, good and bad leverage points are differentiated by their respective orthogonal distance. Bad leverage points display higher orthogonal distance than good leverage points.\nThe score distance for the \\(i\\)-th observation on the \\(k\\)-dimensional PCA subspace is given by:\n\\[\n\\text{SD}^2_i = \\sum^k_{j=1}{\\frac{\\text{t}^2_{ij}}{l_j}}\n\\]\nwhere \\(l\\) are the eigenvalues of the MCD scatter matrix and t are the robust scores, for each \\(j = 1, \\cdots, k\\). Like the Mahalanobis distance, the cut-off values for outlying the squared score distances are obtained from the 0.975 quantile of the Chi-Square distributionwith \\(k\\)degrees of freedom.\n\\[\n\\text{SD}^2 &gt;  \\chi^2_{k,0.975}\n\\]\nThe corresponding orthogonal distance is given by:\n\\[\n\\text{OD}_i = \\| x_i-\\hat{\\mu}_x - P\\times t_i \\|\n\\]\nwhere \\(P\\) is the loading matrix, ûₓ is the robust estimate of center. The cut-off values for the orthogonal distances are obtained using the Wilson-Hilferty approximation for a Chi-Squared distribution. As a result, the orthogonal distances, raised to the power 2/3, are approximately normally distributed. Thus, the cut-off values for outliers observations are given by:\n\\[\n\\text{OD} &gt; \\left[ \\text{median}\\left(\\text{OD}^{2/3} \\right) + Z_{0.975}\\text{MAD}\\left(\\text{OD}^{2/3} \\right)  \\right]^{3/2}\n\\]\nwhere \\(Z_{0.975}\\) is the 0.975 quantile of the Standard Normal distribution.\n\n\n\nOutlier map provides a powerful graphical tool for visually identifying the different types of outliers.\n\n\n\nFIG. 8: Illustration of outlier map.\n\n\nFor each observation in the dataset, the score distance and the orthogonal distance are plotted on the x- and y-axis. Their respective cut-off values divide the map into four quadrants:\n\nRegular observations are in the bottom left corner of the map,\nOrthogonal outliers are in the upper left corner of the map,\nLeverage points are on the right side, with good leverage points at the bottom and bad leverage points at the top.\n\n\n\n\nFIG. 9: Outlier map obtained from ROBPCA using LIBS spectra of plant samples.\n\n\nThe above figure was obtained with ROBPCA. It took 15.86 seconds of execution time (laptop, 2.80GHz) to process the data matrix of size 439×7153. We can see that most of the measurements are in the quadrant of regular observations while some are flagged as orthogonal outliers and bad leverage points. Some of them are good leverage points. On the other hand, the measurements with error are mostly orthogonal outliers, although some are considered regular observations."
  },
  {
    "objectID": "blog/posts/post2/index.html#introduction",
    "href": "blog/posts/post2/index.html#introduction",
    "title": "Multivariate Outlier Detection in High-Dimensional Spectral Data",
    "section": "",
    "text": "In the realm of laser spectroscopy, outliers are usually characterized by values that deviate significantly from the values of other observations, abnormally high or low — for example, humidity change, spatial heterogeneity on the samples surface, drift of the instrument parameters, human error, etc. Or by a subtle change in the characteristics of the spectra not taken into account before, since calibration models are usually trained on samples with relatively “similar” chemical and physical properties — what we call matrix effects.\n\n“An outlier is an observation which deviates so much from the other observations as to arouse suspicions that it was generated by a different mechanism.” D. M. Hawkins\n\nSolutions for the former case — referred to as gross outliers or extreme values — are often straightforward: we fix what was wrong and repeat the measurements if necessary. The latter case is usually more complex and requires advanced statistical methods. However, as discussed later, these subtle changes are often very difficult to detect with traditional methods in addition to being ineffective in high dimensions. The challenge is therefore to find reliable methods which are both:\n\nFast\nRobust to outliers or contamination\nApplicable to high-dimensional data\n\nIn high-dimensional space, the data points are very sparse, so that all points are almost equidistant from each other. In other words, the use of Euclidean distances become meaningless. The result is that the degree of outlyingness of data points is indistinguishable from each other. For this reason, outliers are best detected by using a lower-dimensional local subspace in which only a subset of features may be relevant.\n\n“Outliers are often hidden in the unusual local behavior of low-dimensional subspaces, and this deviant behavior is masked by full-dimensional analysis.” C. C. Aggarwal"
  },
  {
    "objectID": "blog/posts/post2/index.html#mahalanobis-distance",
    "href": "blog/posts/post2/index.html#mahalanobis-distance",
    "title": "Multivariate Outlier Detection in High-Dimensional Spectral Data",
    "section": "",
    "text": "Standard and widely used distance-based methods consist of computing the Mahalanobis distance. This class of methods only uses distance space to flag outlier observations. The Mahalanobis distance (MD) for the \\(i\\)-th observation is given by:\n\\[\n\\text{MD}_i = \\sqrt{(x_i-\\bar{x})^T C^{-1} (x_i-\\bar{x})} \\quad \\text{with} \\quad C = \\frac{1}{n-1}X^TX\n\\]\n\\(X\\) is the data matrix of size \\(n \\times p\\), where \\(p\\)is the number of variables and n is the number of observations. \\(x_i\\) is an observation (a row of \\(X\\)), \\(\\bar{x}\\) is the mean vector, \\(C\\) is the sample covariance matrix which gives information about the covariance structure of the data — i.e. the shape of the ellipsoid specified by the covariance matrix.\n\n\n\nFIG. 1: Robust Mahalanobis distance versus the sample (observation) number.\n\n\nTo identify outlier candidates, MD² is computed and compared to a cut-off value equal to the 0.975 quantile of the Chi-Square distributionwith m degrees of freedom, m being the number of variables. This comes from the fact that MD² of multivariate normal data follows a Chi-Square distribution. Albeit, Hardin and Rocke (2005) have reported that a better approximation is found using an F-distribution, especially for small sample sizes. Nonetheless, Chi-Square distribution is still reasonably good for approximating the squared Mahalanobis distance. Hence, an observation is considered as an outlier candidate if:\n\\[\n\\text{MD}^2 = \\chi^2_{m,0.975}\n\\]\nHowever, this approach has two major issues: (1) the arithmetic mean and the sample covariance matrix are sensitive to outliers and (2) the covariance matrix \\(X^TX\\) must be invertible — more formally non singular. The former is solved by applying robust statistics, whilst the latter is obviously a severe limitation for high-dimensional data often encountered in chemometrics where \\(p \\ggg n\\). Indeed, the same limitation holds even when one chooses to use the robust version of MD, which is derived from the Minimum Covariance Determinant (MCD) estimator, instead of the classical MD."
  },
  {
    "objectID": "blog/posts/post2/index.html#robust-principal-component-analysis",
    "href": "blog/posts/post2/index.html#robust-principal-component-analysis",
    "title": "Multivariate Outlier Detection in High-Dimensional Spectral Data",
    "section": "",
    "text": "In chemometrics, Principal Component Analysis (PCA) is widely used for exploratory analysis and for dimensionality reduction and can be used as outlier detection method.\nIndeed, PCA score is often used in conjunction with the Mahalanobis distance (or Hotelling’s \\(T^2\\) at 95% confidence level) to determine how far an observation is from the centroid of the elliptical region that encompasses most of the observations.\n\n\n\nFIG. 2: PCA score for the LIBS spectra of plant samples with the 95% confidence limit for Hotelling’s \\(T^2\\). The transparency level of the data points shows the contribution of each sample to the first component. The darkest samples are those that contribute most.\n\n\nHowever, this approach is not robust since classical PCA relies on the sample covariance matrix C, and therefore suffers from being extremely sensitive to outliers observations, as can be seen in the above figure. The consequences of this are twofold: on one hand, the variance explained by the principal components is inflated towards the outliers direction and therefore masking observations that deviate from the regular observations — the so-called masking effect. On the other hand, regular observations might be falsely flagged as outliers — the so-called swamping effect.\n\n\n\nFIG. 3: Same PCA score but with the addition of two colors: (red) with and (blue) without measurements error. The former (70 samples) are indistinguishable from error-free measurements (369 samples), although they seem to cluster in the upper right corner which may indicate a similar error.\n\n\nHence, to avoid these effects, a number of robustified versions of PCA based upon a robust estimate of the covariance matrix (each characterized by their breakdown point), by means of S-estimator, MM-estimator, (Fast)MCD-estimator or, the re-weighted MCD- (RMCD) estimator have been proposed over the past decades. However, these methods are limited to small or moderate dimensions since the robust estimates are only applicable when the number of observations is at least twice the number of variables \\((n﹥2p)\\).\n\n\n\nFIG. 4: Robust PCA (Fast-MCD) score for the plant samples. Prior to perform robust PCA, feature selection was carried out to reduce the dimensionality. Here, we can better differentiate the measurements with (red) and without (blue) error. It turns out that the former contribute the most (dark red) to the first component.\n\n\n\n\n\nFIG. 5: 3D score plot for the plant samples. Now we can see that the red samples form two separate clusters. The first is clearly separated from error-free measurements, while the second is still difficult to discern but clusters above the regular values.\n\n\nThere is another group of robust PCA methods that have been developed and that are better suited for handling high-dimensional data in the situation where the sample size is lower than the dimension \\((p \\ggg n)\\). These methods are:\n\nRobust PCA by projection-pursuit (PP-PCA)\nSpherical PCA (SPCA)\nRobust PCA (ROBPCA)\nRobust Sparse PCA (ROSPCA)\n\nThe projection-pursuit approach to robust PCA has been initially introduced by Li and Chen (1985) and is based on finding the directions that maximize a projection index. PP-PCA uses the median absolute deviation (MAD) or Qn-estimator as projection index instead of the variance. SPCA was derived by Locantore et al. (1999) and aims to project data onto the surface of a unit sphere to alleviate outliers effect. Like PP-PCA, SPCA uses MAD or Qn-estimator as a robust estimate of spread.\nMore recently, ROBPCA and ROSPCA have been introduced by Hubert et al. 2005 and by Hubert et al. 2015. ROBPCA combines the projection pursuit approach with the robust covariance estimation based on the RMCD in a low dimensional space. Interestingly, ROBPCA is much faster than the aforementioned methods and has the advantage to be applicable to both symmetrically distributed data and skewed data (Hubert et al. 2009). On the other hand, ROSPCA is derived from ROBPCA, but uses Sparse PCA (Zou et al. 2006) instead of PCA."
  },
  {
    "objectID": "blog/posts/post2/index.html#different-types-of-outliers",
    "href": "blog/posts/post2/index.html#different-types-of-outliers",
    "title": "Multivariate Outlier Detection in High-Dimensional Spectral Data",
    "section": "",
    "text": "After having briefly reviewed outlier detection methods based upon robust PCA, it is worth mentioning that outliers can be classified into two categories: Leverage points and Orthogonal outliers. As the figure below illustrates, the leverage points category can be split into Good leverage points and Bad leverage points. As their name suggests, one can have a positive effect while the other has a negative effect.\n\n\n\nFIG. 6: Illustration of the different types of outliers on the 2-dimensional PCA subspace. Image credit: Christian Goueguel.\n\n\nLeverage points and orthogonal outliers are differentiated by their respective scores and orthogonal distances. These distances tell us how far an observation is from the center of the ellipse defined by the regular observations (score = 0). The score distance (SD) is a measure of the distance between an observation belonging to the \\(k\\)-dimensional PCA subspace and the origin of that subspace. The orthogonal distance (OD) measures the deviation — i.e. lack of fit — of an observation from the \\(k\\)-dimensional PCA subspace.\n\n\n\nFIG. 7: Illustration of the score and orthogonal distances. Image credit: Christian Goueguel.\n\n\nThus, leverage points are characterized by a high score distance, while orthogonal outliers are characterized by a high orthogonal distance. Likewise, good and bad leverage points are differentiated by their respective orthogonal distance. Bad leverage points display higher orthogonal distance than good leverage points.\nThe score distance for the \\(i\\)-th observation on the \\(k\\)-dimensional PCA subspace is given by:\n\\[\n\\text{SD}^2_i = \\sum^k_{j=1}{\\frac{\\text{t}^2_{ij}}{l_j}}\n\\]\nwhere \\(l\\) are the eigenvalues of the MCD scatter matrix and t are the robust scores, for each \\(j = 1, \\cdots, k\\). Like the Mahalanobis distance, the cut-off values for outlying the squared score distances are obtained from the 0.975 quantile of the Chi-Square distributionwith \\(k\\)degrees of freedom.\n\\[\n\\text{SD}^2 &gt;  \\chi^2_{k,0.975}\n\\]\nThe corresponding orthogonal distance is given by:\n\\[\n\\text{OD}_i = \\| x_i-\\hat{\\mu}_x - P\\times t_i \\|\n\\]\nwhere \\(P\\) is the loading matrix, ûₓ is the robust estimate of center. The cut-off values for the orthogonal distances are obtained using the Wilson-Hilferty approximation for a Chi-Squared distribution. As a result, the orthogonal distances, raised to the power 2/3, are approximately normally distributed. Thus, the cut-off values for outliers observations are given by:\n\\[\n\\text{OD} &gt; \\left[ \\text{median}\\left(\\text{OD}^{2/3} \\right) + Z_{0.975}\\text{MAD}\\left(\\text{OD}^{2/3} \\right)  \\right]^{3/2}\n\\]\nwhere \\(Z_{0.975}\\) is the 0.975 quantile of the Standard Normal distribution."
  },
  {
    "objectID": "blog/posts/post2/index.html#outlier-map",
    "href": "blog/posts/post2/index.html#outlier-map",
    "title": "Multivariate Outlier Detection in High-Dimensional Spectral Data",
    "section": "",
    "text": "Outlier map provides a powerful graphical tool for visually identifying the different types of outliers.\n\n\n\nFIG. 8: Illustration of outlier map.\n\n\nFor each observation in the dataset, the score distance and the orthogonal distance are plotted on the x- and y-axis. Their respective cut-off values divide the map into four quadrants:\n\nRegular observations are in the bottom left corner of the map,\nOrthogonal outliers are in the upper left corner of the map,\nLeverage points are on the right side, with good leverage points at the bottom and bad leverage points at the top.\n\n\n\n\nFIG. 9: Outlier map obtained from ROBPCA using LIBS spectra of plant samples.\n\n\nThe above figure was obtained with ROBPCA. It took 15.86 seconds of execution time (laptop, 2.80GHz) to process the data matrix of size 439×7153. We can see that most of the measurements are in the quadrant of regular observations while some are flagged as orthogonal outliers and bad leverage points. Some of them are good leverage points. On the other hand, the measurements with error are mostly orthogonal outliers, although some are considered regular observations."
  },
  {
    "objectID": "blog/index.html",
    "href": "blog/index.html",
    "title": "Christian L. Goueguel",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nA Comparative Exploration of Orthogonal Signal Correction Methods\n\n\n\n\n\nOrthogonal signal correction (OSC) is a powerful preprocessing technique frequently used to remove variation in spectral data that is orthogonal to the property of interest. Over the years, several implementations of OSC have emerged, with the most notable being those by Wold et al., Sjöblom et al., and Fearn. This post compares these three methods, exploring their algorithmic approaches and practical implications.\n\n\n\n\n\nMay 25, 2023\n\n\nChristian L. Goueguel\n\n\n1 min\n\n\n\n\n\n\n\n\n\n\n\n\nAn Overview of Orthogonal Partial Least Squares\n\n\n\n\n\nHave you ever heard of Orthogonal Partial Least-Squares (OPLS)? This article aims to give you a clear and concise overview of OPLS and its advantages in the development of more efficient predictive models.\n\n\n\n\n\nDec 20, 2019\n\n\nChristian L. Goueguel\n\n\n6 min\n\n\n\n\n\n\n\n\n\n\n\n\nChemometric Modeling with Tidymodels: A Tutorial for Spectroscopic Data\n\n\n\n\n\nIn this post, we demonstrate how to build robust chemometric models for spectroscopic data using the Tidymodels framework in R. This workflow is designed to cater to beginners and advanced practitioners alike, offering an end-to-end guide from data preprocessing to model evaluation and interpretation.\n\n\n\n\n\nApr 17, 2022\n\n\nChristian L. Goueguel\n\n\n10 min\n\n\n\n\n\n\n\n\n\n\n\n\nMultivariate Outlier Detection in High-Dimensional Spectral Data\n\n\n\n\n\nHigh-dimensional data are particularly challenging for outlier detection. Robust PCA methods have been developed to build models that are unaffected by outliers in high dimensions. These outliers are generally characterized by their deviation from the PCA subspace.\n\n\n\n\n\nJan 7, 2020\n\n\nChristian L. Goueguel\n\n\n9 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "cv.html",
    "href": "cv.html",
    "title": "Curriculum Vitae",
    "section": "",
    "text": ":::"
  },
  {
    "objectID": "talks.html#oral-presentations",
    "href": "talks.html#oral-presentations",
    "title": "Christian L. Goueguel",
    "section": "Oral Presentations",
    "text": "Oral Presentations\n\nC. L. Goueguel, Direct determination of soil texture using laser-induced breakdown spectroscopy and multivariate linear regressions. Annual meetings of the North American Society for Laser-Induced Breakdown Spectroscopy (NASLIBS) hosted by FACSS SciX Conference 2019 (Palm Springs, CA, USA)\nC. L. Goueguel, Using LIBS analyzer for laser-based soil analysis for precision agriculture. Invited speaker at the Canadian Society for Analytical Sciences and Spectroscopy, Jan. 1, 2018 (Toronto, ON, Canada)\nJ. C. Jain, C. L. Goueguel, D. L. McIntyre, H. M. Edenborn, Dissolution of mineral carbonates with increasing CO2 pressure and the implications for carbon sequestration leak detection. GSA annual meeting, 2016 (Denver, CO, USA)\nC. G. Carson, C. Goueguel, J. Jain, D. McIntyre, Development of laser induced breakdown spectroscopy sensor to assess groundwater quality impacts resulting from geologic carbon sequestration. SPIE Micro- and Nanotechnology Sensors, Systems, and Applications VII, 2015 (Baltimore, MD, SA)\nC. L. Goueguel, J. C. Jain, D. L. McIntyre, A. K. Karamalidis, Application of laser induced breakdown spectroscopy (LIBS) to analyze CO2-bearing solutions in high pressure high temperature environment. ACS 248th - National Meeting, 2014 (San Francisco, CA, USA)\nC. L. Goueguel, J. C. Jain, A. K. Karamalidis, D. L. McIntyre, J. P. Singh, Laser-induced breakdown spectroscopy of high-pressure carbonated brine solutions. 2014 PITTCON Conference & Expo, 2014 (Chicago, IL, USA)\nJ. P. Singh, C L. Goueguel, D. L. McIntyre, J. C. Jain, A. K. Karamalidis, Laser-induced breakdown spectroscopy for elemental analysis of brines. 7th International Conference on Laser-Induced Breakdown Spectroscopy, 2012 (Luxor, Egypt)\nC. Goueguel, Laser Induced Breakdown Spectroscopy (LIBS): From Mars exploration to GCS monitoring. Postdoctoral Seminar at the Department of Civil and Environmental Engineering, Carnegie Mellon University, 2012 (Pittsburgh, PA, USA)\nC. Goueguel, S. Laville, F. Vidal, M. Sabsabi, Chaker, Investigation of resonance-enhanced laser-induced breakdown spectroscopy (RELIBS) for the analysis of aluminum alloys, PITTCON Conference & Expo, 2010 (Orlando, FL, USA)\nC. Goueguel, S. Laville, H. Loudyi, F. Vidal, M. Sabsabi, M. Chaker, Investigation of resonance-enhanced laser-induced breakdown spectroscopy for the analysis of aluminum alloys. 2nd North American Symposium on Laser-Induced Breakdown Spectroscopy, 2009 (New Orleans, LA, USA)\nC. Goueguel, H. Loudyi, S. Laville, M. Chaker, M. Sabsabi, F. Vidal, Excitation spectrale sélective d’un plasma d’aluminium par une seconde impulsion laser: Optimisation des paramètres expérimentaux. Colloque Plasma- Québec, 2009 (Montreal, QC, Canada)\nS. Laville, K. Rifai, H. Loudyi, M. Chaker, M. Sabsabi, F. Vidal, C. Goueguel, Analysis of trace elements in liquids using LIBS combined with laser-induced fluorescence. 5th International Conference on Laser-Induced Breakdown Spectroscopy, 2008 (Berlin, Germany)\nF. Vidal, M. Chaker, C. Goueguel, S. Laville, H. Loudyi, K. Rifai, M. Sabsabi, Enhanced laser-induced breakdown spectroscopy by second pulse selective wavelength excitation. Laser and plasma applications in materials science: 1st International Conference on Laser Plasma Applications in Materials Science, LAPAMS, 2008 (Alger, Algeria)\nC. Goueguel, S. Laville, H. Loudyi, M. Chaker, M. Sabsabi, F. Vidal. Detection of lead in brass by Laser-Induced Breakdown Spectroscopy combined with Laser-Induced Fluorescence. Photonics North, 2008 (Montreal, QC, Canada)\nH. Loudyi, C. Goueguel, K. Rifai, S. Laville, M. Sabsabi, M. Chaker, F. Vidal, Enhancing the sensitivity of the laser-Induced Breakdown Spectroscopy technique: spectrally selective excitation of specific elements in laser produced plasma. Canadian Association of Physicists Congress, 2008 (Laval, QC, Canada)\nC. Goueguel, H. Loudyi, S. Laville, M. Chaker, M. Sabsabi, F. Vidal, Analyse des matériaux solides par LIBS: Effet d’une deuxième impulsion laser basée sur une excitation spectrale sélective. Colloque Plasma-Québec, 2008 (Montreal, QC, Canada)\nF. Vidal, S. Laville, C. Goueguel, H. Loudyi, M. Chaker, M. Sabsabi, Investigation of laser-induced breakdown spectroscopy combined with laser- induced fluorescence. 4th Euro-Mediterranean Symposium on Laser-Induced Breakdown Spectroscopy, 2007 (Paris, France)"
  },
  {
    "objectID": "talks.html#poster-presentations",
    "href": "talks.html#poster-presentations",
    "title": "Christian L. Goueguel",
    "section": "Poster Presentations",
    "text": "Poster Presentations\n\nC. L. Goueguel, D. L. McIntyre, J. C. Jain, J. P. Singh, A. K. Karamalidis, Analysis of calcium in CO2-laden brine (NaCl-CaCl2) by Laser-induced breakdown spectroscopy (LIBS). Annual meetings of the North American Society for Laser-Induced Breakdown Spectroscopy (NASLIBS) hosted by FACSS SciX Conference 2013 (Milwaukee, WI, USA)\nC. Goueguel, S. Laville, F. Vidal, M. Sabsabi, M. Chaker, Investigation of resonance-enhanced laser-induced breakdown spectroscopy (RELIBS) for the analysis of aluminum alloys. 6th International Conference on Laser- Induced Breakdown Spectroscopy, 2010 (Memphis, TN, USA)"
  },
  {
    "objectID": "blog/posts/post4/index.html",
    "href": "blog/posts/post4/index.html",
    "title": "A Comparative Exploration of Orthogonal Signal Correction Methods",
    "section": "",
    "text": "Photo by Jonatan Pie.\n\n\nWold’s method was the first formal OSC algorithm. It operates iteratively to identify orthogonal components unrelated to the dependent variable \\(Y\\). The method leverages a combination of principal component analysis (PCA) and partial least squares (PLS). Sjöblom’s approach builds on Wold’s by introducing a direct orthogonalization step. The algorithm emphasizes calibration transfer, making it especially useful for standardizing spectral datasets across instruments or conditions. Whereas, Fearn proposed a mathematically elegant version of OSC, simplifying the computation by leveraging matrix operations. The method directly orthogonalizes \\(X\\) using a singular value decomposition (SVD) of a residual matrix."
  }
]