[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Christian L. Goueguel",
    "section": "",
    "text": "Christian L. Goueguel is a research scientist specializing in laser spectroscopy and chemometrics. His work spans industries such as environmental monitoring, agriculture, medical devices, and dairy production. He has authored 20+ peer-reviewed articles and presented his research at numerous international conferences.\nThroughout his career, Christian has consistently pushed the boundaries of analytical chemistry, delivering innovative, high-impact solutions that bridge research and practical applications.\nHe actively contributes to the academic and research community by serving as a referee for several scientific journals including J. Anal. At. Spectrom., ChemComm, Appl. Opt., Photonics Research, Opt. Lett., and Opt. Express. In this role, he evaluates and reviews submitted manuscripts, providing constructive feedback to ensure the quality, rigor, and integrity of the published research."
  },
  {
    "objectID": "bio.html",
    "href": "bio.html",
    "title": "About",
    "section": "",
    "text": "Christian earned his PhD in 2012 from the Institut National de la Recherche Scientifique (INRS)-University of Quebec, in Varennes, Canada. Before pursuing his PhD in Canada, he completed both his Bachelor’s and Master’s degrees in Physics at the Grenoble Institute of Technology, France. His doctoral research, conducted at the National Research Council (NRC) in Boucherville, Canada, focused on improving the analytical performance of Laser-Induced Breakdown Spectroscopy (LIBS) for trace element analysis in metallic alloys.\nFollowing his PhD, Christian undertook a NETL-RAU postdoctoral role at Carnegie Mellon University (CMU) in Pittsburgh, Pennsylvania, and an ORISE postdoctoral fellowship at the U.S. Department of Energy’s National Energy Technology Laboratory (NETL). He conducted research on underwater LIBS for real-time environmental monitoring in carbon capture and storage (CCS). His efforts centered on developing innovative approaches to address analytical challenges associated with monitoring trace elements under extreme subsurface conditions.\nChristian has also applied his expertise in agricultural innovation as a research scientist in an agronomic services company. He developed LIBS-based systems for rapid, cost-effective soil and plant tissue analysis, optimizing measurement procedures, designing multivariate calibration models, and integrating machine learning algorithms to predict soil textural properties and nutrient content with high accuracy. Christian also worked as a senior optical engineer for a California-based tech company headquartered in Fremont, while contributing to operations at their Scottsdale office in Arizona. Additionally, he has worked as a chemometrics consultant, where he specialized in developing machine learning models for milk quality assessment using near-infrared spectroscopy (NIRS) data.\nChristian values quality time with his friends and family, finding joy in outdoor activities. A passionate amateur photographer, he also enjoys capturing the beauty of landscapes and cities through his lens, blending his love for optics with his artistic passion."
  },
  {
    "objectID": "bio.html#about",
    "href": "bio.html#about",
    "title": "Christian L. Goueguel",
    "section": "",
    "text": "Christian earned his PhD in 2012 from the Institut National de la Recherche Scientifique (INRS)-University of Quebec, in Varennes, Canada, under the supervision of Mohamed Chaker and François Vidal. Before pursuing his PhD in Canada, he completed both his Bachelor’s and Master’s degrees in Physics at the Grenoble Institute of Technology, France. His doctoral research, conducted at the National Research Council (NRC) in Boucherville, Canada, focused on improving the analytical performance of Laser-Induced Breakdown Spectroscopy (LIBS) for trace element analysis in metallic alloys.\nFollowing his PhD, Christian undertook a NETL-RAU postdoctoral role at Carnegie Mellon University (CMU) in Pittsburgh, Pennsylvania, and an ORISE postdoctoral fellowship at the U.S. Department of Energy’s National Energy Technology Laboratory (NETL). He conducted research on underwater LIBS for real-time environmental monitoring in carbon capture and storage (CCS). His efforts centered on developing innovative approaches to address analytical challenges associated with monitoring trace elements under extreme subsurface conditions.\nChristian has also applied his expertise in agricultural innovation as a research scientist in an agronomic services company. He developed LIBS-based systems for rapid, cost-effective soil and plant tissue analysis, optimizing measurement procedures, designing multivariate calibration models, and integrating machine learning algorithms to predict soil textural properties and nutrient content with high accuracy. Christian also worked as a senior optical engineer for a California-based tech company headquartered in Fremont, while contributing to operations at their Scottsdale office in Arizona. Additionally, he has worked as a chemometrics consultant, where he specialized in developing machine learning models for milk quality assessment using near-infrared spectroscopy (NIRS) data.\nChristian values quality time with his friends and family, finding joy in outdoor activities. A passionate amateur photographer, he also enjoys capturing the beauty of landscapes and cities through his lens, blending his love for optics with his artistic passion. Driven by his belief in learning through teaching, Christian launched his blog to share insights and ideas. Having written a few posts already, he looks forward to publishing many more in the future."
  },
  {
    "objectID": "publications.html",
    "href": "publications.html",
    "title": "Publications",
    "section": "",
    "text": "Photo by Sylvia Yang."
  },
  {
    "objectID": "publications.html#book-chapters",
    "href": "publications.html#book-chapters",
    "title": "Publications",
    "section": "Book Chapters",
    "text": "Book Chapters\n\nC.R. Bhatt, C.L. Goueguel, J.C. Jain, D.L. McIntyre, J.P. Singh, LIBS application to liquid samples In: Laser-Induced Breakdown Spectroscopy (2nd Edition), Elsevier Science, 2020\nD.L. McIntyre, J.C. Jain, C.L. Goueguel, J.P. Singh, Application of Laser-Induced Breakdown Spectroscopy (LIBS) to Carbon Sequestration Research and Development In: Spectroscopic Techniques for Security, Forensic and Environmental Applications, Nova Science Publishers, 2014"
  },
  {
    "objectID": "publications.html#articles",
    "href": "publications.html#articles",
    "title": "Publications",
    "section": "Articles",
    "text": "Articles\n\nC.L. Goueguel, A. Soumare, C. Nault, J. Nault, Direct determination of soil texture using laser-induced breakdown spectroscopy and multivariate linear regressions, Journal of Analytical Atomic Spectrometry 34, 2019\nC.L. Goueguel, C.R. Bhatt, J.C. Jain, C.L. Lopano, D.L. McIntyre, Quantification of dissolved metals in high-pressure CO2-water solutions by underwater laser-induced breakdown spectroscopy, Optics & Laser Technology 108, 2018\nC.R. Bhatt, J.C. Jain, C.L. Goueguel, D.L. McIntyre, J.P. Singh, Determination of rare earth elements in geological samples using laser-induced breakdown spectroscopy (LIBS), Applied spectroscopy 72, 2018\nC.R. Bhatt, J.C. Jain, C.L. Goueguel, D.L. McIntyre, J.P. Singh, Measurement of Eu and Yb in aqueous solutions by underwater laser-induced breakdown spectroscopy, Spectrochemical Acta Part B: Atomic Spectroscopy 137, 2017\nC.R. Bhatt, C.L. Goueguel, J.C. Jain, H.M. Edenborn, D.L. McIntyre, Analysis of charcoal blast furnace slags by laser-induced breakdown spectroscopy, Applied Optics 56, 2017\nJ.C. Jain, C.L. Goueguel, C.R. Bhatt, D.L. McIntyre, LIBS Sensor for Sub-surface CO2 Leak Detection in Carbon Sequestration, Sensors & Transducers Journal 214, 2017\nJ.C. Jain, D.L. McIntyre, C.L. Goueguel, Harsh environment low- cost LIBS sensor for sub-surface CO2 leak detection in carbon sequestration, Materials for Energy, Efficiency, and Sustainability: TechConnect Briefs 2017\nC.L. Goueguel, J.C. Jain, D.L. McIntyre, C.G. Carson, H.M. Edenborn, In situ measurements of calcium carbonate dissolution under rising CO2 pressure using underwater laser-induced breakdown spectroscopy, Journal of Analytical Atomic Spectrometry 31, 2016\nC.L. Goueguel, D.L. McIntyre, J.C. Jain, Matrix effect of sodium compounds on the determination of metal ions in aqueous solutions by underwater laser-induced breakdown spectroscopy, Optics letters 41, 2016\nC.G. Carson, C.L. Goueguel, H. Sanghapi, J.C. Jain, D.L. McIntyre, Evaluation of a commercially available passively Q-switched Nd: YAG laser with LiF: F2- saturable absorber for laser-induced breakdown spectroscopy, Optics & Laser Technology 79, 2016\nC.L. Goueguel, D.L. McIntyre, J.C. Jain, A.K. Karamalidis, C.G. Carson, Matrix effect of sodium compounds on the determination of metal ions in aqueous solutions by underwater laser-induced breakdown spectroscopy, Applied optics 54, 2015\nC.L. Goueguel, D.L. McIntyre, J.C. Jain, A.K. Karamalidis, Laser-Induced Breakdown Spectroscopy (LIBS) of a High-Pressure CO2–Water Mixture: Application to Carbon Sequestration, Applied spectroscopy 68, 2014\nJ.C. Jain, D.L. McIntyre, K. Ayyalasomayajula, V. Dikshit, C.L. Goueguel, F. Yu-Yueh, J.P. Singh, Application of laser-induced breakdown spectroscopy in carbon sequestration research and development, Pramana 83, 2014\nC.L. Goueguel, J.P. Singh, D.L. McIntyre, J.C. Jain, A.K. Karamalidis, Effect of sodium chloride concentration on elemental analysis of brines by laser-induced breakdown spectroscopy (LIBS), Applied spectroscopy 68, 2014\nC.L. Goueguel, S. Laville, F. Vidal, M. Chaker, M. Sabsabi, Resonant laser-induced breakdown spectroscopy for analysis of lead traces in copper alloys, Journal of Analytical Atomic Spectrometry 26, 2011\nF. Vidal, S. Laville, C.L. Goueguel, H. Loudyi, K. Rifai, M. Chaker, M. Sabsabi, A simple model of laser-induced fluorescence under arbitrary optical thickness conditions at the excitation wavelength, Journal of Quantitative Spectroscopy and Radiative Transfer 111, 2010\nC.L. Goueguel, S. Laville, F. Vidal, M. Sabsabi, M. Chaker, Investigation of resonance-enhanced laser-induced breakdown spectroscopy for analysis of aluminum alloys, Journal of Analytical Atomic Spectrometry 25, 2010\nS. Laville, C.L. Goueguel, H. Loudyi, F. Vidal, M. Chaker, M. Sabsabi, Laser-induced fluorescence detection of lead atoms in a laser-induced plasma: An experimental analytical optimization study, Spectrochimica Acta Part B: Atomic Spectroscopy 64, 2009"
  },
  {
    "objectID": "publications.html#conference-proceedings",
    "href": "publications.html#conference-proceedings",
    "title": "Publications",
    "section": "Conference Proceedings",
    "text": "Conference Proceedings\n\nJ.C. Jain, H.M. Edenborn, C.L. Goueguel, C.R. Bhatt, D. Hartzler, D.L. McIntyre, A Novel Approach for Tracking CO2 Leakage Into Groundwater Using Carbonate Mineral Dissolution, Geological Society of America, 50, 2018\nJ.C. Jain, C.L. Goueguel, D.L. McIntyre, Development of LIBS Sensor for Sub-Surface CO2 Leak Detection in Carbon Sequestration, 2017 AIChE Annual Meeting\nJ.C. Jain, H.M. Edenborn, C.L. Goueguel, C.R. Bhatt, D.L. McIntyre, A Rapid Method for the Chemical Analysis of Charcoal Iron Furnace Slags, Geological Society of America, 49, 2017\nC.G. Carson, C.L. Goueguel, J.C. Jain, D.L. McIntyre, Development of laser-induced breakdown spectroscopy sensor to assess groundwater quality impacts resulting from geologic carbon sequestration, Micro-and Nanotechnology Sensors, Systems, and Applications VII 9467, 2015\nF. Vidal, M. Chaker, C.L. Goueguel, S. Laville, H. Loudyi, K. Rifai, M. Sabsabi, Enhanced Laser‐Induced Breakdown Spectroscopy by Second‐Pulse Selective Wavelength Excitation, AIP Conference Proceedings 1047, 2008\nC.L. Goueguel, S. Laville, H. Loudyi, M. Chaker, M. Sabsabi, F. Vidal, Detection of lead in brass by laser-induced breakdown spectroscopy combined with laser-induced fluorescence, Photonics North 2008, 709927, 2008"
  },
  {
    "objectID": "blog/posts/post3/index.html",
    "href": "blog/posts/post3/index.html",
    "title": "Chemometric Modeling with Tidymodels: A Tutorial for Spectroscopic Data",
    "section": "",
    "text": "Photo by Robert Lukeman.\n\n\n\n\nFor this tutorial, we use the beer dataset, publicly available and commonly used for spectroscopy-based regression problems. This dataset contains near-infrared spectroscopy (NIRS) data of beer samples alongside measurements of the original gravity (alcoholic beverage), which serves as the target variable. Original gravity (OG) is one of the primary metrics used by brewers to estimate the potential alcohol content of the beer, as it reflects the fermentable sugar content available for yeast metabolism. By analyzing OG alongside the NIRS spectra, we can explore how the spectral data correlates with this fundamental brewing property, offering insights into the chemical composition and quality of the beer samples.\n\nSetup\nBelow, we use suppressPackageStartupMessages to suppress startup messages for clarity and load essential packages.\n\nssh = suppressPackageStartupMessages\n\n\ntidyverse for data manipulation and visualization.\ntidymodels for modeling workflows and machine learning.\ntidymodels_prefer() ensures consistency across conflicting tidymodels functions.\n\n\nssh(library(tidyverse))\nssh(library(tidymodels))\ntidymodels_prefer()\n\nAdditional libraries include:\n\nkknn: Implements k-nearest neighbors (KNN).\nglmnet: Used for elastic net regression.\nranger: Used for random forest.\nplsmod: Supports partial least squares (PLS) regression.\nmagrittr: Provides pipe operators (%&gt;%, %&lt;&gt;%).\npatchwork: Simplifies combining ggplot2 plots.\n\n\nlibrary(kknn)\nlibrary(plsmod)\nssh(library(glmnet))\nssh(library(ranger))\n\n\nssh(library(magrittr))\nlibrary(patchwork)\n\nWe set a custom theme with a clean white background and adjusted sizes for all ggplot2 plots.\n\nbase_size = 15 \ntheme_bw(\n  base_size = base_size,\n  base_line_size = base_size / 22,\n  base_rect_size = base_size / 15\n  ) %&gt;% \n  theme_set()\n\n\n\nDataset Overview\nWe begin by loading the beer dataset and identifying the spectral predictor columns, which correspond to the NIRS wavelength variables. Usually, I prefer storing spectral wavelengths as character strings in a variable named wavelength because it makes data manipulation easier. This approach enhances flexibility when selecting, filtering, or grouping columns, simplifies integration with tidyverse functions, and ensures compatibility with tidymodels preprocessing workflows.\n\nbeer_data &lt;- read_csv(\"beer.csv\", show_col_types = FALSE)\nwavelength &lt;- beer_data %&gt;% select(starts_with(\"xtrain\")) %&gt;% names()\n\nPreviewing the first rows of the dataset helps us ensure its integrity and understand its structure.\n\nbeer_data %&gt;% head(5) %&gt;% DT::datatable()\n\n\n\n\n\n\n\nShow the code\np &lt;- beer_data %&gt;% mutate(spectra_id = paste0(\"s\", 1:80)) %&gt;%\n  pivot_longer(\n  cols = -c(originalGravity, spectra_id),\n  names_to = \"wavelength\",\n  values_to = \"intensity\"\n  ) %&gt;%\n  mutate(wavelength = rep(seq(1100, 2250, 2), times = 80)) %&gt;%\n  ggplot() +\n  aes(x = wavelength, y = intensity, colour = originalGravity, group = spectra_id) +\n  geom_line() +\n  scale_color_viridis_c(option = \"inferno\", direction = 1) +\n  labs(\n    x = \"Wavelength [nm]\", \n    y = \"Absorbance [arb. units]\", \n    title = \"NIRS Spectra of Beer Samples\", \n    subtitle = \"Contains 80 samples, measured from 1100 to 2250 nm\", \n    color = \"Original Gravity\") +\n  theme_minimal()\n\nplotly::ggplotly(p)\n\n\n\n\n\n\n\n\nSupervised Learning Techniques\nFor this analysis, we’ll evaluate and compare the performance of four supervised learning algorithms, categorized by their linearity or modeling approach (parametric vs. non-parametric):\n\n\n\nAlgorithm\nAcronym\nApproach\n\n\n\n\nsparse Partial Least Squares\nsPLS\nLinear\n\n\nElastic Net\nENet\nLinear\n\n\nk-Nearest Neighbors\nKNN\nNon-linear\n\n\nRandom Forests\nRF\nNon-linear\n\n\n\n\n\nStep 1: Data Splitting\nTo ensure unbiased model evaluation, we partition the data into training (80%) and testing (20%) sets, employing stratified sampling based on the target variable’s distribution.\n\nset.seed(123)\nsplit_data &lt;- initial_split(beer_data, prop = 0.8, strata = originalGravity)\ntrain_data &lt;- training(split_data)\ntest_data &lt;- testing(split_data)\n\n\n\nStep 2: Cross-Validation\nWe use a 5-fold repeated cross-validation strategy for hyperparameters tuning and performance evaluation, minimizing the risk of overfitting.\n\ncv_folds &lt;- vfold_cv(train_data, v = 5, repeats = 3)\n\nInterestingly, the vfold_cv function provides a powerful way to visualize the distribution of data across folds, allowing us to confirm that the stratification and splits are evenly balanced. This ensures that each fold accurately represents the overall dataset, enhancing the reliability of cross-validation results.\n\n\nShow the code\ncv_folds %&gt;%\n  tidy() %&gt;%\n  ggplot(aes(x = Fold, y = Row, fill = Data)) +\n  geom_tile() + \n  facet_wrap(~Repeat) + \n  scale_fill_brewer(palette = \"Paired\")\n\n\n\n\n\n\n\n\n\n\n\nStep 3: Preprocessing\nPreprocessing spectral data is a vast and intricate topic, deserving its own dedicated discussion, which we will explore in a future post. For this tutorial, we remove zero-variance predictors, and center the spectra intensity to ensure the data is well-suited for modeling.\ntidymodels provides a wide array of preprocessing steps through its versatile step_* functions. These functions allow for comprehensive data transformations, including centering, scaling, feature selection, and more, to be seamlessly integrated into the modeling workflow. Additionally, tidymodels offers the flexibility to create custom recipe steps, enabling you to design and implement tailored data transformations that meet your specific needs.\n\nbase_recipe &lt;- recipe(originalGravity ~ ., data = train_data) %&gt;%\n  update_role(originalGravity, new_role = \"outcome\") %&gt;%\n  update_role(all_of(wavelength), new_role = \"predictor\") %&gt;%\n  step_zv(all_predictors()) %&gt;%\n  step_center(all_predictors())\n\nFor comparison with the base preprocessing step, we introduce an additional step that applies Principal Component Analysis (PCA) to the predictor variables. This transformation reduces the dimensionality of the data while retaining the most significant variance.\n\npca_recipe &lt;- base_recipe %&gt;%\n  step_pca(all_predictors())\n\n\n\nStep 4: Model Specifications\nWe now define model specifications for each algorithm, incorporating hyperparameter tuning within the tidymodels framework. Notably, the tune function is used to specify hyperparameters that require optimization during the tuning process. For parameters with predefined values, these can be directly assigned within their allowable range.\n\n# k-Nearest Neighbors (KNN)\nknn_spec &lt;- nearest_neighbor() %&gt;%\n  set_args(neighbors = tune(), weight_func = tune(), dist_power = tune()) %&gt;%\n  set_engine('kknn') %&gt;%\n  set_mode('regression')\n\n# Partial Least Squares Regression (PLSR)\nspls_spec &lt;- pls() %&gt;%\n  set_args(predictor_prop = tune(), num_comp = tune()) %&gt;%\n  set_engine('mixOmics') %&gt;%\n  set_mode('regression')\n\n# Elastic Net (ENet)\nenet_spec &lt;-linear_reg() %&gt;%\n  set_args(penalty = tune(), mixture = tune()) %&gt;%\n  set_engine(\"glmnet\") %&gt;%\n  set_mode(\"regression\")\n\n# Random Forest (RF)\nrf_spec &lt;- rand_forest() %&gt;%\n  set_args(mtry = tune(), trees = tune(), min_n = tune()) %&gt;%\n  set_engine(\"ranger\") %&gt;% \n  set_mode(\"regression\")\n\n\n\nStep 5: Model Training and Tuning\nUsing the workflow_set function, we define a workflow for each model and tune hyperparameters using a grid search, and enable parallel processing using 4 CPU cores, accelerating computation during tuning.\n\nwflowSet &lt;- workflow_set(\n  preproc = list(base = base_recipe, pca = pca_recipe), \n  models = list(\n    knn = knn_spec, \n    spls = spls_spec, \n    enet = enet_spec, \n    rf = rf_spec), \n  cross = TRUE\n  )\n\nAs mentionned earlier, we opted to train each model using both the base preprocessing approach and the PCA-transformed data. This strategy results in a total of eight unique training models as follows:\n\nwflowSet\n\n# A workflow set/tibble: 8 × 4\n  wflow_id  info             option    result    \n  &lt;chr&gt;     &lt;list&gt;           &lt;list&gt;    &lt;list&gt;    \n1 base_knn  &lt;tibble [1 × 4]&gt; &lt;opts[0]&gt; &lt;list [0]&gt;\n2 base_spls &lt;tibble [1 × 4]&gt; &lt;opts[0]&gt; &lt;list [0]&gt;\n3 base_enet &lt;tibble [1 × 4]&gt; &lt;opts[0]&gt; &lt;list [0]&gt;\n4 base_rf   &lt;tibble [1 × 4]&gt; &lt;opts[0]&gt; &lt;list [0]&gt;\n5 pca_knn   &lt;tibble [1 × 4]&gt; &lt;opts[0]&gt; &lt;list [0]&gt;\n6 pca_spls  &lt;tibble [1 × 4]&gt; &lt;opts[0]&gt; &lt;list [0]&gt;\n7 pca_enet  &lt;tibble [1 × 4]&gt; &lt;opts[0]&gt; &lt;list [0]&gt;\n8 pca_rf    &lt;tibble [1 × 4]&gt; &lt;opts[0]&gt; &lt;list [0]&gt;\n\n\n\nctrl_grid &lt;- control_grid(\n  verbose = TRUE,\n  allow_par = TRUE,\n  extract = NULL,\n  save_pred = TRUE,\n  pkgs = c(\"doParallel\", \"doFuture\"),\n  save_workflow = FALSE,\n  event_level = \"first\",\n  parallel_over = \"resamples\"\n  )\n\n\ncl &lt;- parallel::makePSOCKcluster(4)\ndoParallel::registerDoParallel(cl)\n\n\nwflowSet %&lt;&gt;%\n  workflow_map(\n    fn = \"tune_grid\",\n    resamples = cv_folds,\n    grid = 10,\n    metrics = metric_set(rmse, mae, rsq),\n    control = ctrl_grid,\n    seed = 3L,\n    verbose = TRUE\n  )\n\n\nparallel::stopCluster(cl)\n\nNext, we utilize the rank_results function to systematically rank the training models based on key performance metrics such as the root mean squared error (RMSE), the mean absolute error (MAE), and the coefficient of determination \\((R^2)\\). Following this, we visualize the model performance with error bars, providing a clear and insightful comparison of their predictive capabilities.\n\nwflowSet %&gt;%\n  rank_results(rank_metric = \"rmse\") %&gt;%\n  relocate(c(rank, model, .metric, mean, std_err), .before = wflow_id) %&gt;%\n  mutate(mean = round(mean, 4), std_err = round(std_err, 4)) %&gt;%\n  DT::datatable(rownames = FALSE, filter =\"top\")\n\n\n\n\n\n\nwflowSet %&gt;% autoplot(std_errs = qnorm(0.95), type = \"wflow_id\") +\n  ggsci::scale_color_lancet() +\n  theme(legend.position = \"bottom\") +\n  ylab(\"\")\n\n\n\n\n\n\n\n\n\n\nStep 6: Model Selection\nBy configuring the autoplot function with the argument select_best = TRUE, we rank the training models while visually emphasizing the best-performing model, making it easy to identify the optimal choice for further evaluation.\n\nwflowSet %&gt;% autoplot(select_best = TRUE, std_errs = qnorm(0.95), type = \"wflow_id\") +\n  geom_point(size = 3) +\n  ggsci::scale_color_lancet() +\n  theme(legend.position = \"bottom\") +\n  ylab(\"\")\n\n\n\n\n\n\n\n\nThe extract_workflow_set_result function retrieves the optimized hyperparameter values for the best-performing model, as determined by the lowest RMSE. In this analysis, it determines the optimal settings for base_spls, specifically identifying the optimized number of latent variables (num_comp) and the proportion of predictors (predictor_prop) allowed to have non-zero coefficients.\n\nbest_model &lt;- wflowSet %&gt;% \n  extract_workflow_set_result(\"base_spls\") %&gt;% \n  select_best(metric = \"rmse\") %&gt;%\n  print()\n\n# A tibble: 1 × 3\n  predictor_prop num_comp .config              \n           &lt;dbl&gt;    &lt;int&gt; &lt;chr&gt;                \n1         0.0633        3 Preprocessor1_Model01\n\n\nWe use the collect_predictions function to gather the best-performing model’s training data and visualize the relationship between the actual and predicted values. This allows us to assess the model’s predictive accuracy. Additionally, we perform a residual analysis using standardized residuals to further evaluate the model’s performance and identify any potential areas for improvement.\n\ntrain_results &lt;- wflowSet %&gt;% \n  collect_predictions() %&gt;% \n  filter(wflow_id == \"base_spls\" & .config == best_model %&gt;% pull(.config)) %&gt;%\n  select(-.row)\n\n\ntrain_results %&gt;% \n  mutate(\n    .pred = round(.pred, 2), \n    originalGravity = round(originalGravity, 4),\n    residuals = round((originalGravity - .pred)/sd((originalGravity - .pred)), 2)\n    ) %&gt;%\n  relocate(c(.pred, originalGravity, residuals, model), .before = wflow_id) %&gt;%\n  DT::datatable(rownames = FALSE)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nStep 7: Model Testing\nFinally, we finalize the best-performing model (base_spls) by utilizing the extract_workflow and finalize_workflow functions. We then assess the model’s performance on the test set using last_fit(split = split_data), calculating key metrics to evaluate its accuracy. These metrics are retrieved using the collect_metrics() function.\nAs done previously, we visualize the results through actual vs. predicted plots, complemented by residual diagnostics, to provide a comprehensive evaluation of the model’s performance.\n\ntest_results &lt;- wflowSet %&gt;% \n  extract_workflow(\"base_spls\") %&gt;% \n  finalize_workflow(best_model) %&gt;% \n  last_fit(split = split_data)\n\n\ntest_results$.predictions[[1]] %&gt;%\n  mutate(\n    .pred = round(.pred, 2), \n    originalGravity = round(originalGravity, 4),\n    residuals = round((originalGravity - .pred)/sd((originalGravity - .pred)), 2)\n    ) %&gt;%\n  relocate(residuals, .before = .config) %&gt;%\n  DT::datatable(rownames = FALSE, width = 400)\n\n\n\n\n\n\ntest_results %&gt;% \n  collect_metrics() %&gt;%\n  mutate(.estimate = round(.estimate, 4)) %&gt;%\n  DT::datatable(rownames = FALSE, width = 400)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nConclusion\nThis tutorial showcased the versatility of the Tidymodels framework for chemometric applications. By leveraging its modular and tidy design, you can implement robust spectroscopic models tailored to your dataset, ensuring both accuracy and reproducibility."
  },
  {
    "objectID": "blog/posts/post1/index.html",
    "href": "blog/posts/post1/index.html",
    "title": "An Overview of Orthogonal Partial Least Squares",
    "section": "",
    "text": "Photo by Simon Berger.\n\n\n\n\nFirst and foremost, let me briefly recall that Partial Least Squares (PLS) regression is, without doubt,one of the most, or maybe the most, multivariate regression methods commonly used in chemometrics. In fact, PLS was originally developed around 1975 by Herman Wold for use in the field of econometrics and was later embraced in the 1980’s by prominent chemometricians such as Svante Wold — Herman Wold’s son, who in 1971 coined the term chemometrics to describe this emerging field of using advanced statistical and machine learning methods in analytical chemistry applications — , Harald Martens, Bruce Kowalski, Tormod Naes, and Paul Geladi, just to name a few. Over the past few decades, PLS — whether as a regression or classification method — has become very popular among chemometrics practitioners for dealing with multicollinearity in applications such as multivariate calibration and process analytical technology, where there are usually fewer observations (samples) than variables (wavelengths).\nIn 2002, Johan Trygg and Svante Wold introduced a variant of PLS in a paper entitled “Orthogonal projections to latent structures (O‐PLS)”. The new approach is a supervised multivariate data projection method used to relate a set of predictor variables \\((X)\\) to one or more responses \\((Y)\\).Basically, like PLS, O-PLS attempts to extract the maximum information reflecting the variation in the data set, while assuming the existence of a small subset of hidden variables in the \\(X\\)-data to predict the response variables. These subsets are formally called latent variables (or LVs) because they are unmeasurable. Historically, this concept of hidden structure in a data set is a century old and comes from methods such as Principal Component Analysis (PCA). The O-PLS method, as named by the authors, uses orthogonal signal correction previously developed by S. Wold — Chemometrics Intell. Lab. Syst., 1998, 44, 175 — to maximize the explained covariance on the first LV, while the remaining LVs capture variance in the predictors which is orthogonal, i.e. statistically uncorrelated to the response variables. To put it simply, this means that unlike PLS — which handle random noise fairly well — , the new method also known as Orthogonal Partial Least-Squares (OPLS) enables to filter out the structured noise in the data set by modeling separately variations of the \\(X\\)-predictors correlated and uncorrelated to the \\(Y\\)-responses. Ultimately, this reduces the model’s complexity by lowing the number of LVs, in addition to allowing identification, analysis and investigation of the main source of orthogonal variation.\n\n\n\nLet’s take a quick look at the mathematics behind OPLS. Herein we consider \\(X\\) as a matrix of size \\(n \\times p\\) and \\(Y\\) as a matrix of size \\(n\\times m\\), where \\(p\\)isthe number of predictor variables, \\(m\\) isthe number of response variables and, \\(n\\) is the number of observations. Recall that PLS has been developed with the aim of searching the direction of a certain number of LV — with the constraint of being orthogonal to each other — that meet the following criteria: (1) capture maximum variance in the \\(X\\)-space, (2) capture maximum variance in the \\(Y\\)-space and, (3) maximize correlation between the \\(X\\)- and \\(Y\\)-space.\nA PLS model can be written as:\n\\[\n\\begin{align*}\nX =& TP^T + E \\\\\nY =& UC^T + F \\\\\nT =& U + H\n\\end{align*}\n\\]\nwhere \\(T\\) is the scores matrix summarizing variation in the \\(X\\)-predictors — i.e. similarities/differences between samples — , \\(P\\) is the loadings matrix — or often called the \\(X\\)‐loadings to distinguish it from the \\(Y\\)‐loadings — , \\(U\\) is the scores matrix that summarizes variation in the \\(Y\\)-responses, \\(C\\) is the \\(Y\\)‐loadings and, \\(E\\),\\(F\\),\\(H\\)arethe residual matrices. The superscript \\(\"T\"\\) denotes the matrix transpose. Note that \\(P\\) expresses the correlation between \\(X\\) and \\(U\\), whilst \\(C\\) expresses the correlation between \\(Y\\) and \\(T\\).\n\n\n\nFIG. 1: Geometrical illustration of a PLS model. Data (black dots) on the \\(X\\)-space are projected (orthogonal projection) onto the subspace defined by the first two latent variables (LVs).\n\n\nOPLS model operates similarly to PLS model but separates the systematic variation in \\(X\\)into three parts: (1) a predictive part that is correlated to \\(Y\\), (2) an orthogonal part, “orth”, that is uncorrelated to \\(Y\\), (3) a noise part — residual variation.\nThus, an OPLS model can be formulated as:\n\\[\n\\begin{align*}\nX =& TP^T + T_{\\text{orth}}P^T_{\\text{orth}} + E \\\\\nY =& UC^T + F\n\\end{align*}\n\\]\n\n\n\nFIG. 2: Geometrical illustration of the difference between PLS and OPLS models. Orthogonal variation (white dots) are filtered out by the second latent variable.\n\n\nIt should be mentioned that in fact two different OPLS algorithms have been developed. The first algorithm, most frequently referred to as O1-PLS or simply OPLS, as described above is unidirectional \\((X \\implies Y)\\), meaning that only orthogonal variations in the \\(X\\)-space are filtered out. The second algorithm, referred to as O2-PLS, is bi-directional \\((X \\iff Y)\\), meaning that orthogonal variations in both the \\(X\\)- and \\(Y\\)-space are filtered out.\nAn O2-PLS model can be written as:\n\\[\n\\begin{align*}\nX =& TP^T + T_{\\text{Y-orth}}P^T_{\\text{Y-orth}} + E \\\\\nY =& UC^T + U_{\\text{X-orth}}C^T_{\\text{X-orth}} + F\n\\end{align*}\n\\]\nO2-PLS separates \\(X\\) and \\(Y\\) into three parts: (1) a joint part (correlation between \\(X\\) and \\(Y\\)), (2) an orthogonal part — unrelated latent variation in \\(X\\) and \\(Y\\) separately— ,and (3) a noise part.\n\n\n\nFor comparison purposes, I used the LIBS spectra of plant materials to predict the concentration of potassium using the PLS and OPLS approaches. PLS was performed using the R package caret. Since OPLS is not available in caret, I used the package ropls instead.\n\n\n\nFIG. 3: LIBS spectra of five plant samples. A baseline offset has been applied for easier viewing of the spectra.\n\n\nFigures below show the \\(X\\)-scores scatter plots (projection of the \\(X\\)-space) for PLS (left panel) and OPLS (right panel) modeling. In both figures, the \\(x\\)-axis represents the first component (or latent variable), whereas the \\(y\\)-axis represents the second component for PLS and the first orthogonal component for OPLS. The rainbow colors represent the concentration of potassium (K) in each sample, from 1% (dark blue) to 7% (dark red). As we can see, OPLS only requires one component to correlate variation in the samples with K concentration (see the black arrow).\n\n\n\nFIG. 4: Comparison between the \\(X\\)-scores scatter plots for the PLS and OPLS models.\n\n\n\n\n\nSo in summary, the most important thing to remember is that OPLS (or O2-PLS) offers a valuable preprocessing tool, and will help to generate more efficient predictive models, especially in situations where structured noise dominates."
  },
  {
    "objectID": "blog/posts/post1/index.html#what-is-opls",
    "href": "blog/posts/post1/index.html#what-is-opls",
    "title": "An Overview of Orthogonal Partial Least Squares",
    "section": "",
    "text": "First and foremost, let me briefly recall that Partial Least Squares (PLS) regression is, without doubt,one of the most, or maybe the most, multivariate regression methods commonly used in chemometrics. In fact, PLS was originally developed around 1975 by Herman Wold for use in the field of econometrics and was later embraced in the 1980’s by prominent chemometricians such as Svante Wold — Herman Wold’s son, who in 1971 coined the term chemometrics to describe this emerging field of using advanced statistical and machine learning methods in analytical chemistry applications — , Harald Martens, Bruce Kowalski, Tormod Naes, and Paul Geladi, just to name a few. Over the past few decades, PLS — whether as a regression or classification method — has become very popular among chemometrics practitioners for dealing with multicollinearity in applications such as multivariate calibration and process analytical technology, where there are usually fewer observations (samples) than variables (wavelengths).\nIn 2002, Johan Trygg and Svante Wold introduced a variant of PLS in a paper entitled “Orthogonal projections to latent structures (O‐PLS)”. The new approach is a supervised multivariate data projection method used to relate a set of predictor variables \\((X)\\) to one or more responses \\((Y)\\).Basically, like PLS, O-PLS attempts to extract the maximum information reflecting the variation in the data set, while assuming the existence of a small subset of hidden variables in the \\(X\\)-data to predict the response variables. These subsets are formally called latent variables (or LVs) because they are unmeasurable. Historically, this concept of hidden structure in a data set is a century old and comes from methods such as Principal Component Analysis (PCA). The O-PLS method, as named by the authors, uses orthogonal signal correction previously developed by S. Wold — Chemometrics Intell. Lab. Syst., 1998, 44, 175 — to maximize the explained covariance on the first LV, while the remaining LVs capture variance in the predictors which is orthogonal, i.e. statistically uncorrelated to the response variables. To put it simply, this means that unlike PLS — which handle random noise fairly well — , the new method also known as Orthogonal Partial Least-Squares (OPLS) enables to filter out the structured noise in the data set by modeling separately variations of the \\(X\\)-predictors correlated and uncorrelated to the \\(Y\\)-responses. Ultimately, this reduces the model’s complexity by lowing the number of LVs, in addition to allowing identification, analysis and investigation of the main source of orthogonal variation."
  },
  {
    "objectID": "blog/posts/post1/index.html#mathematical-framework",
    "href": "blog/posts/post1/index.html#mathematical-framework",
    "title": "An Overview of Orthogonal Partial Least Squares",
    "section": "",
    "text": "Let’s take a quick look at the mathematics behind OPLS. Herein we consider \\(X\\) as a matrix of size \\(n \\times p\\) and \\(Y\\) as a matrix of size \\(n\\times m\\), where \\(p\\)isthe number of predictor variables, \\(m\\) isthe number of response variables and, \\(n\\) is the number of observations. Recall that PLS has been developed with the aim of searching the direction of a certain number of LV — with the constraint of being orthogonal to each other — that meet the following criteria: (1) capture maximum variance in the \\(X\\)-space, (2) capture maximum variance in the \\(Y\\)-space and, (3) maximize correlation between the \\(X\\)- and \\(Y\\)-space.\nA PLS model can be written as:\n\\[\n\\begin{align*}\nX =& TP^T + E \\\\\nY =& UC^T + F \\\\\nT =& U + H\n\\end{align*}\n\\]\nwhere \\(T\\) is the scores matrix summarizing variation in the \\(X\\)-predictors — i.e. similarities/differences between samples — , \\(P\\) is the loadings matrix — or often called the \\(X\\)‐loadings to distinguish it from the \\(Y\\)‐loadings — , \\(U\\) is the scores matrix that summarizes variation in the \\(Y\\)-responses, \\(C\\) is the \\(Y\\)‐loadings and, \\(E\\),\\(F\\),\\(H\\)arethe residual matrices. The superscript \\(\"T\"\\) denotes the matrix transpose. Note that \\(P\\) expresses the correlation between \\(X\\) and \\(U\\), whilst \\(C\\) expresses the correlation between \\(Y\\) and \\(T\\).\n\n\n\nFIG. 1: Geometrical illustration of a PLS model. Data (black dots) on the \\(X\\)-space are projected (orthogonal projection) onto the subspace defined by the first two latent variables (LVs).\n\n\nOPLS model operates similarly to PLS model but separates the systematic variation in \\(X\\)into three parts: (1) a predictive part that is correlated to \\(Y\\), (2) an orthogonal part, “orth”, that is uncorrelated to \\(Y\\), (3) a noise part — residual variation.\nThus, an OPLS model can be formulated as:\n\\[\n\\begin{align*}\nX =& TP^T + T_{\\text{orth}}P^T_{\\text{orth}} + E \\\\\nY =& UC^T + F\n\\end{align*}\n\\]\n\n\n\nFIG. 2: Geometrical illustration of the difference between PLS and OPLS models. Orthogonal variation (white dots) are filtered out by the second latent variable.\n\n\nIt should be mentioned that in fact two different OPLS algorithms have been developed. The first algorithm, most frequently referred to as O1-PLS or simply OPLS, as described above is unidirectional \\((X \\implies Y)\\), meaning that only orthogonal variations in the \\(X\\)-space are filtered out. The second algorithm, referred to as O2-PLS, is bi-directional \\((X \\iff Y)\\), meaning that orthogonal variations in both the \\(X\\)- and \\(Y\\)-space are filtered out.\nAn O2-PLS model can be written as:\n\\[\n\\begin{align*}\nX =& TP^T + T_{\\text{Y-orth}}P^T_{\\text{Y-orth}} + E \\\\\nY =& UC^T + U_{\\text{X-orth}}C^T_{\\text{X-orth}} + F\n\\end{align*}\n\\]\nO2-PLS separates \\(X\\) and \\(Y\\) into three parts: (1) a joint part (correlation between \\(X\\) and \\(Y\\)), (2) an orthogonal part — unrelated latent variation in \\(X\\) and \\(Y\\) separately— ,and (3) a noise part."
  },
  {
    "objectID": "blog/posts/post1/index.html#example-libs-spectra",
    "href": "blog/posts/post1/index.html#example-libs-spectra",
    "title": "An Overview of Orthogonal Partial Least Squares",
    "section": "",
    "text": "For comparison purposes, I used the LIBS spectra of plant materials to predict the concentration of potassium using the PLS and OPLS approaches. PLS was performed using the R package caret. Since OPLS is not available in caret, I used the package ropls instead.\n\n\n\nFIG. 3: LIBS spectra of five plant samples. A baseline offset has been applied for easier viewing of the spectra.\n\n\nFigures below show the \\(X\\)-scores scatter plots (projection of the \\(X\\)-space) for PLS (left panel) and OPLS (right panel) modeling. In both figures, the \\(x\\)-axis represents the first component (or latent variable), whereas the \\(y\\)-axis represents the second component for PLS and the first orthogonal component for OPLS. The rainbow colors represent the concentration of potassium (K) in each sample, from 1% (dark blue) to 7% (dark red). As we can see, OPLS only requires one component to correlate variation in the samples with K concentration (see the black arrow).\n\n\n\nFIG. 4: Comparison between the \\(X\\)-scores scatter plots for the PLS and OPLS models."
  },
  {
    "objectID": "blog/posts/post1/index.html#summary",
    "href": "blog/posts/post1/index.html#summary",
    "title": "An Overview of Orthogonal Partial Least Squares",
    "section": "",
    "text": "So in summary, the most important thing to remember is that OPLS (or O2-PLS) offers a valuable preprocessing tool, and will help to generate more efficient predictive models, especially in situations where structured noise dominates."
  },
  {
    "objectID": "photography.html",
    "href": "photography.html",
    "title": "Photography",
    "section": "",
    "text": "“To me, photography is an art of observation. It’s about finding something interesting in an ordinary place… I’ve found it has little to do with the things you see and everything to do with the way you see them.”\n– Elliott Erwitt\n\n\n\n\nLandscape Photography\n\n \n\n\n\n\n\n\nCity Photography\n\n  \n\n\n\n\n\n\nBlack and White Photography"
  },
  {
    "objectID": "research.html",
    "href": "research.html",
    "title": "Research",
    "section": "",
    "text": "Designing a highly compact, long-path tunable mid-infrared diode laser system for the detection and quantification of trace gases, with applications in environmental monitoring.\n\n\n\n\n\n\nLed research to enhance quantitative analysis of agricultural soils, with a focus on mitigating matrix interference through LIBS combined with advanced chemometric modeling. This work aims to support sustainable agricultural practices.\n\n\n\nDeveloped a fiber-optic LIBS system integrated with chemometrics for online and real-time monitoring of electrolyte solutes in clinical and point-of-care (POC) settings.\n\n\n\n\n\n\n\n\n\nDeveloped a LIBS-based benchtop instrument for the quantitative analysis of agricultural soils, leveraging machine learning and chemometrics to interpret complex spectral data. This research was highlighted in the Journal of Analytical Atomic Spectrometry on the back cover of Volume 34, Issue 8 (2019), for its contributions to soil texture prediction.\n\n\n\nAn example of LIBS spectra recorded from three soil types: sandy, silty, and clayey.\n\n\n\n\n\nPrincipal component analysis (PCA) biplot of spectral data. Sandy soils (grey) and clayey soils (blue) clusters.\n\n\n\n\n\n\nDeveloped a compact, robust LIBS sensor for in-situ monitoring of CCS activities in extreme environments (high temperature, pressure, and salinity). One of our research projects earned the prestigious distinction of being featured on the front cover of the Journal of Analytical Atomic Spectrometry (Volume 31, Issue 7, 2016). This study explored the application of underwater LIBS for real-time, in situ monitoring of carbonate mineral dissolution under rising CO2 pressure.\n\n\n\nUnderwater LIBS experimental setup. Measurements were taken in a controlled, high-pressure environment, replicating up to 400 bar (≈ 6,000 psi) conditions in carbonated brine environments. Quantitative analysis of key elements (Mg, Ca, K, Ba, Mn, Sr, etc.) in brine.\n\n\n\n\n\nLIBS for CCS: Signal-to-noise ratio for key emission lines.\n\n\n\n\n\nNETL Team: During my time at NETL, I had the privilege of collaborating with a group of exceptional, experienced researchers. Pictured from left to right in the second row are Dustin McIntyre, Harry Edenborn, Jinesh Jain, and Christian Goueguel.\n\n\n\n\n\n\nResearch focused on improving the limit of detection (LOD) for trace impurities in metallic alloys using selective wavelength approaches in laser-produced plasma. Explored LA-LIF, resonance-enhance LIBS, and RLA-LIF approaches for enhanced LODs in impurity analysis. Proposed efficient excitation-fluorescence schemes for targeted element detection in RLA-LIF. Our research on the investigation of resonance-enhance LIBS was featured on the inside front cover of the Journal of Analytical Atomic Spectrometry, Volume 25, Issue 5 (2010).\n\n\n\nLA-LIF and resonance-enhance LIBS experimental setup. A first Nd:YAG pulse generates low-density vapor, followed by a second tunable pulse targeting a resonant transition of an analyte (LA-LIF), or of a matrix element (resonance-enhance LIBS).\n\n\n\n\n\n(upper panel) Confocal microscopy images of resonance-enhance LIBS sample damage at three ablation laser fluences. (lower panel) Resonance-enhance LIBS is especially advantageous when minimal surface damage is critical, as it ablates approximately 10 times less material per shot than standard LIBS.\n\n\n\n\n\nPartial Grotrian diagram illustrating the excitation and fluorescence emission processes of lead, as used in LA-LIF and RLA-LIF experiments.\n\n\n\n\n\n(left panel) RLA-LIF improves detection sensitivity by approximately 1 order of magnitude at a significantly lower laser fluence compared to standard LIBS. (right panel) Optical coherence tomography (OCT)-based 3D plots and scanning electron microscope (SEM) images of sample ablation craters in RLA-LIF."
  },
  {
    "objectID": "research.html#current-research",
    "href": "research.html#current-research",
    "title": "Research",
    "section": "",
    "text": "Designing a highly compact, long-path tunable mid-infrared diode laser system for the detection and quantification of trace gases, with applications in environmental monitoring."
  },
  {
    "objectID": "research.html#past-research",
    "href": "research.html#past-research",
    "title": "Research",
    "section": "",
    "text": "Led research to enhance quantitative analysis of agricultural soils, with a focus on mitigating matrix interference through LIBS combined with advanced chemometric modeling. This work aims to support sustainable agricultural practices.\n\n\n\nDeveloped a fiber-optic LIBS system integrated with chemometrics for online and real-time monitoring of electrolyte solutes in clinical and point-of-care (POC) settings.\n\n\n\n\n\n\n\n\n\nDeveloped a LIBS-based benchtop instrument for the quantitative analysis of agricultural soils, leveraging machine learning and chemometrics to interpret complex spectral data. This research was highlighted in the Journal of Analytical Atomic Spectrometry on the back cover of Volume 34, Issue 8 (2019), for its contributions to soil texture prediction.\n\n\n\nAn example of LIBS spectra recorded from three soil types: sandy, silty, and clayey.\n\n\n\n\n\nPrincipal component analysis (PCA) biplot of spectral data. Sandy soils (grey) and clayey soils (blue) clusters.\n\n\n\n\n\n\nDeveloped a compact, robust LIBS sensor for in-situ monitoring of CCS activities in extreme environments (high temperature, pressure, and salinity). One of our research projects earned the prestigious distinction of being featured on the front cover of the Journal of Analytical Atomic Spectrometry (Volume 31, Issue 7, 2016). This study explored the application of underwater LIBS for real-time, in situ monitoring of carbonate mineral dissolution under rising CO2 pressure.\n\n\n\nUnderwater LIBS experimental setup. Measurements were taken in a controlled, high-pressure environment, replicating up to 400 bar (≈ 6,000 psi) conditions in carbonated brine environments. Quantitative analysis of key elements (Mg, Ca, K, Ba, Mn, Sr, etc.) in brine.\n\n\n\n\n\nLIBS for CCS: Signal-to-noise ratio for key emission lines.\n\n\n\n\n\nNETL Team: During my time at NETL, I had the privilege of collaborating with a group of exceptional, experienced researchers. Pictured from left to right in the second row are Dustin McIntyre, Harry Edenborn, Jinesh Jain, and Christian Goueguel.\n\n\n\n\n\n\nResearch focused on improving the limit of detection (LOD) for trace impurities in metallic alloys using selective wavelength approaches in laser-produced plasma. Explored LA-LIF, resonance-enhance LIBS, and RLA-LIF approaches for enhanced LODs in impurity analysis. Proposed efficient excitation-fluorescence schemes for targeted element detection in RLA-LIF. Our research on the investigation of resonance-enhance LIBS was featured on the inside front cover of the Journal of Analytical Atomic Spectrometry, Volume 25, Issue 5 (2010).\n\n\n\nLA-LIF and resonance-enhance LIBS experimental setup. A first Nd:YAG pulse generates low-density vapor, followed by a second tunable pulse targeting a resonant transition of an analyte (LA-LIF), or of a matrix element (resonance-enhance LIBS).\n\n\n\n\n\n(upper panel) Confocal microscopy images of resonance-enhance LIBS sample damage at three ablation laser fluences. (lower panel) Resonance-enhance LIBS is especially advantageous when minimal surface damage is critical, as it ablates approximately 10 times less material per shot than standard LIBS.\n\n\n\n\n\nPartial Grotrian diagram illustrating the excitation and fluorescence emission processes of lead, as used in LA-LIF and RLA-LIF experiments.\n\n\n\n\n\n(left panel) RLA-LIF improves detection sensitivity by approximately 1 order of magnitude at a significantly lower laser fluence compared to standard LIBS. (right panel) Optical coherence tomography (OCT)-based 3D plots and scanning electron microscope (SEM) images of sample ablation craters in RLA-LIF."
  },
  {
    "objectID": "talks.html",
    "href": "talks.html",
    "title": "Talks",
    "section": "",
    "text": "Photo by Susan Q Yin."
  },
  {
    "objectID": "talks.html#oralposter-presentations",
    "href": "talks.html#oralposter-presentations",
    "title": "Home",
    "section": "Oral/Poster Presentations",
    "text": "Oral/Poster Presentations\n\nC. L. Goueguel, Direct determination of soil texture using laser-induced breakdown spectroscopy and multivariate linear regressions. Annual meetings of the North American Society for Laser-Induced Breakdown Spectroscopy (NASLIBS) hosted by FACSS SciX Conference 2019 (Palm Springs, CA, USA)\nC. L. Goueguel, Using LIBS analyzer for laser-based soil analysis for precision agriculture. Invited speaker at the Canadian Society for Analytical Sciences and Spectroscopy, Jan. 1, 2018 (Toronto, ON, Canada)\nJ. C. Jain, C. L. Goueguel, D. L. McIntyre, H. M. Edenborn, Dissolution of mineral carbonates with increasing CO2 pressure and the implications for carbon sequestration leak detection. GSA annual meeting, 2016 (Denver, CO, USA)\nC. G. Carson, C. Goueguel, J. Jain, D. McIntyre, Development of laser induced breakdown spectroscopy sensor to assess groundwater quality impacts resulting from geologic carbon sequestration. SPIE Micro- and Nanotechnology Sensors, Systems, and Applications VII, 2015 (Baltimore, MD, SA)\nC. L. Goueguel, J. C. Jain, D. L. McIntyre, A. K. Karamalidis, Application of laser induced breakdown spectroscopy (LIBS) to analyze CO2-bearing solutions in high pressure high temperature environment. ACS 248th - National Meeting, 2014 (San Francisco, CA, USA)\nC. L. Goueguel, J. C. Jain, A. K. Karamalidis, D. L. McIntyre, J. P. Singh, Laser-induced breakdown spectroscopy of high-pressure carbonated brine solutions. 2014 PITTCON Conference & Expo, 2014 (Chicago, IL, USA)\nC. L. Goueguel, D. L. McIntyre, J. C. Jain, J. P. Singh, A. K. Karamalidis, Analysis of calcium in CO2-laden brine (NaCl-CaCl2) by Laser-induced breakdown spectroscopy (LIBS). Annual meetings of the North American Society for Laser-Induced Breakdown Spectroscopy (NASLIBS) hosted by FACSS SciX Conference 2013 (Milwaukee, WI, USA)\nJ. P. Singh, C L. Goueguel, D. L. McIntyre, J. C. Jain, A. K. Karamalidis, Laser-induced breakdown spectroscopy for elemental analysis of brines. 7th International Conference on Laser-Induced Breakdown Spectroscopy, 2012 (Luxor, Egypt)\nC. Goueguel, Laser Induced Breakdown Spectroscopy (LIBS): From Mars exploration to GCS monitoring. Postdoctoral Seminar at the Department of Civil and Environmental Engineering, Carnegie Mellon University, 2012 (Pittsburgh, PA, USA)\nC. Goueguel, S. Laville, F. Vidal, M. Sabsabi, M. Chaker, Investigation of resonance-enhanced laser-induced breakdown spectroscopy (RELIBS) for the analysis of aluminum alloys. 6th International Conference on Laser- Induced Breakdown Spectroscopy, 2010 (Memphis, TN, USA)\nC. Goueguel, S. Laville, F. Vidal, M. Sabsabi, Chaker, Investigation of resonance-enhanced laser-induced breakdown spectroscopy (RELIBS) for the analysis of aluminum alloys, PITTCON Conference & Expo, 2010 (Orlando, FL, USA)\nC. Goueguel, S. Laville, H. Loudyi, F. Vidal, M. Sabsabi, M. Chaker, Investigation of resonance-enhanced laser-induced breakdown spectroscopy for the analysis of aluminum alloys. 2nd North American Symposium on Laser-Induced Breakdown Spectroscopy, 2009 (New Orleans, LA, USA)\nC. Goueguel, H. Loudyi, S. Laville, M. Chaker, M. Sabsabi, F. Vidal, Excitation spectrale sélective d’un plasma d’aluminium par une seconde impulsion laser: Optimisation des paramètres expérimentaux. Colloque Plasma- Québec, 2009 (Montreal, QC, Canada)\nS. Laville, K. Rifai, H. Loudyi, M. Chaker, M. Sabsabi, F. Vidal, C. Goueguel, Analysis of trace elements in liquids using LIBS combined with laser-induced fluorescence. 5th International Conference on Laser-Induced Breakdown Spectroscopy, 2008 (Berlin, Germany)\nF. Vidal, M. Chaker, C. Goueguel, S. Laville, H. Loudyi, K. Rifai, M. Sabsabi, Enhanced laser-induced breakdown spectroscopy by second pulse selective wavelength excitation. Laser and plasma applications in materials science: 1st International Conference on Laser Plasma Applications in Materials Science, LAPAMS, 2008 (Alger, Algeria)\nC. Goueguel, S. Laville, H. Loudyi, M. Chaker, M. Sabsabi, F. Vidal. Detection of lead in brass by Laser-Induced Breakdown Spectroscopy combined with Laser-Induced Fluorescence. Photonics North, 2008 (Montreal, QC, Canada)\nH. Loudyi, C. Goueguel, K. Rifai, S. Laville, M. Sabsabi, M. Chaker, F. Vidal, Enhancing the sensitivity of the laser-Induced Breakdown Spectroscopy technique: spectrally selective excitation of specific elements in laser produced plasma. Canadian Association of Physicists Congress, 2008 (Laval, QC, Canada)\nC. Goueguel, H. Loudyi, S. Laville, M. Chaker, M. Sabsabi, F. Vidal, Analyse des matériaux solides par LIBS: Effet d’une deuxième impulsion laser basée sur une excitation spectrale sélective. Colloque Plasma-Québec, 2008 (Montreal, QC, Canada)\nF. Vidal, S. Laville, C. Goueguel, H. Loudyi, M. Chaker, M. Sabsabi, Investigation of laser-induced breakdown spectroscopy combined with laser- induced fluorescence. 4th Euro-Mediterranean Symposium on Laser-Induced Breakdown Spectroscopy, 2007 (Paris, France)"
  },
  {
    "objectID": "projects/index.html",
    "href": "projects/index.html",
    "title": "R Packages",
    "section": "",
    "text": "Hotelling’s T-Squared Statistic and Ellipse\n\n\n\nR\n\nMachine Learning\n\nChemometrics\n\nExploratory Data Analysis\n\n\n\nThe HotellingEllipse package facilitates the comparison of multivariate datasets based on their PCA or PLS scores by computing the Hotelling’s T-squared statistic. The package also provides the semi-minor and semi-major axes for drawing…\n\n\n\nMay 18, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\nComputation of 2D and 3D Elliptical Joint Confidence Regions\n\n\n\nR\n\nMachine Learning\n\nChemometrics\n\nClustering\n\n\n\nThe ConfidenceEllipse package calculates the coordinates of elliptical joint regions at a specified confidence level. It provides the flexibility to estimate classical or robust confidence regions, which can be visualized as 2D or 3D plots.…\n\n\n\nApr 22, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\nPreprocessing Tools for Spectroscopy Data Analysis\n\n\n\nR\n\nVisualization\n\nChemometrics\n\nPreprocessing\n\nExploratory Data Analysis\n\n\n\nThe specProc package performs a wide range of preprocessing tasks essential for spectroscopic data analysis. Spectral preprocessing is essential in ensuring accurate and reliable results by minimizing the impact of various distortions and…\n\n\n\nNov 1, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\nCRAN Package Usage App\n\n\n\nR\n\nVisualization\n\nApp\n\n\n\nShiny app that provides an interactive way to explore CRAN package usage. It enables users to display detailed download trends and compare up to 20 packages simultaneously.\n\n\n\nDec 1, 2024\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blog/posts/post2/index.html",
    "href": "blog/posts/post2/index.html",
    "title": "Multivariate Outlier Detection in High-Dimensional Spectral Data",
    "section": "",
    "text": "Photo by Will Myers.\n\n\n\n\nIn the realm of laser spectroscopy, outliers are usually characterized by values that deviate significantly from the values of other observations, abnormally high or low — for example, humidity change, spatial heterogeneity on the samples surface, drift of the instrument parameters, human error, etc. Or by a subtle change in the characteristics of the spectra not taken into account before, since calibration models are usually trained on samples with relatively “similar” chemical and physical properties — what we call matrix effects.\n\n“An outlier is an observation which deviates so much from the other observations as to arouse suspicions that it was generated by a different mechanism.” D. M. Hawkins\n\nSolutions for the former case — referred to as gross outliers or extreme values — are often straightforward: we fix what was wrong and repeat the measurements if necessary. The latter case is usually more complex and requires advanced statistical methods. However, as discussed later, these subtle changes are often very difficult to detect with traditional methods in addition to being ineffective in high dimensions. The challenge is therefore to find reliable methods which are both:\n\nFast\nRobust to outliers or contamination\nApplicable to high-dimensional data\n\nIn high-dimensional space, the data points are very sparse, so that all points are almost equidistant from each other. In other words, the use of Euclidean distances become meaningless. The result is that the degree of outlyingness of data points is indistinguishable from each other. For this reason, outliers are best detected by using a lower-dimensional local subspace in which only a subset of features may be relevant.\n\n“Outliers are often hidden in the unusual local behavior of low-dimensional subspaces, and this deviant behavior is masked by full-dimensional analysis.” C. C. Aggarwal\n\n\n\n\nStandard and widely used distance-based methods consist of computing the Mahalanobis distance. This class of methods only uses distance space to flag outlier observations. The Mahalanobis distance (MD) for the \\(i\\)-th observation is given by:\n\\[\n\\text{MD}_i = \\sqrt{(x_i-\\bar{x})^T C^{-1} (x_i-\\bar{x})} \\quad \\text{with} \\quad C = \\frac{1}{n-1}X^TX\n\\]\n\\(X\\) is the data matrix of size \\(n \\times p\\), where \\(p\\)is the number of variables and n is the number of observations. \\(x_i\\) is an observation (a row of \\(X\\)), \\(\\bar{x}\\) is the mean vector, \\(C\\) is the sample covariance matrix which gives information about the covariance structure of the data — i.e. the shape of the ellipsoid specified by the covariance matrix.\n\n\n\nFIG. 1: Robust Mahalanobis distance versus the sample (observation) number.\n\n\nTo identify outlier candidates, MD² is computed and compared to a cut-off value equal to the 0.975 quantile of the Chi-Square distributionwith m degrees of freedom, m being the number of variables. This comes from the fact that MD² of multivariate normal data follows a Chi-Square distribution. Albeit, Hardin and Rocke (2005) have reported that a better approximation is found using an F-distribution, especially for small sample sizes. Nonetheless, Chi-Square distribution is still reasonably good for approximating the squared Mahalanobis distance. Hence, an observation is considered as an outlier candidate if:\n\\[\n\\text{MD}^2 = \\chi^2_{m,0.975}\n\\]\nHowever, this approach has two major issues: (1) the arithmetic mean and the sample covariance matrix are sensitive to outliers and (2) the covariance matrix \\(X^TX\\) must be invertible — more formally non singular. The former is solved by applying robust statistics, whilst the latter is obviously a severe limitation for high-dimensional data often encountered in chemometrics where \\(p \\ggg n\\). Indeed, the same limitation holds even when one chooses to use the robust version of MD, which is derived from the Minimum Covariance Determinant (MCD) estimator, instead of the classical MD.\n\n\n\nIn chemometrics, Principal Component Analysis (PCA) is widely used for exploratory analysis and for dimensionality reduction and can be used as outlier detection method.\nIndeed, PCA score is often used in conjunction with the Mahalanobis distance (or Hotelling’s \\(T^2\\) at 95% confidence level) to determine how far an observation is from the centroid of the elliptical region that encompasses most of the observations.\n\n\n\nFIG. 2: PCA score for the LIBS spectra of plant samples with the 95% confidence limit for Hotelling’s \\(T^2\\). The transparency level of the data points shows the contribution of each sample to the first component. The darkest samples are those that contribute most.\n\n\nHowever, this approach is not robust since classical PCA relies on the sample covariance matrix C, and therefore suffers from being extremely sensitive to outliers observations, as can be seen in the above figure. The consequences of this are twofold: on one hand, the variance explained by the principal components is inflated towards the outliers direction and therefore masking observations that deviate from the regular observations — the so-called masking effect. On the other hand, regular observations might be falsely flagged as outliers — the so-called swamping effect.\n\n\n\nFIG. 3: Same PCA score but with the addition of two colors: (red) with and (blue) without measurements error. The former (70 samples) are indistinguishable from error-free measurements (369 samples), although they seem to cluster in the upper right corner which may indicate a similar error.\n\n\nHence, to avoid these effects, a number of robustified versions of PCA based upon a robust estimate of the covariance matrix (each characterized by their breakdown point), by means of S-estimator, MM-estimator, (Fast)MCD-estimator or, the re-weighted MCD- (RMCD) estimator have been proposed over the past decades. However, these methods are limited to small or moderate dimensions since the robust estimates are only applicable when the number of observations is at least twice the number of variables \\((n﹥2p)\\).\n\n\n\nFIG. 4: Robust PCA (Fast-MCD) score for the plant samples. Prior to perform robust PCA, feature selection was carried out to reduce the dimensionality. Here, we can better differentiate the measurements with (red) and without (blue) error. It turns out that the former contribute the most (dark red) to the first component.\n\n\n\n\n\nFIG. 5: 3D score plot for the plant samples. Now we can see that the red samples form two separate clusters. The first is clearly separated from error-free measurements, while the second is still difficult to discern but clusters above the regular values.\n\n\nThere is another group of robust PCA methods that have been developed and that are better suited for handling high-dimensional data in the situation where the sample size is lower than the dimension \\((p \\ggg n)\\). These methods are:\n\nRobust PCA by projection-pursuit (PP-PCA)\nSpherical PCA (SPCA)\nRobust PCA (ROBPCA)\nRobust Sparse PCA (ROSPCA)\n\nThe projection-pursuit approach to robust PCA has been initially introduced by Li and Chen (1985) and is based on finding the directions that maximize a projection index. PP-PCA uses the median absolute deviation (MAD) or Qn-estimator as projection index instead of the variance. SPCA was derived by Locantore et al. (1999) and aims to project data onto the surface of a unit sphere to alleviate outliers effect. Like PP-PCA, SPCA uses MAD or Qn-estimator as a robust estimate of spread.\nMore recently, ROBPCA and ROSPCA have been introduced by Hubert et al. 2005 and by Hubert et al. 2015. ROBPCA combines the projection pursuit approach with the robust covariance estimation based on the RMCD in a low dimensional space. Interestingly, ROBPCA is much faster than the aforementioned methods and has the advantage to be applicable to both symmetrically distributed data and skewed data (Hubert et al. 2009). On the other hand, ROSPCA is derived from ROBPCA, but uses Sparse PCA (Zou et al. 2006) instead of PCA.\n\n\n\nAfter having briefly reviewed outlier detection methods based upon robust PCA, it is worth mentioning that outliers can be classified into two categories: Leverage points and Orthogonal outliers. As the figure below illustrates, the leverage points category can be split into Good leverage points and Bad leverage points. As their name suggests, one can have a positive effect while the other has a negative effect.\n\n\n\nFIG. 6: Illustration of the different types of outliers on the 2-dimensional PCA subspace. Image credit: Christian Goueguel.\n\n\nLeverage points and orthogonal outliers are differentiated by their respective scores and orthogonal distances. These distances tell us how far an observation is from the center of the ellipse defined by the regular observations (score = 0). The score distance (SD) is a measure of the distance between an observation belonging to the \\(k\\)-dimensional PCA subspace and the origin of that subspace. The orthogonal distance (OD) measures the deviation — i.e. lack of fit — of an observation from the \\(k\\)-dimensional PCA subspace.\n\n\n\nFIG. 7: Illustration of the score and orthogonal distances. Image credit: Christian Goueguel.\n\n\nThus, leverage points are characterized by a high score distance, while orthogonal outliers are characterized by a high orthogonal distance. Likewise, good and bad leverage points are differentiated by their respective orthogonal distance. Bad leverage points display higher orthogonal distance than good leverage points.\nThe score distance for the \\(i\\)-th observation on the \\(k\\)-dimensional PCA subspace is given by:\n\\[\n\\text{SD}^2_i = \\sum^k_{j=1}{\\frac{\\text{t}^2_{ij}}{l_j}}\n\\]\nwhere \\(l\\) are the eigenvalues of the MCD scatter matrix and t are the robust scores, for each \\(j = 1, \\cdots, k\\). Like the Mahalanobis distance, the cut-off values for outlying the squared score distances are obtained from the 0.975 quantile of the Chi-Square distributionwith \\(k\\)degrees of freedom.\n\\[\n\\text{SD}^2 &gt;  \\chi^2_{k,0.975}\n\\]\nThe corresponding orthogonal distance is given by:\n\\[\n\\text{OD}_i = \\| x_i-\\hat{\\mu}_x - P\\times t_i \\|\n\\]\nwhere \\(P\\) is the loading matrix, ûₓ is the robust estimate of center. The cut-off values for the orthogonal distances are obtained using the Wilson-Hilferty approximation for a Chi-Squared distribution. As a result, the orthogonal distances, raised to the power 2/3, are approximately normally distributed. Thus, the cut-off values for outliers observations are given by:\n\\[\n\\text{OD} &gt; \\left[ \\text{median}\\left(\\text{OD}^{2/3} \\right) + Z_{0.975}\\text{MAD}\\left(\\text{OD}^{2/3} \\right)  \\right]^{3/2}\n\\]\nwhere \\(Z_{0.975}\\) is the 0.975 quantile of the Standard Normal distribution.\n\n\n\nOutlier map provides a powerful graphical tool for visually identifying the different types of outliers.\n\n\n\nFIG. 8: Illustration of outlier map.\n\n\nFor each observation in the dataset, the score distance and the orthogonal distance are plotted on the x- and y-axis. Their respective cut-off values divide the map into four quadrants:\n\nRegular observations are in the bottom left corner of the map,\nOrthogonal outliers are in the upper left corner of the map,\nLeverage points are on the right side, with good leverage points at the bottom and bad leverage points at the top.\n\n\n\n\nFIG. 9: Outlier map obtained from ROBPCA using LIBS spectra of plant samples.\n\n\nThe above figure was obtained with ROBPCA. It took 15.86 seconds of execution time (laptop, 2.80GHz) to process the data matrix of size 439×7153. We can see that most of the measurements are in the quadrant of regular observations while some are flagged as orthogonal outliers and bad leverage points. Some of them are good leverage points. On the other hand, the measurements with error are mostly orthogonal outliers, although some are considered regular observations."
  },
  {
    "objectID": "blog/posts/post2/index.html#introduction",
    "href": "blog/posts/post2/index.html#introduction",
    "title": "Multivariate Outlier Detection in High-Dimensional Spectral Data",
    "section": "",
    "text": "In the realm of laser spectroscopy, outliers are usually characterized by values that deviate significantly from the values of other observations, abnormally high or low — for example, humidity change, spatial heterogeneity on the samples surface, drift of the instrument parameters, human error, etc. Or by a subtle change in the characteristics of the spectra not taken into account before, since calibration models are usually trained on samples with relatively “similar” chemical and physical properties — what we call matrix effects.\n\n“An outlier is an observation which deviates so much from the other observations as to arouse suspicions that it was generated by a different mechanism.” D. M. Hawkins\n\nSolutions for the former case — referred to as gross outliers or extreme values — are often straightforward: we fix what was wrong and repeat the measurements if necessary. The latter case is usually more complex and requires advanced statistical methods. However, as discussed later, these subtle changes are often very difficult to detect with traditional methods in addition to being ineffective in high dimensions. The challenge is therefore to find reliable methods which are both:\n\nFast\nRobust to outliers or contamination\nApplicable to high-dimensional data\n\nIn high-dimensional space, the data points are very sparse, so that all points are almost equidistant from each other. In other words, the use of Euclidean distances become meaningless. The result is that the degree of outlyingness of data points is indistinguishable from each other. For this reason, outliers are best detected by using a lower-dimensional local subspace in which only a subset of features may be relevant.\n\n“Outliers are often hidden in the unusual local behavior of low-dimensional subspaces, and this deviant behavior is masked by full-dimensional analysis.” C. C. Aggarwal"
  },
  {
    "objectID": "blog/posts/post2/index.html#mahalanobis-distance",
    "href": "blog/posts/post2/index.html#mahalanobis-distance",
    "title": "Multivariate Outlier Detection in High-Dimensional Spectral Data",
    "section": "",
    "text": "Standard and widely used distance-based methods consist of computing the Mahalanobis distance. This class of methods only uses distance space to flag outlier observations. The Mahalanobis distance (MD) for the \\(i\\)-th observation is given by:\n\\[\n\\text{MD}_i = \\sqrt{(x_i-\\bar{x})^T C^{-1} (x_i-\\bar{x})} \\quad \\text{with} \\quad C = \\frac{1}{n-1}X^TX\n\\]\n\\(X\\) is the data matrix of size \\(n \\times p\\), where \\(p\\)is the number of variables and n is the number of observations. \\(x_i\\) is an observation (a row of \\(X\\)), \\(\\bar{x}\\) is the mean vector, \\(C\\) is the sample covariance matrix which gives information about the covariance structure of the data — i.e. the shape of the ellipsoid specified by the covariance matrix.\n\n\n\nFIG. 1: Robust Mahalanobis distance versus the sample (observation) number.\n\n\nTo identify outlier candidates, MD² is computed and compared to a cut-off value equal to the 0.975 quantile of the Chi-Square distributionwith m degrees of freedom, m being the number of variables. This comes from the fact that MD² of multivariate normal data follows a Chi-Square distribution. Albeit, Hardin and Rocke (2005) have reported that a better approximation is found using an F-distribution, especially for small sample sizes. Nonetheless, Chi-Square distribution is still reasonably good for approximating the squared Mahalanobis distance. Hence, an observation is considered as an outlier candidate if:\n\\[\n\\text{MD}^2 = \\chi^2_{m,0.975}\n\\]\nHowever, this approach has two major issues: (1) the arithmetic mean and the sample covariance matrix are sensitive to outliers and (2) the covariance matrix \\(X^TX\\) must be invertible — more formally non singular. The former is solved by applying robust statistics, whilst the latter is obviously a severe limitation for high-dimensional data often encountered in chemometrics where \\(p \\ggg n\\). Indeed, the same limitation holds even when one chooses to use the robust version of MD, which is derived from the Minimum Covariance Determinant (MCD) estimator, instead of the classical MD."
  },
  {
    "objectID": "blog/posts/post2/index.html#robust-principal-component-analysis",
    "href": "blog/posts/post2/index.html#robust-principal-component-analysis",
    "title": "Multivariate Outlier Detection in High-Dimensional Spectral Data",
    "section": "",
    "text": "In chemometrics, Principal Component Analysis (PCA) is widely used for exploratory analysis and for dimensionality reduction and can be used as outlier detection method.\nIndeed, PCA score is often used in conjunction with the Mahalanobis distance (or Hotelling’s \\(T^2\\) at 95% confidence level) to determine how far an observation is from the centroid of the elliptical region that encompasses most of the observations.\n\n\n\nFIG. 2: PCA score for the LIBS spectra of plant samples with the 95% confidence limit for Hotelling’s \\(T^2\\). The transparency level of the data points shows the contribution of each sample to the first component. The darkest samples are those that contribute most.\n\n\nHowever, this approach is not robust since classical PCA relies on the sample covariance matrix C, and therefore suffers from being extremely sensitive to outliers observations, as can be seen in the above figure. The consequences of this are twofold: on one hand, the variance explained by the principal components is inflated towards the outliers direction and therefore masking observations that deviate from the regular observations — the so-called masking effect. On the other hand, regular observations might be falsely flagged as outliers — the so-called swamping effect.\n\n\n\nFIG. 3: Same PCA score but with the addition of two colors: (red) with and (blue) without measurements error. The former (70 samples) are indistinguishable from error-free measurements (369 samples), although they seem to cluster in the upper right corner which may indicate a similar error.\n\n\nHence, to avoid these effects, a number of robustified versions of PCA based upon a robust estimate of the covariance matrix (each characterized by their breakdown point), by means of S-estimator, MM-estimator, (Fast)MCD-estimator or, the re-weighted MCD- (RMCD) estimator have been proposed over the past decades. However, these methods are limited to small or moderate dimensions since the robust estimates are only applicable when the number of observations is at least twice the number of variables \\((n﹥2p)\\).\n\n\n\nFIG. 4: Robust PCA (Fast-MCD) score for the plant samples. Prior to perform robust PCA, feature selection was carried out to reduce the dimensionality. Here, we can better differentiate the measurements with (red) and without (blue) error. It turns out that the former contribute the most (dark red) to the first component.\n\n\n\n\n\nFIG. 5: 3D score plot for the plant samples. Now we can see that the red samples form two separate clusters. The first is clearly separated from error-free measurements, while the second is still difficult to discern but clusters above the regular values.\n\n\nThere is another group of robust PCA methods that have been developed and that are better suited for handling high-dimensional data in the situation where the sample size is lower than the dimension \\((p \\ggg n)\\). These methods are:\n\nRobust PCA by projection-pursuit (PP-PCA)\nSpherical PCA (SPCA)\nRobust PCA (ROBPCA)\nRobust Sparse PCA (ROSPCA)\n\nThe projection-pursuit approach to robust PCA has been initially introduced by Li and Chen (1985) and is based on finding the directions that maximize a projection index. PP-PCA uses the median absolute deviation (MAD) or Qn-estimator as projection index instead of the variance. SPCA was derived by Locantore et al. (1999) and aims to project data onto the surface of a unit sphere to alleviate outliers effect. Like PP-PCA, SPCA uses MAD or Qn-estimator as a robust estimate of spread.\nMore recently, ROBPCA and ROSPCA have been introduced by Hubert et al. 2005 and by Hubert et al. 2015. ROBPCA combines the projection pursuit approach with the robust covariance estimation based on the RMCD in a low dimensional space. Interestingly, ROBPCA is much faster than the aforementioned methods and has the advantage to be applicable to both symmetrically distributed data and skewed data (Hubert et al. 2009). On the other hand, ROSPCA is derived from ROBPCA, but uses Sparse PCA (Zou et al. 2006) instead of PCA."
  },
  {
    "objectID": "blog/posts/post2/index.html#different-types-of-outliers",
    "href": "blog/posts/post2/index.html#different-types-of-outliers",
    "title": "Multivariate Outlier Detection in High-Dimensional Spectral Data",
    "section": "",
    "text": "After having briefly reviewed outlier detection methods based upon robust PCA, it is worth mentioning that outliers can be classified into two categories: Leverage points and Orthogonal outliers. As the figure below illustrates, the leverage points category can be split into Good leverage points and Bad leverage points. As their name suggests, one can have a positive effect while the other has a negative effect.\n\n\n\nFIG. 6: Illustration of the different types of outliers on the 2-dimensional PCA subspace. Image credit: Christian Goueguel.\n\n\nLeverage points and orthogonal outliers are differentiated by their respective scores and orthogonal distances. These distances tell us how far an observation is from the center of the ellipse defined by the regular observations (score = 0). The score distance (SD) is a measure of the distance between an observation belonging to the \\(k\\)-dimensional PCA subspace and the origin of that subspace. The orthogonal distance (OD) measures the deviation — i.e. lack of fit — of an observation from the \\(k\\)-dimensional PCA subspace.\n\n\n\nFIG. 7: Illustration of the score and orthogonal distances. Image credit: Christian Goueguel.\n\n\nThus, leverage points are characterized by a high score distance, while orthogonal outliers are characterized by a high orthogonal distance. Likewise, good and bad leverage points are differentiated by their respective orthogonal distance. Bad leverage points display higher orthogonal distance than good leverage points.\nThe score distance for the \\(i\\)-th observation on the \\(k\\)-dimensional PCA subspace is given by:\n\\[\n\\text{SD}^2_i = \\sum^k_{j=1}{\\frac{\\text{t}^2_{ij}}{l_j}}\n\\]\nwhere \\(l\\) are the eigenvalues of the MCD scatter matrix and t are the robust scores, for each \\(j = 1, \\cdots, k\\). Like the Mahalanobis distance, the cut-off values for outlying the squared score distances are obtained from the 0.975 quantile of the Chi-Square distributionwith \\(k\\)degrees of freedom.\n\\[\n\\text{SD}^2 &gt;  \\chi^2_{k,0.975}\n\\]\nThe corresponding orthogonal distance is given by:\n\\[\n\\text{OD}_i = \\| x_i-\\hat{\\mu}_x - P\\times t_i \\|\n\\]\nwhere \\(P\\) is the loading matrix, ûₓ is the robust estimate of center. The cut-off values for the orthogonal distances are obtained using the Wilson-Hilferty approximation for a Chi-Squared distribution. As a result, the orthogonal distances, raised to the power 2/3, are approximately normally distributed. Thus, the cut-off values for outliers observations are given by:\n\\[\n\\text{OD} &gt; \\left[ \\text{median}\\left(\\text{OD}^{2/3} \\right) + Z_{0.975}\\text{MAD}\\left(\\text{OD}^{2/3} \\right)  \\right]^{3/2}\n\\]\nwhere \\(Z_{0.975}\\) is the 0.975 quantile of the Standard Normal distribution."
  },
  {
    "objectID": "blog/posts/post2/index.html#outlier-map",
    "href": "blog/posts/post2/index.html#outlier-map",
    "title": "Multivariate Outlier Detection in High-Dimensional Spectral Data",
    "section": "",
    "text": "Outlier map provides a powerful graphical tool for visually identifying the different types of outliers.\n\n\n\nFIG. 8: Illustration of outlier map.\n\n\nFor each observation in the dataset, the score distance and the orthogonal distance are plotted on the x- and y-axis. Their respective cut-off values divide the map into four quadrants:\n\nRegular observations are in the bottom left corner of the map,\nOrthogonal outliers are in the upper left corner of the map,\nLeverage points are on the right side, with good leverage points at the bottom and bad leverage points at the top.\n\n\n\n\nFIG. 9: Outlier map obtained from ROBPCA using LIBS spectra of plant samples.\n\n\nThe above figure was obtained with ROBPCA. It took 15.86 seconds of execution time (laptop, 2.80GHz) to process the data matrix of size 439×7153. We can see that most of the measurements are in the quadrant of regular observations while some are flagged as orthogonal outliers and bad leverage points. Some of them are good leverage points. On the other hand, the measurements with error are mostly orthogonal outliers, although some are considered regular observations."
  },
  {
    "objectID": "blog/index.html",
    "href": "blog/index.html",
    "title": "Blog",
    "section": "",
    "text": "Order By\n      Default\n      \n        Date - Oldest\n      \n      \n        Date - Newest\n      \n      \n        Title\n      \n    \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nTensor Decomposition: PARAFAC and Tucker\n\n\n\n\n\nTraditional data analysis methods often rely on matrices, but when applied to complex, multimodal datasets they can struggle to uncover meaningful latent structures. High-order (multiway) data analysis extends beyond the two-dimensional matrix framework, providing more powerful tools for identifying patterns and relationships in multi-dimensional data. In this post, I review two cornerstone techniques (PARAFAC and Tucker) which generalize familiar methods like singular value decomposition (SVD) and bilinear principal component analysis (PCA) to higher-order tensors.\n\n\n\n\n\nAug 23, 2025\n\n\nChristian L. Goueguel\n\n62 min\n\n\n\n\n\n\n\n\n\n\n\nBeyond Standard Boxplot: The Adjusted and Generalized Boxplots\n\n\n\n\n\nBoxplots, also known as box-and-whisker plots, have been a cornerstone of data visualization since their introduction by John Tukey in the late 1970s. Despite their enduring utility, boxplot assumes a symmetrical mesokurtic distribution and might misrepresent datasets with skewness or heavy tails. Alternative approaches have been proposed to address these limitations.\n\n\n\n\n\nDec 10, 2024\n\n\nChristian L. Goueguel\n\n16 min\n\n\n\n\n\n\n\n\n\n\n\nRobust Measures of Tail Weight in Distributions\n\n\n\n\n\nTraditional measures like kurtosis have long been used to capture the weight of the tails of a distribution. However, kurtosis comes with limitations, particularly its sensitivity to outliers and lack of robustness. In this blog post, we will explore two robust measures for tail weight.\n\n\n\n\n\nOct 20, 2023\n\n\nChristian L. Goueguel\n\n12 min\n\n\n\n\n\n\n\n\n\n\n\nExploring Three Orthogonal Signal Correction (OSC) Algorithms\n\n\n\n\n\nOrthogonal signal correction (OSC) is a powerful preprocessing technique frequently used to remove variation in spectral data that is orthogonal to the property of interest. Over the years, several implementations of OSC have emerged, with the most notable being those by Wold et al., Sjöblom et al., and Fearn. This post compares these three methods, exploring their algorithmic approaches and practical implications.\n\n\n\n\n\nMay 25, 2023\n\n\nChristian L. Goueguel\n\n19 min\n\n\n\n\n\n\n\n\n\n\n\nOptical Breakdown Threshold in Water\n\n\n\n\n\nCascade (or avalanche) ionization and multiphoton ionization are the two primary mechanisms responsible for laser-induced plasma (LIP) formation in water. These absorption processes are influenced by the intensity of the laser pulse and the physical and chemical properties of the water itself. This post focuses on providing a concise overview of these key mechanisms.\n\n\n\n\n\nSep 7, 2022\n\n\nChristian L. Goueguel\n\n8 min\n\n\n\n\n\n\n\n\n\n\n\nChemometric Modeling with Tidymodels: A Tutorial for Spectroscopic Data\n\n\n\n\n\nIn this post, we demonstrate how to build robust chemometric models for spectroscopic data using the Tidymodels framework in R. This workflow is designed to cater to beginners and advanced practitioners alike, offering an end-to-end guide from data preprocessing to model evaluation and interpretation.\n\n\n\n\n\nApr 17, 2022\n\n\nChristian L. Goueguel\n\n11 min\n\n\n\n\n\n\n\n\n\n\n\nCovariance vs Correlation in PCA: What’s the Difference?\n\n\n\n\n\nPrincipal Component Analysis can use either correlation or covariance matrices, but when should you use which? This post walks through the fundamental differences between these two approaches.\n\n\n\n\n\nMar 15, 2021\n\n\nChristian L. Goueguel\n\n4 min\n\n\n\n\n\n\n\n\n\n\n\nMultivariate Outlier Detection in High-Dimensional Spectral Data\n\n\n\n\n\nHigh-dimensional data are particularly challenging for outlier detection. Robust PCA methods have been developed to build models that are unaffected by outliers in high dimensions. These outliers are generally characterized by their deviation from the PCA subspace.\n\n\n\n\n\nJan 7, 2020\n\n\nChristian L. Goueguel\n\n9 min\n\n\n\n\n\n\n\n\n\n\n\nAn Overview of Orthogonal Partial Least Squares\n\n\n\n\n\nHave you ever heard of Orthogonal Partial Least-Squares (OPLS)? This article aims to give you a clear and concise overview of OPLS and its advantages in the development of more efficient predictive models.\n\n\n\n\n\nDec 20, 2019\n\n\nChristian L. Goueguel\n\n6 min\n\n\n\n\n\n\n\n\n\n\n\nShould We Use Standard Error or Standard Deviation?\n\n\n\n\n\nOne of the most common questions in statistics is whether to report standard error (SE) or standard deviation (SD) when presenting data. While these two measures are related, they serve fundamentally different purposes and choosing the wrong one can mislead your audience. Let’s break down when to use each.\n\n\n\n\n\nNov 28, 2019\n\n\nChristian L. Goueguel\n\n5 min\n\n\n\n\n\n\n\n\n\n\n\nThe Pseudo-Voigt Function\n\n\n\n\n\nIn spectroscopy, especially laser spectroscopy, accurate modeling of spectral line shapes is essential for analyzing the physical and chemical properties of matter. A commonly used approximation is the pseudo-Voigt function, which serves as a simplified representation of the Voigt profile. The Voigt profile, defined as the convolution of a Gaussian function and a Lorentzian function, accurately describes the line shapes, but its calculation is often time consuming.\n\n\n\n\n\nApr 5, 2019\n\n\nChristian L. Goueguel\n\n14 min\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "cv.html",
    "href": "cv.html",
    "title": "Curriculum Vitae",
    "section": "",
    "text": ":::"
  },
  {
    "objectID": "talks.html#oral-presentations",
    "href": "talks.html#oral-presentations",
    "title": "Christian L. Goueguel",
    "section": "Oral Presentations",
    "text": "Oral Presentations\n\nC. L. Goueguel, Direct determination of soil texture using laser-induced breakdown spectroscopy and multivariate linear regressions. Annual meetings of the North American Society for Laser-Induced Breakdown Spectroscopy (NASLIBS) hosted by FACSS SciX Conference 2019 (Palm Springs, CA, USA)\nC. L. Goueguel, Using LIBS analyzer for laser-based soil analysis for precision agriculture. Invited speaker at the Canadian Society for Analytical Sciences and Spectroscopy, Jan. 1, 2018 (Toronto, ON, Canada)\nJ. C. Jain, C. L. Goueguel, D. L. McIntyre, H. M. Edenborn, Dissolution of mineral carbonates with increasing CO2 pressure and the implications for carbon sequestration leak detection. GSA annual meeting, 2016 (Denver, CO, USA)\nC. G. Carson, C. Goueguel, J. Jain, D. McIntyre, Development of laser induced breakdown spectroscopy sensor to assess groundwater quality impacts resulting from geologic carbon sequestration. SPIE Micro- and Nanotechnology Sensors, Systems, and Applications VII, 2015 (Baltimore, MD, USA)\nC. L. Goueguel, J. C. Jain, D. L. McIntyre, A. K. Karamalidis, Application of laser induced breakdown spectroscopy (LIBS) to analyze CO2-bearing solutions in high pressure high temperature environment. ACS 248th - National Meeting, 2014 (San Francisco, CA, USA)\nC. L. Goueguel, J. C. Jain, A. K. Karamalidis, D. L. McIntyre, J. P. Singh, Laser-induced breakdown spectroscopy of high-pressure carbonated brine solutions. 2014 PITTCON Conference & Expo, 2014 (Chicago, IL, USA)\nJ. P. Singh, C L. Goueguel, D. L. McIntyre, J. C. Jain, A. K. Karamalidis, Laser-induced breakdown spectroscopy for elemental analysis of brines. 7th International Conference on Laser-Induced Breakdown Spectroscopy, 2012 (Luxor, Egypt)\nC. Goueguel, Laser Induced Breakdown Spectroscopy (LIBS): From Mars exploration to GCS monitoring. Postdoctoral Seminar at the Department of Civil and Environmental Engineering, Carnegie Mellon University, 2012 (Pittsburgh, PA, USA)\nC. Goueguel, S. Laville, F. Vidal, M. Sabsabi, Chaker, Investigation of resonance-enhanced laser-induced breakdown spectroscopy (RELIBS) for the analysis of aluminum alloys, PITTCON Conference & Expo, 2010 (Orlando, FL, USA)\nC. Goueguel, S. Laville, H. Loudyi, F. Vidal, M. Sabsabi, M. Chaker, Investigation of resonance-enhanced laser-induced breakdown spectroscopy for the analysis of aluminum alloys. 2nd North American Symposium on Laser-Induced Breakdown Spectroscopy, 2009 (New Orleans, LA, USA)\nC. Goueguel, H. Loudyi, S. Laville, M. Chaker, M. Sabsabi, F. Vidal, Excitation spectrale sélective d’un plasma d’aluminium par une seconde impulsion laser: Optimisation des paramètres expérimentaux. Colloque Plasma- Québec, 2009 (Montreal, QC, Canada)\nS. Laville, K. Rifai, H. Loudyi, M. Chaker, M. Sabsabi, F. Vidal, C. Goueguel, Analysis of trace elements in liquids using LIBS combined with laser-induced fluorescence. 5th International Conference on Laser-Induced Breakdown Spectroscopy, 2008 (Berlin, Germany)\nF. Vidal, M. Chaker, C. Goueguel, S. Laville, H. Loudyi, K. Rifai, M. Sabsabi, Enhanced laser-induced breakdown spectroscopy by second pulse selective wavelength excitation. Laser and plasma applications in materials science: 1st International Conference on Laser Plasma Applications in Materials Science, LAPAMS, 2008 (Alger, Algeria)\nC. Goueguel, S. Laville, H. Loudyi, M. Chaker, M. Sabsabi, F. Vidal. Detection of lead in brass by Laser-Induced Breakdown Spectroscopy combined with Laser-Induced Fluorescence. Photonics North, 2008 (Montreal, QC, Canada)\nH. Loudyi, C. Goueguel, K. Rifai, S. Laville, M. Sabsabi, M. Chaker, F. Vidal, Enhancing the sensitivity of the laser-Induced Breakdown Spectroscopy technique: spectrally selective excitation of specific elements in laser produced plasma. Canadian Association of Physicists Congress, 2008 (Laval, QC, Canada)\nC. Goueguel, H. Loudyi, S. Laville, M. Chaker, M. Sabsabi, F. Vidal, Analyse des matériaux solides par LIBS: Effet d’une deuxième impulsion laser basée sur une excitation spectrale sélective. Colloque Plasma-Québec, 2008 (Montreal, QC, Canada)\nF. Vidal, S. Laville, C. Goueguel, H. Loudyi, M. Chaker, M. Sabsabi, Investigation of laser-induced breakdown spectroscopy combined with laser- induced fluorescence. 4th Euro-Mediterranean Symposium on Laser-Induced Breakdown Spectroscopy, 2007 (Paris, France)"
  },
  {
    "objectID": "talks.html#poster-presentations",
    "href": "talks.html#poster-presentations",
    "title": "Christian L. Goueguel",
    "section": "Poster Presentations",
    "text": "Poster Presentations\n\nC. L. Goueguel, D. L. McIntyre, J. C. Jain, J. P. Singh, A. K. Karamalidis, Analysis of calcium in CO2-laden brine (NaCl-CaCl2) by Laser-induced breakdown spectroscopy (LIBS). Annual meetings of the North American Society for Laser-Induced Breakdown Spectroscopy (NASLIBS) hosted by FACSS SciX Conference 2013 (Milwaukee, WI, USA)\nC. Goueguel, S. Laville, F. Vidal, M. Sabsabi, M. Chaker, Investigation of resonance-enhanced laser-induced breakdown spectroscopy (RELIBS) for the analysis of aluminum alloys. 6th International Conference on Laser- Induced Breakdown Spectroscopy, 2010 (Memphis, TN, USA)"
  },
  {
    "objectID": "blog/posts/post4/index.html",
    "href": "blog/posts/post4/index.html",
    "title": "Exploring Three Orthogonal Signal Correction (OSC) Algorithms",
    "section": "",
    "text": "Photo by Jonatan Pie.\n\n\n\n\nWold’s method was the first formal OSC algorithm. It operates iteratively to identify orthogonal components unrelated to the dependent variable \\(Y\\). The method leverages a combination of principal component analysis (PCA) and partial least squares (PLS). Sjöblom’s approach builds on Wold’s by introducing a direct orthogonalization step. The algorithm emphasizes calibration transfer, making it especially useful for standardizing spectral datasets across instruments or conditions. Whereas, Fearn proposed a mathematically elegant version of OSC, simplifying the computation by leveraging matrix operations. The method directly orthogonalizes \\(X\\) using a singular value decomposition (SVD) of a residual matrix.\n\n\nThe Wold algorithm is like a precise sculptor of spectroscopic data. It uses Partial Least Squares (PLS) regression to systematically remove spectral variations that are unrelated to the target variable. The key steps involve:\nInitialize \\(t\\), the first score vector (e.g., using PCA on \\(X\\)).\n\nDeflate \\(t\\) using \\(Y\\): \\(t_{\\text{new}} = t - Y(Y^\\top Y)^{-1}Y^\\top t\\)\nCalculate a loading vector \\(p\\) from \\(t_{\\text{new}}\\) to model \\(X\\): \\(p = \\frac{X^\\top t_{\\text{new}}}{t_{\\text{new}}^\\top t_{\\text{new}}}\\)\nDeflate \\(X\\): \\(X_{\\text{new}} = X - t_{\\text{new}} p^\\top\\)\nRepeat until \\(n_{\\text{comp}}\\).\n\n\n\nShow the code\nwold_osc &lt;- function(X, Y, ncomp = 1, tol = 1e-6, max.iter = 100) {\n  # Ensure X and Y are matrices\n  X &lt;- as.matrix(X)\n  Y &lt;- as.matrix(Y)\n\n  # Store the original X matrix\n  X_original &lt;- X\n\n  # Initialize lists to store components\n  scores &lt;- list()\n  loadings &lt;- list()\n  weights &lt;- list()\n\n  for (comp in seq_len(ncomp)) {\n    # Step 1: Initial PCA on X to get the first principal component score vector (t)\n    t &lt;- svd(X, nu = 1, nv = 0)$u * svd(X, nu = 1, nv = 0)$d[1]\n\n    # Step 2: Orthogonalize t with respect to Y to obtain t*\n    t_star &lt;- t - Y %*% MASS::ginv(crossprod(Y, Y)) %*% crossprod(Y, t)\n\n    iter &lt;- 0\n    diff &lt;- tol + 1\n\n    while (diff &gt; tol && iter &lt; max.iter) {\n      iter &lt;- iter + 1\n\n      # Step 3: Compute weights (w) to make Xw as close as possible to t*\n      w &lt;- crossprod(X, t_star) / sum(t_star^2)\n      w &lt;- w / sqrt(sum(w^2))  # Normalize the weights\n\n      # Step 4: Update t as Xw\n      t_new &lt;- X %*% w\n\n      # Step 5: Orthogonalize t_new with respect to Y\n      t_star &lt;- t_new - Y %*% MASS::ginv(crossprod(Y, Y)) %*% crossprod(Y, t_new)\n\n      # Compute convergence criterion\n      diff &lt;- sqrt(sum((t_star - t)^2)) / sqrt(sum(t_star^2))\n\n      # Update t for the next iteration\n      t &lt;- t_star\n    }\n\n    if (iter == max.iter) {\n      warning(\"Iteration limit reached without convergence.\")\n    }\n\n    # Step 6: Compute the loading vector (p)\n    p &lt;- crossprod(X, t_star) / sum(t_star^2)\n\n    # Step 7: Deflate X\n    X &lt;- X - t_star %*% t(p)\n\n    # Store results\n    scores[[comp]] &lt;- t_star\n    loadings[[comp]] &lt;- p\n    weights[[comp]] &lt;- w\n  }\n\n  # Combine components into matrices\n  T_star &lt;- do.call(cbind, scores)\n  P &lt;- do.call(cbind, loadings)\n  W &lt;- do.call(cbind, weights)\n\n  # Calculate the filtered X matrix\n  X_filtered &lt;- X_original - T_star %*% t(P)\n\n  # Return results as a list\n  return(list(\n    scores = T_star,\n    loadings = P,\n    weights = W,\n    X_filtered = X_filtered\n  ))\n}\n\n\n\n\n\nSjöblom’s approach is the pragmatic cousin of the Wold method. It uses similar steps but simplifies certain iterative aspects, focusing on the orthogonal direction more explicitly.\n\nIdentify a direction vector \\(w\\) from \\(X\\) and \\(t\\), the orthogonal scores \\(w = \\frac{X^\\top t}{t^\\top t}\\)​\nNormalize \\(w\\): \\(w = \\frac{w}{\\|w\\|}\\)​\nDeflate \\(t\\) from \\(Y\\) as in Wold’s method.\nRemove the orthogonal variation from \\(X\\): \\(X_{\\text{new}} = X - t p^\\top\\)\nIterate for each component.\n\n\n\nShow the code\nsjoblom_osc &lt;- function(x, y, ncomp, tol, max.iter) {\n  x_original &lt;- x\n  ps &lt;- ws &lt;- ts &lt;- vector(\"list\", ncomp)\n  for (i in seq_len(ncomp)) {\n    pc &lt;- stats::prcomp(x, center = FALSE)\n    t &lt;- pc$x[, 1]\n    .diff &lt;- 1\n    .iter &lt;- 0\n    while (.diff &gt; tol && .iter &lt; max.iter) {\n      .iter &lt;- .iter + 1\n      t_new &lt;- t - y %*% MASS::ginv(crossprod(y, y)) %*% crossprod(y, t)\n      w &lt;- crossprod(x, t_new) %*% MASS::ginv(crossprod(t_new, t_new))\n      w &lt;- w / sqrt(sum(w^2))\n      t_new &lt;- x %*% w\n      .diff &lt;- sqrt(sum((t_new - t)^2) / sum(t_new^2))\n      t &lt;- t_new\n    }\n    plsFit &lt;- pls::simpls.fit(x, t, ncomp)\n    w &lt;- plsFit$coefficients[ , , ncomp]\n    t &lt;- x %*% w\n    t &lt;- t - y %*% MASS::ginv(crossprod(y, y)) %*% crossprod(y, t)\n    p &lt;- crossprod(x, t) %*% MASS::ginv(crossprod(t, t))\n    x &lt;- x - tcrossprod(t, p)\n    ws[[i]] &lt;- w\n    ps[[i]] &lt;- p\n    ts[[i]] &lt;- t\n  }\n  w_ortho &lt;- do.call(cbind, ws)\n  p_ortho &lt;- do.call(cbind, ps)\n  t_ortho &lt;- do.call(cbind, ts)\n  x_osc &lt;- x_original - x_original %*% tcrossprod(w_ortho, p_ortho)\n\n  R2 &lt;- sum(x_osc^2) / sum(x_original^2) * 100\n  angle &lt;- crossprod(t_ortho, y)\n  norm &lt;- MASS::ginv(sqrt(apply(t_ortho^2, 2, sum) * sum(y^2)))\n  angle &lt;- t(angle) %*% t(norm)\n  angle &lt;- mean(acos(angle) * 180 / pi)\n\n  res &lt;- list(\n    \"correction\" = tibble::as_tibble(x_osc),\n    \"weights\" = tibble::as_tibble(w_ortho),\n    \"scores\" = tibble::as_tibble(t_ortho),\n    \"loadings\" = tibble::as_tibble(p_ortho),\n    \"angle\" = angle,\n    \"R2\" = R2\n  )\n  return(res)\n}\n\n\n\n\n\nFearn’s method stands out by using Singular Value Decomposition (SVD) as its foundation. Its characteristics include:\n\nCompute the residual matrix \\(Z\\): \\(Z = X - Y (Y^\\top Y)^{-1} Y^\\top X\\)\nPerform SVD on \\(Z\\): \\(Z = U S V^\\top\\)\nExtract the first \\(n_{\\text{comp}}\\) components from \\(V\\) and reconstruct the orthogonal scores \\(t\\) and loadings \\(p\\): \\(t = Z V_{:, i}, \\quad p = \\frac{X^\\top t}{t^\\top t}\\)\nDeflate \\(X\\): \\(X_{\\text{new}} = X - t p^\\top\\)\n\n\n\nShow the code\nfearn_osc &lt;- function(x, y, ncomp, tol, max.iter) {\n  x_original &lt;- x\n  ps &lt;- ws &lt;- ts &lt;- vector(\"list\", ncomp)\n  m &lt;- diag(row(x)) - crossprod(x, y) %*% MASS::ginv(crossprod(y, x) %*% crossprod(x, y)) %*% crossprod(y, x)\n  z &lt;- x %*% m\n  decomp &lt;- svd(t(z))\n  u &lt;- decomp$u\n  s &lt;- decomp$d\n  v &lt;- decomp$v\n  g &lt;- diag(s[1:ncomp])\n  c &lt;- v[, 1:ncomp, drop = FALSE]\n\n  for (i in seq_len(ncomp)) {\n    w_old &lt;- rep(0, ncol(x))\n    w_new &lt;- rep(1, ncol(x))\n    dif &lt;- 1\n    iter &lt;- 0\n    while (dif &gt; tol && iter &lt; max.iter) {\n      iter &lt;- iter + 1\n      w_old &lt;- w_new\n      t_new &lt;- c[, i] %*% g[i, i]\n      p_new &lt;- tcrossprod(x, t_new) / tcrossprod(t_new, t_new)\n      w_new &lt;- m %*% tcrossprod(x, p_new)\n      dif &lt;- sqrt(sum((w_new - w_old)^2) / sum(w_new^2))\n    }\n    ws[[i]] &lt;- w_new\n    ts[[i]] &lt;- c[, i] %*% g[i, i]\n    ps[[i]] &lt;- tcrossprod(x, t[[i]]) / tcrossprod(t[[i]], t[[i]])\n  }\n  w_ortho &lt;- do.call(cbind, ws)\n  t_ortho &lt;- do.call(cbind, ts)\n  p_ortho &lt;- do.call(cbind, ps)\n  x_osc &lt;- x - tcrossprod(t_ortho, p_ortho)\n\n  R2 &lt;- sum(x_osc^2) / sum(x_original^2) * 100\n  angle &lt;- crossprod(t_ortho, y)\n  norm &lt;- MASS::ginv(sqrt(apply(t_ortho^2, 2, sum) * sum(y^2)))\n  angle &lt;- t(angle) %*% t(norm)\n  angle &lt;- mean(acos(angle) * 180 / pi)\n\n  res &lt;- list(\n    \"correction\" = tibble::as_tibble(x_osc),\n    \"weights\" = tibble::as_tibble(w_ortho),\n    \"scores\" = tibble::as_tibble(t_ortho),\n    \"loadings\" = tibble::as_tibble(p_ortho),\n    \"angle\" = angle,\n    \"R2\" = R2\n  )\n  return(res)\n}\n\n\n\n\n\n\nWe begin by implementing these algorithms, creating functions named wold_osc, sjoblom_osc, and fearn_osc. Each function takes five key parameters. The first parameter, x, represents the input data matrix, which typically contains spectral or chemical measurements. The second parameter, y, corresponds to the target variable or response vector. The ncomp parameter specifies the number of orthogonal components to extract, while tol sets the tolerance level for convergence, determining the stopping criterion for iterations. Finally, max.iter establishes the maximum number of iterations allowed during the optimization process. The function definitions for these algorithms follow this structure:\nwold_osc &lt;- function(x, y, ncomp, tol, max.iter)\nsjoblom_osc &lt;- function(x, y, ncomp, tol, max.iter)\nfearn_osc &lt;- function(x, y, ncomp, tol, max.iter)\nTo begin, the original data matrix x is stored, and empty lists are initialized to hold the extracted principal components, weights, and scores for each component. This step ensures that the algorithm’s outputs are organized for further processing or analysis:\nx_original &lt;- x\nps &lt;- ws &lt;- ts &lt;- vector(\"list\", ncomp)\nThe algorithm proceeds with a loop to extract the specified number of orthogonal components. For each iteration, Principal Component Analysis (PCA) is performed on the current x matrix without centering, using the stats::prcomp function. The initial score vector t is derived from the first principal component. Variables .iter and .diff are initialized to track the number of iterations and the difference between successive score vectors, which serves as the convergence criterion.\nWithin the loop, the orthogonalization process begins. Variation correlated with the response variable y is iteratively removed from the score vector t, refining its orthogonality. Weights, representing the relationship between the input matrix x and the score vector, are calculated and normalized to unit length. A new score vector is then computed, and the convergence check compares the difference between successive score vectors (.diff) to the tolerance level (tol). The loop continues until the difference falls below the specified tolerance or the maximum number of iterations is reached.\nwhile (.diff &gt; tol && .iter &lt; max.iter) {\n  .iter &lt;- .iter + 1\n  t_new &lt;- t - y %*% MASS::ginv(crossprod(y, y)) %*% crossprod(y, t)\n  w &lt;- crossprod(x, t_new) %*% MASS::ginv(crossprod(t_new, t_new))\n  w &lt;- w / sqrt(sum(w^2))\n  t_new &lt;- x %*% w\n  .diff &lt;- sqrt(sum((t_new - t)^2) / sum(t_new^2))\n  t &lt;- t_new\n}\nAfter achieving convergence, a Partial Least Squares (PLS) model is fitted to the data using the extracted scores. The weights and scores are updated, and the loadings are computed. At this stage, y-correlated variation is removed, and the input matrix x is deflated by subtracting the modeled variation. This step prepares the matrix for the next orthogonal component extraction.\nThe extracted weights, loadings, and scores for each orthogonal component are stored in their respective lists:\nws[[i]] &lt;- w\nps[[i]] &lt;- p\nts[[i]] &lt;- t\nOnce all components are extracted, the results are combined to construct the orthogonal components matrix. The orthogonally corrected matrix x_osc is then computed by removing the contributions of the orthogonal components from the original data matrix:\nx_osc &lt;- x_original - x_original %*% tcrossprod(w_ortho, p_ortho)\nFinally, to evaluate the algorithm’s performance, two metrics are computed. The percentage of variation removed R2 quantifies how effectively the algorithm deflates the input matrix, while the angle between the orthogonal scores and the target variable y provides insight into the degree of orthogonality achieved. These metrics allow us to assess the quality and effectiveness of the orthogonal signal correction methods.\n\n\n\nWe will use the beer dataset introduced in our previous post. The dataset consists of Near-Infrared Spectroscopy (NIRS) spectra collected from 80 beer samples, x_matrix. The target variable of interest is the Original Gravity (OG), also known as the original extract, y_target. This parameter measures the concentration of dissolved solids in the wort before fermentation begins, providing a crucial indicator of the brewing process.\n\n\nShow the code\nbeer &lt;- readr::read_csv(\"beer.csv\", show_col_types = FALSE) |&gt; dplyr::rename(extract = originalGravity)\nx_matrix &lt;- beer |&gt; dplyr::select(-extract) |&gt; as.matrix() |&gt; scale(scale = FALSE) \ny_target &lt;- beer |&gt; dplyr::pull(extract)\n\n\nNext, we will perform standard PCA and PLS on the NIRS spectra of the beer dataset to assess how applying orthogonal correction modifies the data structure.\n\n\nShow the code\nset.seed(123)\nn_samples &lt;- as.integer(nrow(x_matrix))\n# PCA\npca_result &lt;- stats::prcomp(x_matrix, center = FALSE, scale = FALSE)\npca_scores &lt;- pca_result$x\npca_df &lt;- data.frame(\n  comp1 = pca_scores[, 1],\n  comp2 = pca_scores[, 2],\n  extract = y_target,\n  Sample = factor(n_samples)\n)\n\n# PLS\npls_result &lt;- pls::plsr(y_target ~ x_matrix, ncomp = 10, validation = \"none\")\npls_scores &lt;- pls::scores(pls_result)[, 1:10]\npls_df &lt;- data.frame(\n  comp1 = pls_scores[, 1],\n  comp2 = pls_scores[, 2],\n  extract = y_target,\n  Sample = factor(n_samples)\n)\n\n# Hotelling's T-squared ellipse\npca_T2ellipse &lt;- HotellingEllipse::ellipseParam(pca_scores, k = 2, pcx = 1, pcy = 2)\npls_T2ellipse &lt;- HotellingEllipse::ellipseParam(pls_scores, k = 2, pcx = 1, pcy = 2)\n\n\n\n\n\n\n\n\n\n\n\nNow, we can apply the OSC algorithms to the dataset and compare their impact. Specifically, we’ll examine how the OSC filtering affects the distribution of samples in the reduced-dimensionality space and whether the variation captured aligns better with the response variable.\n\n\nShow the code\nwold_filter &lt;- wold_osc(x_matrix, y_target, ncomp = 10, tol = 1e10, max.iter = 10)\nsjoblom_filter &lt;- sjoblom_osc(x_matrix, y_target, ncomp = 10, tol = 1, max.iter = 10)\n\nwold_scores &lt;- wold_filter %&gt;%\n  pluck(\"scores\") %&gt;%\n  as_tibble() %&gt;%\n  mutate(extract = y_target)\n\nsjoblom_scores &lt;- sjoblom_filter %&gt;%\n  pluck(\"scores\") %&gt;%\n  as_tibble() %&gt;%\n  mutate(extract = y_target)\n\n\n\n\n\n\n\n\n\n\n\nThe following plot compares the results of OSC, PCA, and PLS modeling by overlaying their respective loadings on a single graph. By presenting their loadings together, we can clearly observe the differences in how each method captures and prioritizes spectral features and filter noise, highlighting their unique contributions and areas of overlap.\n\n\nShow the code\npca_loadings &lt;- pca_result %&gt;% \n  pluck(\"rotation\") %&gt;%\n  as_tibble() %&gt;%\n  mutate(wavelength = rep(seq(1100, 2250, 2), times = 1))\n\npls_loadings &lt;- loadings(pls_result)[, 1:10] %&gt;%\n  as_tibble() %&gt;%\n  janitor::clean_names() %&gt;%\n  mutate(wavelength = rep(seq(1100, 2250, 2), times = 1))\n\nwold_loadings &lt;- wold_filter %&gt;% \n  pluck(\"loadings\") %&gt;%\n  as_tibble() %&gt;%\n  mutate(wavelength = rep(seq(1100, 2250, 2), times = 1))\n  \nsjoblom_loadings &lt;- sjoblom_filter %&gt;% \n  pluck(\"loadings\") %&gt;%\n  mutate(wavelength = rep(seq(1100, 2250, 2), times = 1))\n\n\n\n\n\n\n\n\n\n\n\nUnlike PCA (in red) and PLS (in dark green), which show substantial variability, particularly in the region above 1350 nm, OSC (in blue) effectively filters the noise, reducing it to near-zero levels. This smoothing demonstrates OSC’s ability to isolate and preserve only the information strongly correlated with the target variable while systematically discarding irrelevant or orthogonal components. However, its aggressive filtering comes at the expense of potentially reducing some useful signal.\nThe PCA and PLS loadings, on the other hand, display pronounced fluctuations, reflecting their sensitivity to variance within the dataset. PCA, focusing solely on maximizing variance without considering the target variable, captures not only relevant features but also substantial noise. PLS, while more targeted as it incorporates the correlation with the target variable, still exhibits residual noise, especially in the higher wavelengths, indicating its partial retention of irrelevant variance."
  },
  {
    "objectID": "blog/posts/post4/index.html#wolds-osc-algorithm",
    "href": "blog/posts/post4/index.html#wolds-osc-algorithm",
    "title": "A Comparative Exploration of Orthogonal Signal Correction Methods",
    "section": "",
    "text": "The Wold algorithm is like a precise sculptor of spectroscopic data. It uses Partial Least Squares (PLS) regression to systematically remove spectral variations that are unrelated to the target variable. The key steps involve:\nInitialize \\(t\\), the first score vector (e.g., using PCA on \\(X\\)).\n\nDeflate \\(t\\) using \\(Y\\): \\(t_{\\text{new}} = t - Y(Y^\\top Y)^{-1}Y^\\top t\\)\nCalculate a loading vector \\(p\\) from \\(t_{\\text{new}}\\) to model \\(X\\): \\(p = \\frac{X^\\top t_{\\text{new}}}{t_{\\text{new}}^\\top t_{\\text{new}}}\\)\nDeflate \\(X\\): \\(X_{\\text{new}} = X - t_{\\text{new}} p^\\top\\)\nRepeat until \\(n_{\\text{comp}}\\)."
  },
  {
    "objectID": "blog/posts/post4/index.html#sjöbloms-osc-algorithm",
    "href": "blog/posts/post4/index.html#sjöbloms-osc-algorithm",
    "title": "A Comparative Exploration of Orthogonal Signal Correction Methods",
    "section": "",
    "text": "Sjöblom’s approach is the pragmatic cousin of the Wold method. It uses similar steps but simplifies certain iterative aspects, focusing on the orthogonal direction more explicitly.\n\nIdentify a direction vector \\(w\\) from \\(X\\) and \\(t\\), the orthogonal scores \\(w = \\frac{X^\\top t}{t^\\top t}\\)​\nNormalize \\(w\\): \\(w = \\frac{w}{\\|w\\|}\\)​\nDeflate \\(t\\) from \\(Y\\) as in Wold’s method.\nRemove the orthogonal variation from \\(X\\): \\(X_{\\text{new}} = X - t p^\\top\\)\nIterate for each component."
  },
  {
    "objectID": "blog/posts/post4/index.html#fearns-osc-algorithm",
    "href": "blog/posts/post4/index.html#fearns-osc-algorithm",
    "title": "A Comparative Exploration of Orthogonal Signal Correction Methods",
    "section": "",
    "text": "Fearn’s method stands out by using Singular Value Decomposition (SVD) as its foundation. Its characteristics include:\n\nCompute the residual matrix \\(Z\\): \\(Z = X - Y (Y^\\top Y)^{-1} Y^\\top X\\)\nPerform SVD on \\(Z\\): \\(Z = U S V^\\top\\)\nExtract the first \\(n_{\\text{comp}}\\) components from \\(V\\) and reconstruct the orthogonal scores \\(t\\) and loadings \\(p\\): \\(t = Z V_{:, i}, \\quad p = \\frac{X^\\top t}{t^\\top t}\\)\nDeflate \\(X\\): \\(X_{\\text{new}} = X - t p^\\top\\)\n\nWe begin by implementing these algorithms, creating functions named wold_osc, sjoblom_osc, and fearn_osc. Each function takes five key parameters. The first parameter, x, represents the input data matrix, which typically contains spectral or chemical measurements. The second parameter, y, corresponds to the target variable or response vector. The ncomp parameter specifies the number of orthogonal components to extract, while tol sets the tolerance level for convergence, determining the stopping criterion for iterations. Finally, max.iter establishes the maximum number of iterations allowed during the optimization process. The function definitions for these algorithms follow this structure:\nwold_osc &lt;- function(x, y, ncomp, tol, max.iter)\nsjoblom_osc &lt;- function(x, y, ncomp, tol, max.iter)\nfearn_osc &lt;- function(x, y, ncomp, tol, max.iter)\nTo begin, the original data matrix x is stored, and empty lists are initialized to hold the extracted principal components, weights, and scores for each component. This step ensures that the algorithm’s outputs are organized for further processing or analysis:\nx_original &lt;- x\nps &lt;- ws &lt;- ts &lt;- vector(\"list\", ncomp)\nThe algorithm proceeds with a loop to extract the specified number of orthogonal components. For each iteration, Principal Component Analysis (PCA) is performed on the current x matrix without centering, using the stats::prcomp function. The initial score vector t is derived from the first principal component. Variables .iter and .diff are initialized to track the number of iterations and the difference between successive score vectors, which serves as the convergence criterion.\nWithin the loop, the orthogonalization process begins. Variation correlated with the response variable y is iteratively removed from the score vector t, refining its orthogonality. Weights, representing the relationship between the input matrix x and the score vector, are calculated and normalized to unit length. A new score vector is then computed, and the convergence check compares the difference between successive score vectors (.diff) to the tolerance level (tol). The loop continues until the difference falls below the specified tolerance or the maximum number of iterations is reached.\nwhile (.diff &gt; tol && .iter &lt; max.iter) {\n  .iter &lt;- .iter + 1\n  t_new &lt;- t - y %*% MASS::ginv(crossprod(y, y)) %*% crossprod(y, t)\n  w &lt;- crossprod(x, t_new) %*% MASS::ginv(crossprod(t_new, t_new))\n  w &lt;- w / sqrt(sum(w^2))\n  t_new &lt;- x %*% w\n  .diff &lt;- sqrt(sum((t_new - t)^2) / sum(t_new^2))\n  t &lt;- t_new\n}\nAfter achieving convergence, a Partial Least Squares (PLS) model is fitted to the data using the extracted scores. The weights and scores are updated, and the loadings are computed. At this stage, y-correlated variation is removed, and the input matrix x is deflated by subtracting the modeled variation. This step prepares the matrix for the next orthogonal component extraction.\nThe extracted weights, loadings, and scores for each orthogonal component are stored in their respective lists:\nws[[i]] &lt;- w\nps[[i]] &lt;- p\nts[[i]] &lt;- t\nOnce all components are extracted, the results are combined to construct the orthogonal components matrix. The orthogonally corrected matrix x_osc is then computed by removing the contributions of the orthogonal components from the original data matrix:\nx_osc &lt;- x_original - x_original %*% tcrossprod(w_ortho, p_ortho)\nFinally, to evaluate the algorithm’s performance, two metrics are computed. The percentage of variation removed R2 quantifies how effectively the algorithm deflates the input matrix, while the angle between the orthogonal scores and the target variable y provides insight into the degree of orthogonality achieved. These metrics allow us to assess the quality and effectiveness of the orthogonal signal correction methods."
  },
  {
    "objectID": "blog/posts/post4/index.html#key-differences-among-the-methods",
    "href": "blog/posts/post4/index.html#key-differences-among-the-methods",
    "title": "A Comparative Exploration of Orthogonal Signal Correction Methods",
    "section": "",
    "text": "Aspect\nWold’s OSC\nSjöblom’s OSC\nFearn’s OSC\n\n\n\n\nComputation\nIterative\nuses PLS.\nSimplified iterative approach.\nSVD-based; avoids iteration.\n\n\nFocus\nGeneral orthogonal correction.\nOrthogonal deflation with efficiency.\nDirect analysis of residuals.\n\n\nComplexity\nModerate\nrequires tuning.\nLower than Wold’s.\nSimplest algorithmically.\n\n\nNoise Robustness\nSensitive to noise.\nSimilar to Wold’s.\nMore robust to noise.\n\n\nEase of Use\nEstablished but iterative.\nComputationally efficient.\nSimple and deterministic."
  },
  {
    "objectID": "blog/posts/post4/index.html#introduction",
    "href": "blog/posts/post4/index.html#introduction",
    "title": "Exploring Three Orthogonal Signal Correction (OSC) Algorithms",
    "section": "",
    "text": "Wold’s method was the first formal OSC algorithm. It operates iteratively to identify orthogonal components unrelated to the dependent variable \\(Y\\). The method leverages a combination of principal component analysis (PCA) and partial least squares (PLS). Sjöblom’s approach builds on Wold’s by introducing a direct orthogonalization step. The algorithm emphasizes calibration transfer, making it especially useful for standardizing spectral datasets across instruments or conditions. Whereas, Fearn proposed a mathematically elegant version of OSC, simplifying the computation by leveraging matrix operations. The method directly orthogonalizes \\(X\\) using a singular value decomposition (SVD) of a residual matrix.\n\n\nThe Wold algorithm is like a precise sculptor of spectroscopic data. It uses Partial Least Squares (PLS) regression to systematically remove spectral variations that are unrelated to the target variable. The key steps involve:\nInitialize \\(t\\), the first score vector (e.g., using PCA on \\(X\\)).\n\nDeflate \\(t\\) using \\(Y\\): \\(t_{\\text{new}} = t - Y(Y^\\top Y)^{-1}Y^\\top t\\)\nCalculate a loading vector \\(p\\) from \\(t_{\\text{new}}\\) to model \\(X\\): \\(p = \\frac{X^\\top t_{\\text{new}}}{t_{\\text{new}}^\\top t_{\\text{new}}}\\)\nDeflate \\(X\\): \\(X_{\\text{new}} = X - t_{\\text{new}} p^\\top\\)\nRepeat until \\(n_{\\text{comp}}\\).\n\n\n\nShow the code\nwold_osc &lt;- function(X, Y, ncomp = 1, tol = 1e-6, max.iter = 100) {\n  # Ensure X and Y are matrices\n  X &lt;- as.matrix(X)\n  Y &lt;- as.matrix(Y)\n\n  # Store the original X matrix\n  X_original &lt;- X\n\n  # Initialize lists to store components\n  scores &lt;- list()\n  loadings &lt;- list()\n  weights &lt;- list()\n\n  for (comp in seq_len(ncomp)) {\n    # Step 1: Initial PCA on X to get the first principal component score vector (t)\n    t &lt;- svd(X, nu = 1, nv = 0)$u * svd(X, nu = 1, nv = 0)$d[1]\n\n    # Step 2: Orthogonalize t with respect to Y to obtain t*\n    t_star &lt;- t - Y %*% MASS::ginv(crossprod(Y, Y)) %*% crossprod(Y, t)\n\n    iter &lt;- 0\n    diff &lt;- tol + 1\n\n    while (diff &gt; tol && iter &lt; max.iter) {\n      iter &lt;- iter + 1\n\n      # Step 3: Compute weights (w) to make Xw as close as possible to t*\n      w &lt;- crossprod(X, t_star) / sum(t_star^2)\n      w &lt;- w / sqrt(sum(w^2))  # Normalize the weights\n\n      # Step 4: Update t as Xw\n      t_new &lt;- X %*% w\n\n      # Step 5: Orthogonalize t_new with respect to Y\n      t_star &lt;- t_new - Y %*% MASS::ginv(crossprod(Y, Y)) %*% crossprod(Y, t_new)\n\n      # Compute convergence criterion\n      diff &lt;- sqrt(sum((t_star - t)^2)) / sqrt(sum(t_star^2))\n\n      # Update t for the next iteration\n      t &lt;- t_star\n    }\n\n    if (iter == max.iter) {\n      warning(\"Iteration limit reached without convergence.\")\n    }\n\n    # Step 6: Compute the loading vector (p)\n    p &lt;- crossprod(X, t_star) / sum(t_star^2)\n\n    # Step 7: Deflate X\n    X &lt;- X - t_star %*% t(p)\n\n    # Store results\n    scores[[comp]] &lt;- t_star\n    loadings[[comp]] &lt;- p\n    weights[[comp]] &lt;- w\n  }\n\n  # Combine components into matrices\n  T_star &lt;- do.call(cbind, scores)\n  P &lt;- do.call(cbind, loadings)\n  W &lt;- do.call(cbind, weights)\n\n  # Calculate the filtered X matrix\n  X_filtered &lt;- X_original - T_star %*% t(P)\n\n  # Return results as a list\n  return(list(\n    scores = T_star,\n    loadings = P,\n    weights = W,\n    X_filtered = X_filtered\n  ))\n}\n\n\n\n\n\nSjöblom’s approach is the pragmatic cousin of the Wold method. It uses similar steps but simplifies certain iterative aspects, focusing on the orthogonal direction more explicitly.\n\nIdentify a direction vector \\(w\\) from \\(X\\) and \\(t\\), the orthogonal scores \\(w = \\frac{X^\\top t}{t^\\top t}\\)​\nNormalize \\(w\\): \\(w = \\frac{w}{\\|w\\|}\\)​\nDeflate \\(t\\) from \\(Y\\) as in Wold’s method.\nRemove the orthogonal variation from \\(X\\): \\(X_{\\text{new}} = X - t p^\\top\\)\nIterate for each component.\n\n\n\nShow the code\nsjoblom_osc &lt;- function(x, y, ncomp, tol, max.iter) {\n  x_original &lt;- x\n  ps &lt;- ws &lt;- ts &lt;- vector(\"list\", ncomp)\n  for (i in seq_len(ncomp)) {\n    pc &lt;- stats::prcomp(x, center = FALSE)\n    t &lt;- pc$x[, 1]\n    .diff &lt;- 1\n    .iter &lt;- 0\n    while (.diff &gt; tol && .iter &lt; max.iter) {\n      .iter &lt;- .iter + 1\n      t_new &lt;- t - y %*% MASS::ginv(crossprod(y, y)) %*% crossprod(y, t)\n      w &lt;- crossprod(x, t_new) %*% MASS::ginv(crossprod(t_new, t_new))\n      w &lt;- w / sqrt(sum(w^2))\n      t_new &lt;- x %*% w\n      .diff &lt;- sqrt(sum((t_new - t)^2) / sum(t_new^2))\n      t &lt;- t_new\n    }\n    plsFit &lt;- pls::simpls.fit(x, t, ncomp)\n    w &lt;- plsFit$coefficients[ , , ncomp]\n    t &lt;- x %*% w\n    t &lt;- t - y %*% MASS::ginv(crossprod(y, y)) %*% crossprod(y, t)\n    p &lt;- crossprod(x, t) %*% MASS::ginv(crossprod(t, t))\n    x &lt;- x - tcrossprod(t, p)\n    ws[[i]] &lt;- w\n    ps[[i]] &lt;- p\n    ts[[i]] &lt;- t\n  }\n  w_ortho &lt;- do.call(cbind, ws)\n  p_ortho &lt;- do.call(cbind, ps)\n  t_ortho &lt;- do.call(cbind, ts)\n  x_osc &lt;- x_original - x_original %*% tcrossprod(w_ortho, p_ortho)\n\n  R2 &lt;- sum(x_osc^2) / sum(x_original^2) * 100\n  angle &lt;- crossprod(t_ortho, y)\n  norm &lt;- MASS::ginv(sqrt(apply(t_ortho^2, 2, sum) * sum(y^2)))\n  angle &lt;- t(angle) %*% t(norm)\n  angle &lt;- mean(acos(angle) * 180 / pi)\n\n  res &lt;- list(\n    \"correction\" = tibble::as_tibble(x_osc),\n    \"weights\" = tibble::as_tibble(w_ortho),\n    \"scores\" = tibble::as_tibble(t_ortho),\n    \"loadings\" = tibble::as_tibble(p_ortho),\n    \"angle\" = angle,\n    \"R2\" = R2\n  )\n  return(res)\n}\n\n\n\n\n\nFearn’s method stands out by using Singular Value Decomposition (SVD) as its foundation. Its characteristics include:\n\nCompute the residual matrix \\(Z\\): \\(Z = X - Y (Y^\\top Y)^{-1} Y^\\top X\\)\nPerform SVD on \\(Z\\): \\(Z = U S V^\\top\\)\nExtract the first \\(n_{\\text{comp}}\\) components from \\(V\\) and reconstruct the orthogonal scores \\(t\\) and loadings \\(p\\): \\(t = Z V_{:, i}, \\quad p = \\frac{X^\\top t}{t^\\top t}\\)\nDeflate \\(X\\): \\(X_{\\text{new}} = X - t p^\\top\\)\n\n\n\nShow the code\nfearn_osc &lt;- function(x, y, ncomp, tol, max.iter) {\n  x_original &lt;- x\n  ps &lt;- ws &lt;- ts &lt;- vector(\"list\", ncomp)\n  m &lt;- diag(row(x)) - crossprod(x, y) %*% MASS::ginv(crossprod(y, x) %*% crossprod(x, y)) %*% crossprod(y, x)\n  z &lt;- x %*% m\n  decomp &lt;- svd(t(z))\n  u &lt;- decomp$u\n  s &lt;- decomp$d\n  v &lt;- decomp$v\n  g &lt;- diag(s[1:ncomp])\n  c &lt;- v[, 1:ncomp, drop = FALSE]\n\n  for (i in seq_len(ncomp)) {\n    w_old &lt;- rep(0, ncol(x))\n    w_new &lt;- rep(1, ncol(x))\n    dif &lt;- 1\n    iter &lt;- 0\n    while (dif &gt; tol && iter &lt; max.iter) {\n      iter &lt;- iter + 1\n      w_old &lt;- w_new\n      t_new &lt;- c[, i] %*% g[i, i]\n      p_new &lt;- tcrossprod(x, t_new) / tcrossprod(t_new, t_new)\n      w_new &lt;- m %*% tcrossprod(x, p_new)\n      dif &lt;- sqrt(sum((w_new - w_old)^2) / sum(w_new^2))\n    }\n    ws[[i]] &lt;- w_new\n    ts[[i]] &lt;- c[, i] %*% g[i, i]\n    ps[[i]] &lt;- tcrossprod(x, t[[i]]) / tcrossprod(t[[i]], t[[i]])\n  }\n  w_ortho &lt;- do.call(cbind, ws)\n  t_ortho &lt;- do.call(cbind, ts)\n  p_ortho &lt;- do.call(cbind, ps)\n  x_osc &lt;- x - tcrossprod(t_ortho, p_ortho)\n\n  R2 &lt;- sum(x_osc^2) / sum(x_original^2) * 100\n  angle &lt;- crossprod(t_ortho, y)\n  norm &lt;- MASS::ginv(sqrt(apply(t_ortho^2, 2, sum) * sum(y^2)))\n  angle &lt;- t(angle) %*% t(norm)\n  angle &lt;- mean(acos(angle) * 180 / pi)\n\n  res &lt;- list(\n    \"correction\" = tibble::as_tibble(x_osc),\n    \"weights\" = tibble::as_tibble(w_ortho),\n    \"scores\" = tibble::as_tibble(t_ortho),\n    \"loadings\" = tibble::as_tibble(p_ortho),\n    \"angle\" = angle,\n    \"R2\" = R2\n  )\n  return(res)\n}"
  },
  {
    "objectID": "blog/posts/post4/index.html#implementation",
    "href": "blog/posts/post4/index.html#implementation",
    "title": "Exploring Three Orthogonal Signal Correction (OSC) Algorithms",
    "section": "",
    "text": "We begin by implementing these algorithms, creating functions named wold_osc, sjoblom_osc, and fearn_osc. Each function takes five key parameters. The first parameter, x, represents the input data matrix, which typically contains spectral or chemical measurements. The second parameter, y, corresponds to the target variable or response vector. The ncomp parameter specifies the number of orthogonal components to extract, while tol sets the tolerance level for convergence, determining the stopping criterion for iterations. Finally, max.iter establishes the maximum number of iterations allowed during the optimization process. The function definitions for these algorithms follow this structure:\nwold_osc &lt;- function(x, y, ncomp, tol, max.iter)\nsjoblom_osc &lt;- function(x, y, ncomp, tol, max.iter)\nfearn_osc &lt;- function(x, y, ncomp, tol, max.iter)\nTo begin, the original data matrix x is stored, and empty lists are initialized to hold the extracted principal components, weights, and scores for each component. This step ensures that the algorithm’s outputs are organized for further processing or analysis:\nx_original &lt;- x\nps &lt;- ws &lt;- ts &lt;- vector(\"list\", ncomp)\nThe algorithm proceeds with a loop to extract the specified number of orthogonal components. For each iteration, Principal Component Analysis (PCA) is performed on the current x matrix without centering, using the stats::prcomp function. The initial score vector t is derived from the first principal component. Variables .iter and .diff are initialized to track the number of iterations and the difference between successive score vectors, which serves as the convergence criterion.\nWithin the loop, the orthogonalization process begins. Variation correlated with the response variable y is iteratively removed from the score vector t, refining its orthogonality. Weights, representing the relationship between the input matrix x and the score vector, are calculated and normalized to unit length. A new score vector is then computed, and the convergence check compares the difference between successive score vectors (.diff) to the tolerance level (tol). The loop continues until the difference falls below the specified tolerance or the maximum number of iterations is reached.\nwhile (.diff &gt; tol && .iter &lt; max.iter) {\n  .iter &lt;- .iter + 1\n  t_new &lt;- t - y %*% MASS::ginv(crossprod(y, y)) %*% crossprod(y, t)\n  w &lt;- crossprod(x, t_new) %*% MASS::ginv(crossprod(t_new, t_new))\n  w &lt;- w / sqrt(sum(w^2))\n  t_new &lt;- x %*% w\n  .diff &lt;- sqrt(sum((t_new - t)^2) / sum(t_new^2))\n  t &lt;- t_new\n}\nAfter achieving convergence, a Partial Least Squares (PLS) model is fitted to the data using the extracted scores. The weights and scores are updated, and the loadings are computed. At this stage, y-correlated variation is removed, and the input matrix x is deflated by subtracting the modeled variation. This step prepares the matrix for the next orthogonal component extraction.\nThe extracted weights, loadings, and scores for each orthogonal component are stored in their respective lists:\nws[[i]] &lt;- w\nps[[i]] &lt;- p\nts[[i]] &lt;- t\nOnce all components are extracted, the results are combined to construct the orthogonal components matrix. The orthogonally corrected matrix x_osc is then computed by removing the contributions of the orthogonal components from the original data matrix:\nx_osc &lt;- x_original - x_original %*% tcrossprod(w_ortho, p_ortho)\nFinally, to evaluate the algorithm’s performance, two metrics are computed. The percentage of variation removed R2 quantifies how effectively the algorithm deflates the input matrix, while the angle between the orthogonal scores and the target variable y provides insight into the degree of orthogonality achieved. These metrics allow us to assess the quality and effectiveness of the orthogonal signal correction methods."
  },
  {
    "objectID": "blog/posts/post4/index.html#exemple",
    "href": "blog/posts/post4/index.html#exemple",
    "title": "Exploring Three Orthogonal Signal Correction (OSC) Algorithms",
    "section": "",
    "text": "We will use the beer dataset introduced in our previous post. The dataset consists of Near-Infrared Spectroscopy (NIRS) spectra collected from 80 beer samples, x_matrix. The target variable of interest is the Original Gravity (OG), also known as the original extract, y_target. This parameter measures the concentration of dissolved solids in the wort before fermentation begins, providing a crucial indicator of the brewing process.\n\n\nShow the code\nbeer &lt;- readr::read_csv(\"beer.csv\", show_col_types = FALSE) |&gt; dplyr::rename(extract = originalGravity)\nx_matrix &lt;- beer |&gt; dplyr::select(-extract) |&gt; as.matrix() |&gt; scale(scale = FALSE) \ny_target &lt;- beer |&gt; dplyr::pull(extract)\n\n\nNext, we will perform standard PCA and PLS on the NIRS spectra of the beer dataset to assess how applying orthogonal correction modifies the data structure.\n\n\nShow the code\nset.seed(123)\nn_samples &lt;- as.integer(nrow(x_matrix))\n# PCA\npca_result &lt;- stats::prcomp(x_matrix, center = FALSE, scale = FALSE)\npca_scores &lt;- pca_result$x\npca_df &lt;- data.frame(\n  comp1 = pca_scores[, 1],\n  comp2 = pca_scores[, 2],\n  extract = y_target,\n  Sample = factor(n_samples)\n)\n\n# PLS\npls_result &lt;- pls::plsr(y_target ~ x_matrix, ncomp = 10, validation = \"none\")\npls_scores &lt;- pls::scores(pls_result)[, 1:10]\npls_df &lt;- data.frame(\n  comp1 = pls_scores[, 1],\n  comp2 = pls_scores[, 2],\n  extract = y_target,\n  Sample = factor(n_samples)\n)\n\n# Hotelling's T-squared ellipse\npca_T2ellipse &lt;- HotellingEllipse::ellipseParam(pca_scores, k = 2, pcx = 1, pcy = 2)\npls_T2ellipse &lt;- HotellingEllipse::ellipseParam(pls_scores, k = 2, pcx = 1, pcy = 2)\n\n\n\n\n\n\n\n\n\n\n\nNow, we can apply the OSC algorithms to the dataset and compare their impact. Specifically, we’ll examine how the OSC filtering affects the distribution of samples in the reduced-dimensionality space and whether the variation captured aligns better with the response variable.\n\n\nShow the code\nwold_filter &lt;- wold_osc(x_matrix, y_target, ncomp = 10, tol = 1e10, max.iter = 10)\nsjoblom_filter &lt;- sjoblom_osc(x_matrix, y_target, ncomp = 10, tol = 1, max.iter = 10)\n\nwold_scores &lt;- wold_filter %&gt;%\n  pluck(\"scores\") %&gt;%\n  as_tibble() %&gt;%\n  mutate(extract = y_target)\n\nsjoblom_scores &lt;- sjoblom_filter %&gt;%\n  pluck(\"scores\") %&gt;%\n  as_tibble() %&gt;%\n  mutate(extract = y_target)\n\n\n\n\n\n\n\n\n\n\n\nThe following plot compares the results of OSC, PCA, and PLS modeling by overlaying their respective loadings on a single graph. By presenting their loadings together, we can clearly observe the differences in how each method captures and prioritizes spectral features and filter noise, highlighting their unique contributions and areas of overlap.\n\n\nShow the code\npca_loadings &lt;- pca_result %&gt;% \n  pluck(\"rotation\") %&gt;%\n  as_tibble() %&gt;%\n  mutate(wavelength = rep(seq(1100, 2250, 2), times = 1))\n\npls_loadings &lt;- loadings(pls_result)[, 1:10] %&gt;%\n  as_tibble() %&gt;%\n  janitor::clean_names() %&gt;%\n  mutate(wavelength = rep(seq(1100, 2250, 2), times = 1))\n\nwold_loadings &lt;- wold_filter %&gt;% \n  pluck(\"loadings\") %&gt;%\n  as_tibble() %&gt;%\n  mutate(wavelength = rep(seq(1100, 2250, 2), times = 1))\n  \nsjoblom_loadings &lt;- sjoblom_filter %&gt;% \n  pluck(\"loadings\") %&gt;%\n  mutate(wavelength = rep(seq(1100, 2250, 2), times = 1))\n\n\n\n\n\n\n\n\n\n\n\nUnlike PCA (in red) and PLS (in dark green), which show substantial variability, particularly in the region above 1350 nm, OSC (in blue) effectively filters the noise, reducing it to near-zero levels. This smoothing demonstrates OSC’s ability to isolate and preserve only the information strongly correlated with the target variable while systematically discarding irrelevant or orthogonal components. However, its aggressive filtering comes at the expense of potentially reducing some useful signal.\nThe PCA and PLS loadings, on the other hand, display pronounced fluctuations, reflecting their sensitivity to variance within the dataset. PCA, focusing solely on maximizing variance without considering the target variable, captures not only relevant features but also substantial noise. PLS, while more targeted as it incorporates the correlation with the target variable, still exhibits residual noise, especially in the higher wavelengths, indicating its partial retention of irrelevant variance."
  },
  {
    "objectID": "talks.html#conference-presentations",
    "href": "talks.html#conference-presentations",
    "title": "Talks",
    "section": "Conference Presentations",
    "text": "Conference Presentations\n\nC. L. Goueguel, Direct determination of soil texture using laser-induced breakdown spectroscopy and multivariate linear regressions. Annual meetings of the North American Society for Laser-Induced Breakdown Spectroscopy (NASLIBS) hosted by FACSS SciX Conference 2019 (Palm Springs, CA, USA)\nC. L. Goueguel, Using LIBS analyzer for laser-based soil analysis for precision agriculture. Invited speaker at the Canadian Society for Analytical Sciences and Spectroscopy, Jan. 1, 2018 (Toronto, ON, Canada)\nJ. C. Jain, C. L. Goueguel, D. L. McIntyre, H. M. Edenborn, Dissolution of mineral carbonates with increasing CO2 pressure and the implications for carbon sequestration leak detection. GSA annual meeting, 2016 (Denver, CO, USA)\nC. G. Carson, C. Goueguel, J. Jain, D. McIntyre, Development of laser induced breakdown spectroscopy sensor to assess groundwater quality impacts resulting from geologic carbon sequestration. SPIE Micro- and Nanotechnology Sensors, Systems, and Applications VII, 2015 (Baltimore, MD, USA)\nC. L. Goueguel, J. C. Jain, D. L. McIntyre, A. K. Karamalidis, Application of laser induced breakdown spectroscopy (LIBS) to analyze CO2-bearing solutions in high pressure high temperature environment. ACS 248th - National Meeting, 2014 (San Francisco, CA, USA)\nC. L. Goueguel, J. C. Jain, A. K. Karamalidis, D. L. McIntyre, J. P. Singh, Laser-induced breakdown spectroscopy of high-pressure carbonated brine solutions. 2014 PITTCON Conference & Expo, 2014 (Chicago, IL, USA)\nC. L. Goueguel, D. L. McIntyre, J. C. Jain, J. P. Singh, A. K. Karamalidis, Analysis of calcium in CO2-laden brine (NaCl-CaCl2) by Laser-induced breakdown spectroscopy (LIBS). Annual meetings of the North American Society for Laser-Induced Breakdown Spectroscopy (NASLIBS) hosted by FACSS SciX Conference 2013 (Milwaukee, WI, USA)\nJ. P. Singh, C L. Goueguel, D. L. McIntyre, J. C. Jain, A. K. Karamalidis, Laser-induced breakdown spectroscopy for elemental analysis of brines. 7th International Conference on Laser-Induced Breakdown Spectroscopy, 2012 (Luxor, Egypt)\nC. Goueguel, Laser Induced Breakdown Spectroscopy (LIBS): From Mars exploration to GCS monitoring. Postdoctoral Seminar at the Department of Civil and Environmental Engineering, Carnegie Mellon University, 2012 (Pittsburgh, PA, USA)\nC. Goueguel, S. Laville, F. Vidal, M. Sabsabi, M. Chaker, Investigation of resonance-enhanced laser-induced breakdown spectroscopy (RELIBS) for the analysis of aluminum alloys. 6th International Conference on Laser- Induced Breakdown Spectroscopy, 2010 (Memphis, TN, USA)\nC. Goueguel, S. Laville, H. Loudyi, F. Vidal, M. Sabsabi, M. Chaker, Investigation of resonance-enhanced laser-induced breakdown spectroscopy for the analysis of aluminum alloys. 2nd North American Symposium on Laser-Induced Breakdown Spectroscopy, 2009 (New Orleans, LA, USA)\nC. Goueguel, H. Loudyi, S. Laville, M. Chaker, M. Sabsabi, F. Vidal, Excitation spectrale sélective d’un plasma d’aluminium par une seconde impulsion laser: Optimisation des paramètres expérimentaux. Colloque Plasma- Québec, 2009 (Montreal, QC, Canada)\nS. Laville, K. Rifai, H. Loudyi, M. Chaker, M. Sabsabi, F. Vidal, C. Goueguel, Analysis of trace elements in liquids using LIBS combined with laser-induced fluorescence. 5th International Conference on Laser-Induced Breakdown Spectroscopy, 2008 (Berlin, Germany)\nF. Vidal, M. Chaker, C. Goueguel, S. Laville, H. Loudyi, K. Rifai, M. Sabsabi, Enhanced laser-induced breakdown spectroscopy by second pulse selective wavelength excitation. Laser and plasma applications in materials science: 1st International Conference on Laser Plasma Applications in Materials Science, LAPAMS, 2008 (Alger, Algeria)\nC. Goueguel, S. Laville, H. Loudyi, M. Chaker, M. Sabsabi, F. Vidal. Detection of lead in brass by Laser-Induced Breakdown Spectroscopy combined with Laser-Induced Fluorescence. Photonics North, 2008 (Montreal, QC, Canada)\nH. Loudyi, C. Goueguel, K. Rifai, S. Laville, M. Sabsabi, M. Chaker, F. Vidal, Enhancing the sensitivity of the laser-Induced Breakdown Spectroscopy technique: spectrally selective excitation of specific elements in laser produced plasma. Canadian Association of Physicists Congress, 2008 (Laval, QC, Canada)\nC. Goueguel, H. Loudyi, S. Laville, M. Chaker, M. Sabsabi, F. Vidal, Analyse des matériaux solides par LIBS: Effet d’une deuxième impulsion laser basée sur une excitation spectrale sélective. Colloque Plasma-Québec, 2008 (Montreal, QC, Canada)\nF. Vidal, S. Laville, C. Goueguel, H. Loudyi, M. Chaker, M. Sabsabi, Investigation of laser-induced breakdown spectroscopy combined with laser- induced fluorescence. 4th Euro-Mediterranean Symposium on Laser-Induced Breakdown Spectroscopy, 2007 (Paris, France)"
  },
  {
    "objectID": "blog/blog.html",
    "href": "blog/blog.html",
    "title": "Christian L. Goueguel",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nA Comparative Exploration of Orthogonal Signal Correction Methods\n\n\n\n\n\nOrthogonal signal correction (OSC) is a powerful preprocessing technique frequently used to remove variation in spectral data that is orthogonal to the property of interest. Over the years, several implementations of OSC have emerged, with the most notable being those by Wold et al., Sjöblom et al., and Fearn. This post compares these three methods, exploring their algorithmic approaches and practical implications.\n\n\n\n\n\nMay 25, 2023\n\n\nChristian L. Goueguel\n\n\n13 min\n\n\n\n\n\n\n\n\n\n\n\n\nAn Overview of Orthogonal Partial Least Squares\n\n\n\n\n\nHave you ever heard of Orthogonal Partial Least-Squares (OPLS)? This article aims to give you a clear and concise overview of OPLS and its advantages in the development of more efficient predictive models.\n\n\n\n\n\nDec 20, 2019\n\n\nChristian L. Goueguel\n\n\n6 min\n\n\n\n\n\n\n\n\n\n\n\n\nChemometric Modeling with Tidymodels: A Tutorial for Spectroscopic Data\n\n\n\n\n\nIn this post, we demonstrate how to build robust chemometric models for spectroscopic data using the Tidymodels framework in R. This workflow is designed to cater to beginners and advanced practitioners alike, offering an end-to-end guide from data preprocessing to model evaluation and interpretation.\n\n\n\n\n\nApr 17, 2022\n\n\nChristian L. Goueguel\n\n\n10 min\n\n\n\n\n\n\n\n\n\n\n\n\nMultivariate Outlier Detection in High-Dimensional Spectral Data\n\n\n\n\n\nHigh-dimensional data are particularly challenging for outlier detection. Robust PCA methods have been developed to build models that are unaffected by outliers in high dimensions. These outliers are generally characterized by their deviation from the PCA subspace.\n\n\n\n\n\nJan 7, 2020\n\n\nChristian L. Goueguel\n\n\n9 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blog/posts/post5/index.html",
    "href": "blog/posts/post5/index.html",
    "title": "Optical Breakdown Threshold in Water",
    "section": "",
    "text": "Photo by Vinicius Amano.\n\n\n\n\n\n\nMultiphoton ionization (MPI) is a key first step in laser-induced plasma (LIP) [1-4]. This process occurs when a valence electron absorbs multiple photons simultaneously, leading to ionization. The probability of ionization via MPI is proportional to \\(I^{k}\\) (where \\(I\\) is the laser intensity while \\(k\\) is the minimum number of photons required whose total energy meets or exceeds the ionization energy of the valence electron. For example, consider the MPI process for sodium (Na). The first ionization energy \\((\\text{E}_{ion})\\) of Na is ~ 5.14 eV. To ionize a valence electron from the ground state \\((\\text{E}_{0})\\) to the free state, the simultaneous absorption of five photons from a 1064-nm laser pulse is necessary.\nMPI is a nonlinear optical process that becomes prominent at extremely high laser intensities, resulting in a higher breakdown threshold compared to the cascade ionization (CI) mechanism. Additionally, MPI dominates in the femtosecond regime, where the ultra-short laser pulse duration is insufficient to sustain the cascading free electron generation necessary for CI. Consequently, MPI is the primary absorption mechanism under such conditions.\n\n\n\nIn pure water, seed electrons are primarily generated through the multiphoton ionization (MPI) of water molecules. In contrast, in water containing impurities, seed electrons are more likely produced through the ionization of impurities by thermal excitation, which establishes an initial density of free electrons within the laser’s focal volume. Notably, achieving multiphoton initiation of cascade ionization (CI) in pure water requires significantly higher laser intensities. However, the presence of impurities can drastically lower the breakdown threshold and facilitate plasma formation by providing additional seed electrons. For instance, saline water demonstrates this phenomenon effectively. In such cases, seed electrons originate from the ionization of easily ionizable elements (EIE), such as sodium. These free electrons, liberated from the outermost shells of Na atoms, acquire sufficient kinetic energy through inverse Bremsstrahlung (IB) absorption—a process in which seed electrons absorb laser photons during collisions with heavy particles like molecules or ions. This energy gain enables the electrons to produce a cascade of additional free electrons, ultimately resulting in water breakdown.\nFor breakdown to occur, the rate of electron energy gain via IB absorption must exceed the rate of energy loss due to inelastic collisions. Similarly, the ionization rate must surpass the loss of free electrons through electron-ion recombination and diffusion out of the focal volume. Consequently, the laser intensity within the focal volume must be sufficiently high to drive these energy gains and maintain a net increase in free electrons. Finally, CI is the dominant breakdown mechanism in most scenarios, especially when long-duration laser pulses (in the nanosecond regime) are employed.\n\n\n\n\nIn this section, we summarize some published theoretical modeling results and experimental measurements of breakdown thresholds in water. Before going any further, it is important to remember that for aqueous solutions, in the nanosecond regime, the breakdown threshold is significantly higher for MPI compared to CI. As the duration of the laser pulse decreases, MPI begins to dominate the IC mechanism, both for pure water and for water with impurities. In addition, the uncertainty associated with the search for seed electrons in the focal volume at a given time makes breakdown by CI a probabilistic process, which defined breakdown threshold in terms of breakdown probability.\n\n\nBreakdown threshold in water can be defined experimentally as the minimum laser intensity required for observing a spark, the signal emitted by plasma (which rather refers to the plasma emission threshold), the appearance of bubbles in water, the production of shock waves or the generation of acoustic sound. Breakdown defined as the appearance of a spark visible to the naked eye or detectable by an optical imaging system is the approach commonly used by many authors. In contrast, in theoretical modeling, the breakdown threshold is commonly defined as the minimum laser intensity required for the production of a sufficiently dense plasma to significantly absorb laser photons. The latter is typically defined by a critical free electron density \\(N_{cr}\\) of approximately \\(10^{19}-10^{20} \\text{cm}^{3}\\) in the focal volume, which has a cylindrical shape given by [1]:\n\\[V = (\\sqrt{2}-1)\\frac{πd^{2}}{2}\\frac{f^{2}\\theta}{D_{0}}\\] where \\(d\\) is the beam spot size, \\(f\\) is the focal length of the focusing lens, \\(\\theta\\) is the beam divergence and \\(D_{0}\\) is the unfocused beam diameter.\n\n\nThe rate equation of free electrons density under the MPI mechanism is expressed by [1]:\n\\[\\frac{\\text{d}}{\\text{dt}}(N_{mpi}) = \\frac{2 \\omega}{9 \\pi} \\left( \\frac{m^{'}}{ℏ} \\right )^{3/2} \\left( \\frac{e^{2}}{16nc\\epsilon_{0}m^{'} \\Delta E \\omega^{2}} \\right)^{k} I^{k}e^{(2kI)} \\Phi \\cdot \\sqrt{2 \\left( k-\\frac{\\Delta E}{ℏ\\omega} \\right)}\\] with, \\(\\Phi(x) = e^{(-x^{2})}∫_{0}^{x}e^{y^{2}}\\text{d}y\\), and \\(m^{'} = \\frac{m_{e}m_{ℏ}}{m_{e} + m_{ℏ}}\\), where \\(\\omega\\) is the angular frequency of laser light, \\(I\\) is the laser intensity, \\(m^{'}\\) is the reduced exciton mass, \\(e\\) is the electron charge, \\(n\\) is the refractive index of water, \\(c\\) is the velocity of light, \\(k\\) is degree of non-linearity of MPI (i.e. the smallest number of laser photons required for MPI), \\(ℏ\\) the reduced Planck constant, \\(\\epsilon_{0}\\) is permittivity of free space and \\(\\Delta E\\) is the ionization potential of water. While the rate equation under CI mechanism is expressed by [1]:\n\\[N_{ci} = \\frac{1}{\\omega^{2}\\tau_{m}^{2} + 1} \\left( \\frac{e^{2}\\tau_{m}}{cnm\\epsilon_{0} \\Delta E} I - \\frac{m \\omega^{2} \\tau_{m}}{M} \\right)\\] where \\(\\tau_{m}\\) is the mean free time between electron-heavy particle collisions, \\(M\\) is the mass of the liquid molecule, and \\(m\\) is the electron mass. Following the above equations, the rate equation under the combined effect of both MPI and CI mechanisms is expressed as [1]:\n\\[\\frac{\\text{d}}{\\text{dt}}(N_{e}) = \\frac{\\text{d}}{\\text{dt}}(N_{mpi}) + N_{ci}N_{e} - (RN_{e} + D)N_{e}\\] where \\(N_{e}\\) is the free electron density, \\(D\\) and \\(R\\) are losses by diffusion and recombination, respectively.\n\n\n\n\nThe laser intensity threshold required for the optical breakdown and plasma formation is a function of both the characteristics of the liquid (ionization energy, level of impurities) and the characteristics of the laser beam (wavelength, pulse duration, and spot size).\n\n\nThis graph illustrates the breakdown threshold (irradiance threshold, (\\(\\text{I}_{th}​\\)) as a function of the laser pulse width (pulse duration) for pure and impure water, with the laser operating at a wavelength of 1064 nm and a focal spot size of \\(d = 30 \\mu\\)m.\n\n\n\nFig.1: Breakdown threshold as a function of the laser pulse duration in pure and impure water. Laser spot size is fixed at 30 um and wavelength is at 1064 nm [4].\n\n\nFor ultrashort pulse durations in the femtosecond regime (leftmost part of the graph), the breakdown threshold is very high. This is because multiphoton ionization (MPI) dominates, requiring simultaneous absorption of multiple photons to ionize valence electrons. As the pulse width increases into the picosecond and nanosecond regimes (rightward on the graph), the breakdown threshold significantly decreases. In this range, cascade ionization becomes dominant, which involves free electron generation through seed electrons and their subsequent acceleration via inverse Bremsstrahlung absorption.\nThe difference between the dashed and solid lines in the nanosecond regime highlights the significant role impurities play in lowering the breakdown threshold through thermal excitation.\n\n\n\nThis graph shows the breakdown threshold (irradiance threshold) as a function of pulse duration for pure water at various focal spot sizes (10, 30, and 90 \\(\\mu\\)m) compared to impure water, using a laser wavelength of 1064 nm.\n\n\n\nFig.2: Breakdown threshold as a function of the laser pulse duration and spot size in pure and impure water. Laser wavelength is at 1064 nm [4].\n\n\nAs the pulse duration increases from the femtosecond to the nanosecond regime, the threshold decreases significantly. In pure water, the breakdown threshold is higher and depends strongly on the focal spot size, with smaller focal spots (e.g., 10 \\(\\mu\\)m) requiring higher irradiance to achieve breakdown.\n\n\n\nThis graph illustrates the dependence of the water breakdown threshold on laser wavelength as a function of pulse width. Notably, it reveals a significant decrease in the breakdown threshold at a laser wavelength of 532 nm compared to 1064 nm, highlighting the wavelength’s impact on the process.\n\n\n\nFig.3: Breakdown threshold as a function of the laser pulse duration and wavelength in pure and impure water. Laser spot size is set at 30 um [4].\n\n\n\n\n\n\n\nThis post highlights the strong dependence of the breakdown threshold on the duration of the laser pulse. When using ultrashort pulses in the femtosecond range, a significantly high-intensity laser source is required in the focal volume due to the dominance of multiphoton ionization as the primary absorption mechanism. This also explains why laser sources in the Visible or UV range, which provide higher-energy photons, are more effective for such applications than IR sources. In contrast, laser pulses in the nanosecond regime allow for a lower breakdown threshold by leveraging the thermal excitation of impurities present in the water. However, it is essential to conduct measurements well above the breakdown threshold, as the threshold itself is governed by probabilistic mechanisms, unlike the more deterministic breakdown behavior observed in the femtosecond regime.\n\n\n\n\nGaabour, L. H., Gamal, Y. E. E. D. & Abdellatif, G. Numerical Investigation of the Plasma Formation in Distilled Water by Nd-YAG Laser Pulses of Different Duration. J Mod Phys 03, 1683–1691 (2012).\nHammer, D. X. et al. Experimental investigation of ultrashort pulse laser-induced breakdown thresholds in aqueous media. IEEE J Quantum Elect 32, 670–678 (1996).\nVogel, A., Nahen, K., Theisen, D. & Noack, J. Plasma formation in water by picosecond and nanosecond Nd:YAG laser pulses. I. Optical breakdown at threshold and superthreshold irradiance. IEEE J Sel Top Quant 2, 847–860 (1996).\nKennedy, P. A first-order model for computation of laser-induced breakdown thresholds in ocular and aqueous media. I. Theory. Progress in Quantum Electronics (1995)."
  },
  {
    "objectID": "blog/posts/post5/index.html#absorption-mechanisms",
    "href": "blog/posts/post5/index.html#absorption-mechanisms",
    "title": "Optical Breakdown Threshold in Water",
    "section": "",
    "text": "Multiphoton ionization (MPI) is a key first step in laser-induced plasma (LIP) [1-4]. This process occurs when a valence electron absorbs multiple photons simultaneously, leading to ionization. The probability of ionization via MPI is proportional to \\(I^{k}\\) (where \\(I\\) is the laser intensity while \\(k\\) is the minimum number of photons required whose total energy meets or exceeds the ionization energy of the valence electron. For example, consider the MPI process for sodium (Na). The first ionization energy \\((\\text{E}_{ion})\\) of Na is ~ 5.14 eV. To ionize a valence electron from the ground state \\((\\text{E}_{0})\\) to the free state, the simultaneous absorption of five photons from a 1064-nm laser pulse is necessary.\nMPI is a nonlinear optical process that becomes prominent at extremely high laser intensities, resulting in a higher breakdown threshold compared to the cascade ionization (CI) mechanism. Additionally, MPI dominates in the femtosecond regime, where the ultra-short laser pulse duration is insufficient to sustain the cascading free electron generation necessary for CI. Consequently, MPI is the primary absorption mechanism under such conditions.\n\n\n\nIn pure water, seed electrons are primarily generated through the multiphoton ionization (MPI) of water molecules. In contrast, in water containing impurities, seed electrons are more likely produced through the ionization of impurities by thermal excitation, which establishes an initial density of free electrons within the laser’s focal volume. Notably, achieving multiphoton initiation of cascade ionization (CI) in pure water requires significantly higher laser intensities. However, the presence of impurities can drastically lower the breakdown threshold and facilitate plasma formation by providing additional seed electrons. For instance, saline water demonstrates this phenomenon effectively. In such cases, seed electrons originate from the ionization of easily ionizable elements (EIE), such as sodium. These free electrons, liberated from the outermost shells of Na atoms, acquire sufficient kinetic energy through inverse Bremsstrahlung (IB) absorption—a process in which seed electrons absorb laser photons during collisions with heavy particles like molecules or ions. This energy gain enables the electrons to produce a cascade of additional free electrons, ultimately resulting in water breakdown.\nFor breakdown to occur, the rate of electron energy gain via IB absorption must exceed the rate of energy loss due to inelastic collisions. Similarly, the ionization rate must surpass the loss of free electrons through electron-ion recombination and diffusion out of the focal volume. Consequently, the laser intensity within the focal volume must be sufficiently high to drive these energy gains and maintain a net increase in free electrons. Finally, CI is the dominant breakdown mechanism in most scenarios, especially when long-duration laser pulses (in the nanosecond regime) are employed."
  },
  {
    "objectID": "blog/posts/post5/index.html#breakdown-threshold",
    "href": "blog/posts/post5/index.html#breakdown-threshold",
    "title": "Optical Breakdown Threshold in Water",
    "section": "",
    "text": "In this section, we summarize some published theoretical modeling results and experimental measurements of breakdown thresholds in water. Before going any further, it is important to remember that for aqueous solutions, in the nanosecond regime, the breakdown threshold is significantly higher for MPI compared to CI. As the duration of the laser pulse decreases, MPI begins to dominate the IC mechanism, both for pure water and for water with impurities. In addition, the uncertainty associated with the search for seed electrons in the focal volume at a given time makes breakdown by CI a probabilistic process, which defined breakdown threshold in terms of breakdown probability.\n\n\nBreakdown threshold in water can be defined experimentally as the minimum laser intensity required for observing a spark, the signal emitted by plasma (which rather refers to the plasma emission threshold), the appearance of bubbles in water, the production of shock waves or the generation of acoustic sound. Breakdown defined as the appearance of a spark visible to the naked eye or detectable by an optical imaging system is the approach commonly used by many authors. In contrast, in theoretical modeling, the breakdown threshold is commonly defined as the minimum laser intensity required for the production of a sufficiently dense plasma to significantly absorb laser photons. The latter is typically defined by a critical free electron density \\(N_{cr}\\) of approximately \\(10^{19}-10^{20} \\text{cm}^{3}\\) in the focal volume, which has a cylindrical shape given by [1]:\n\\[V = (\\sqrt{2}-1)\\frac{πd^{2}}{2}\\frac{f^{2}\\theta}{D_{0}}\\] where \\(d\\) is the beam spot size, \\(f\\) is the focal length of the focusing lens, \\(\\theta\\) is the beam divergence and \\(D_{0}\\) is the unfocused beam diameter.\n\n\nThe rate equation of free electrons density under the MPI mechanism is expressed by [1]:\n\\[\\frac{\\text{d}}{\\text{dt}}(N_{mpi}) = \\frac{2 \\omega}{9 \\pi} \\left( \\frac{m^{'}}{ℏ} \\right )^{3/2} \\left( \\frac{e^{2}}{16nc\\epsilon_{0}m^{'} \\Delta E \\omega^{2}} \\right)^{k} I^{k}e^{(2kI)} \\Phi \\cdot \\sqrt{2 \\left( k-\\frac{\\Delta E}{ℏ\\omega} \\right)}\\] with, \\(\\Phi(x) = e^{(-x^{2})}∫_{0}^{x}e^{y^{2}}\\text{d}y\\), and \\(m^{'} = \\frac{m_{e}m_{ℏ}}{m_{e} + m_{ℏ}}\\), where \\(\\omega\\) is the angular frequency of laser light, \\(I\\) is the laser intensity, \\(m^{'}\\) is the reduced exciton mass, \\(e\\) is the electron charge, \\(n\\) is the refractive index of water, \\(c\\) is the velocity of light, \\(k\\) is degree of non-linearity of MPI (i.e. the smallest number of laser photons required for MPI), \\(ℏ\\) the reduced Planck constant, \\(\\epsilon_{0}\\) is permittivity of free space and \\(\\Delta E\\) is the ionization potential of water. While the rate equation under CI mechanism is expressed by [1]:\n\\[N_{ci} = \\frac{1}{\\omega^{2}\\tau_{m}^{2} + 1} \\left( \\frac{e^{2}\\tau_{m}}{cnm\\epsilon_{0} \\Delta E} I - \\frac{m \\omega^{2} \\tau_{m}}{M} \\right)\\] where \\(\\tau_{m}\\) is the mean free time between electron-heavy particle collisions, \\(M\\) is the mass of the liquid molecule, and \\(m\\) is the electron mass. Following the above equations, the rate equation under the combined effect of both MPI and CI mechanisms is expressed as [1]:\n\\[\\frac{\\text{d}}{\\text{dt}}(N_{e}) = \\frac{\\text{d}}{\\text{dt}}(N_{mpi}) + N_{ci}N_{e} - (RN_{e} + D)N_{e}\\] where \\(N_{e}\\) is the free electron density, \\(D\\) and \\(R\\) are losses by diffusion and recombination, respectively.\n\n\n\n\nThe laser intensity threshold required for the optical breakdown and plasma formation is a function of both the characteristics of the liquid (ionization energy, level of impurities) and the characteristics of the laser beam (wavelength, pulse duration, and spot size).\n\n\nThis graph illustrates the breakdown threshold (irradiance threshold, (\\(\\text{I}_{th}​\\)) as a function of the laser pulse width (pulse duration) for pure and impure water, with the laser operating at a wavelength of 1064 nm and a focal spot size of \\(d = 30 \\mu\\)m.\n\n\n\nFig.1: Breakdown threshold as a function of the laser pulse duration in pure and impure water. Laser spot size is fixed at 30 um and wavelength is at 1064 nm [4].\n\n\nFor ultrashort pulse durations in the femtosecond regime (leftmost part of the graph), the breakdown threshold is very high. This is because multiphoton ionization (MPI) dominates, requiring simultaneous absorption of multiple photons to ionize valence electrons. As the pulse width increases into the picosecond and nanosecond regimes (rightward on the graph), the breakdown threshold significantly decreases. In this range, cascade ionization becomes dominant, which involves free electron generation through seed electrons and their subsequent acceleration via inverse Bremsstrahlung absorption.\nThe difference between the dashed and solid lines in the nanosecond regime highlights the significant role impurities play in lowering the breakdown threshold through thermal excitation.\n\n\n\nThis graph shows the breakdown threshold (irradiance threshold) as a function of pulse duration for pure water at various focal spot sizes (10, 30, and 90 \\(\\mu\\)m) compared to impure water, using a laser wavelength of 1064 nm.\n\n\n\nFig.2: Breakdown threshold as a function of the laser pulse duration and spot size in pure and impure water. Laser wavelength is at 1064 nm [4].\n\n\nAs the pulse duration increases from the femtosecond to the nanosecond regime, the threshold decreases significantly. In pure water, the breakdown threshold is higher and depends strongly on the focal spot size, with smaller focal spots (e.g., 10 \\(\\mu\\)m) requiring higher irradiance to achieve breakdown.\n\n\n\nThis graph illustrates the dependence of the water breakdown threshold on laser wavelength as a function of pulse width. Notably, it reveals a significant decrease in the breakdown threshold at a laser wavelength of 532 nm compared to 1064 nm, highlighting the wavelength’s impact on the process.\n\n\n\nFig.3: Breakdown threshold as a function of the laser pulse duration and wavelength in pure and impure water. Laser spot size is set at 30 um [4]."
  },
  {
    "objectID": "blog/posts/post5/index.html#summary",
    "href": "blog/posts/post5/index.html#summary",
    "title": "Optical Breakdown Threshold in Water",
    "section": "",
    "text": "This post highlights the strong dependence of the breakdown threshold on the duration of the laser pulse. When using ultrashort pulses in the femtosecond range, a significantly high-intensity laser source is required in the focal volume due to the dominance of multiphoton ionization as the primary absorption mechanism. This also explains why laser sources in the Visible or UV range, which provide higher-energy photons, are more effective for such applications than IR sources. In contrast, laser pulses in the nanosecond regime allow for a lower breakdown threshold by leveraging the thermal excitation of impurities present in the water. However, it is essential to conduct measurements well above the breakdown threshold, as the threshold itself is governed by probabilistic mechanisms, unlike the more deterministic breakdown behavior observed in the femtosecond regime."
  },
  {
    "objectID": "blog/posts/post5/index.html#references",
    "href": "blog/posts/post5/index.html#references",
    "title": "Optical Breakdown Threshold in Water",
    "section": "",
    "text": "Gaabour, L. H., Gamal, Y. E. E. D. & Abdellatif, G. Numerical Investigation of the Plasma Formation in Distilled Water by Nd-YAG Laser Pulses of Different Duration. J Mod Phys 03, 1683–1691 (2012).\nHammer, D. X. et al. Experimental investigation of ultrashort pulse laser-induced breakdown thresholds in aqueous media. IEEE J Quantum Elect 32, 670–678 (1996).\nVogel, A., Nahen, K., Theisen, D. & Noack, J. Plasma formation in water by picosecond and nanosecond Nd:YAG laser pulses. I. Optical breakdown at threshold and superthreshold irradiance. IEEE J Sel Top Quant 2, 847–860 (1996).\nKennedy, P. A first-order model for computation of laser-induced breakdown thresholds in ocular and aqueous media. I. Theory. Progress in Quantum Electronics (1995)."
  },
  {
    "objectID": "blog/posts/post6/index.html",
    "href": "blog/posts/post6/index.html",
    "title": "The Pseudo-Voigt Function",
    "section": "",
    "text": "The pseudo-Voigt function represents a significant advancement in spectroscopy, emerging from the long-standing challenge of accurately modeling spectral line shapes. While Gaussian and Lorentzian profiles have been fundamental tools in spectral data analysis for decades, they often fell short in capturing the complexity of spectral signal. In the mid-20th century, researchers began to recognize the inherent limitations of relying solely on Gaussian (normal distribution) and Lorentzian (Cauchy distribution) profiles. Complex spectral lines often result from multiple broadening mechanisms, including:\n\nDoppler broadening, which occurs due to thermal motion of particles\nCollisional broadening, resulting from interactions between particles\nInstrumental broadening introduced by measurement apparatus\n\nWhen Doppler broadening dominates, the shape of a spectral line is best modeled by a Gaussian profile. In contrast, collisional broadening typically leads to a Lorentzian line shape. However, in many cases, the observed line shape results from the interplay of multiple broadening mechanisms. As a result, the pseudo-Voigt function often provides a more accurate representation of the line shape. Indeed, the pseudo-Voigt function can account for both collisional and Doppler broadening effects. Moreover, natural broadening resulting from the finite lifetime of excited states can also be modeled using the pseudo-Voigt function. Therefore, the pseudo-Voigt function emerged as an elegant solution to these challenges. By combining the characteristics of Gaussian and Lorentzian profiles, it offers a more nuanced representation of spectral line shapes.\nThe pseudo-Voigt function was first derived by Thompson, Cox, and Hastings in 1987 [1], in their study describing the application of the Rietveld refinement technique to synchrotron X-ray data collected from a capillary sample of Al₂O₃ using Debye–Scherrer geometry at the Cornell High Energy Synchrotron Source (CHESS). Their analysis showed that individual peak shapes are accurately modeled by a pseudo-Voigt function, in which the Gaussian and Lorentzian half-widths vary, respectively, with the Bragg angle due to instrumental resolution and particle-size broadening.\n\n\n\nFundamentally, the pseudo-Voigt function is an approximation of the more precise Voigt profile (named after German physicist Woldemar Voigt). The Voigt profile (\\(V\\)) represents the exact convolution of Gaussian and Lorentzian profiles, providing a more accurate description of spectral line shapes, particularly when both Doppler and Lorentzian broadening are significant.\n\\[\nV(x;\\sigma,\\gamma) = \\int_{-\\infty}^{\\infty} G(x';\\sigma) L(x - x';\\gamma) \\, dx'\n\\]\nThe Voigt profile can also be expressed using the Faddeeva function \\(\\omega(z)\\), given by:\n\\[\nw(z) = e^{-z^2} \\left( 1 + \\frac{2i}{\\sqrt{\\pi}} \\int_0^z e^{t^2} \\, dt \\right)\n\\] where \\(z\\) is a complex number. Using the Faddeeva function, the Voigt profile is: \\[\nV(x; \\sigma, \\gamma) = \\frac{1}{\\sigma\\sqrt{2\\pi}}\\Re\\left[w\\left( \\frac{x + i\\gamma}{\\sigma\\sqrt{2}} \\right)\\right]\n\\]\nwhere \\(\\Re[w(z)]\\) denotes the real part of the Faddeeva function.\n\n\n\n\n\n\n\n\n\nHowever, the Voigt profile is computationally intensive, requiring efficient numerical methods to accurately evaluate the convolution integral. This complexity has led to the development of approximations, such as the pseudo-Voigt function, which strike a balance between computational efficiency and maintaining acceptable levels of accuracy in the spectral line shape.\n\n\n\nThe pseudo-Voigt function (\\(pV\\)) is a linear combination of a Gaussian (\\(G\\)) and a Lorentzian (\\(L\\)) function, and is defined as:\n\\[\npV(x;\\eta) = ηG(x;\\sigma) + (1-η)L(x;\\gamma)\n\\]\n\\[\nG(x;\\sigma) = \\frac{1}{\\sigma \\sqrt{2\\pi}} e^{-\\frac{x^2}{2\\sigma^2}}\n\\]\n\\[\nL(x; \\gamma) = \\frac{\\gamma}{\\pi (x^2 + \\gamma^2)}\n\\]\nwhere, \\(\\eta\\) is a mixing parameter that determines the relative contribution of the Lorentzian (\\(\\eta = 0\\)) and Gaussian (\\(\\eta = 1\\)) components. Here, the parameters \\(\\sigma\\) and \\(\\gamma\\) represent, respectively, the standard deviation of the Gaussian component, related to the full width at half maximum (FWHM) of the Gaussian by \\(w_G = 2\\sqrt{2\\ln 2} \\, \\sigma\\), and the half-width at half maximum (HWHM) of the Lorentzian component, related to the Lorentzian FWHM by \\(w_L = 2\\gamma\\).\n\n\n\n\n\n\n\n\n\nThe following animation illustrates how the pseudo-Voigt function varies with the mixing parameter \\(\\eta\\). As \\(\\eta\\) increases from 0 to 1, the contribution of the Gaussian component becomes more significant. Each case has a FWHM of \\(w_G = w_L = 1\\).\n\n\nShow the code\nlibrary(ggplot2)\nlibrary(gganimate)\n\ngaussian &lt;- function(x, x_c, w_G, A, y_0) {\n  y_0 + A / (w_G * sqrt(pi / (4 * log(2)))) * exp(-4 * log(2) * (x - x_c)^2 / w_G^2)\n}\nlorentzian &lt;- function(x, x_c, w_L, A, y_0) {\n  y_0 + A / pi * w_L / ((x - x_c)^2 + w_L^2)\n}\npseudo_voigt &lt;- function(x, x_c, w_L, w_G, A, y_0, eta) {\n  y_0 + A * (eta * gaussian(x, x_c, w_G, A, y_0)  + (1 - eta) * lorentzian(x, x_c, w_L, A, y_0))\n}\n\nx_c &lt;- y_0 &lt;- 0; w_L &lt;- w_G &lt;- A &lt;- 1\neta_values &lt;- seq(0, 1, by = 0.05)\ndf &lt;- data.frame(x = seq(-5, 5, by = 0.1))\nfor (eta in eta_values) {\n  df[[paste0(\"η = \", eta)]] &lt;- pseudo_voigt(df$x, x_c, w_L, w_G, A, y_0, eta)\n}\ndf_long &lt;- tidyr::pivot_longer(df, cols = -x, names_to = \"eta\", values_to = \"y\")\n\np &lt;- df_long |&gt; \n  ggplot(aes(x = x, y = y, colour = eta)) +\n  geom_line(linewidth = 1) +\n  scale_y_continuous(limits = c(0,1)) +\n  geom_point(size = 2) +\n  theme_bw(base_size = 15) +\n  labs(\n    x = \"x\", \n    y = \"y\", \n    title = \"The Pseudo-Voigt Function\",\n    subtitle = paste0(\"Varying η from Lorentzian (η = 0) to Gaussian (η = 1): \", \"{closest_state}\"),\n    caption = \"The mixing parameter η determines the relative contribution of the Lorentzian and Gaussian components.\") +\n  theme(\n    legend.position = \"none\",\n    plot.title = element_text(hjust = 0, size = 18, face = \"bold\"),\n    plot.subtitle = element_text(hjust = 0),\n    panel.background = element_rect(colour = \"black\", linewidth = 1)\n    )\n\nanim &lt;- p +\n  transition_states(eta, transition_length = 2, state_length = 1) +\n  ease_aes('exponential-in-out') +\n  shadow_mark(alpha = 2/10, size = 0.1)\n\nanimate(\n  anim, \n  nframes = 100, \n  fps = 10, \n  width = 600, \n  height = 400, \n  renderer = gifski_renderer()\n  )\n\n\n\n\n\n\n\n\n\n\nIn the early 2000s, Ida et al. [2] refined the pseudo-Voigt function by introducing an extended formula designed to more accurately approximate the Voigt profile. This formula is given by:\n\\[\nepV(x;\\eta_L, \\eta_I, \\eta_P) = (1 - \\eta_L - \\eta_I - \\eta_P)G(x;\\sigma) + \\eta_L L(x; \\gamma) + \\eta_I F_I(x;\\gamma_I) + \\eta_P F_P(x;\\gamma_P)\n\\]\nwhere \\(F_I\\) and \\(F_P\\) are intermediate functions that represent the transition between the Lorentzian and Gaussian profiles, respectively. \\(F_I\\) is an irrational function involving a square root, while \\(F_P\\) is the squared hyperbolic secant function. These functions are defined as follows:\n\\[\nF_I(x;\\gamma_I) = \\frac{1}{2\\gamma_I}\\left( 1 + \\left(\\frac{x}{\\gamma_I}\\right)^2 \\right)^{-3/2}\n\\]\n\\[\nF_P(x;\\gamma_P) = \\frac{1}{2\\gamma_P}\\text{sech}^2\\left(\\frac{x}{\\gamma_P}\\right)\n\\]\nThe FWHMs are given by \\(w_I = 2 \\gamma_I \\sqrt{(2^{2/3} - 1)}\\), and \\(w_P = 2 \\gamma_P \\ln{(\\sqrt{2} + 1)}\\). A Gaussian profile is obtained when the mixing parameters are set to \\(\\eta_L = \\eta_I = \\eta_P = 0\\), while the Lorentzian contribution is governed by the parameter \\(\\eta_L\\). Specifically, a pure Lorentzian profile arises when \\(\\eta_L = 1\\). These parameters are constrained to the range from 0 to 1 and must satisfy the condition \\(\\eta_L + \\eta_I + \\eta_P = 1\\). This constraint defines a 2D simplex within a plane in 3D space, representing all possible combinations of the mixing parameters.\n\n\n\n\n\n\n\n\n\nThe animation below is a visual representation of the normalized extended pseudo-Voigt profiles, illustrating the transition from Gaussian to Lorentzian shapes as the mixing parameters \\(\\eta_L, \\eta_I\\), and \\(\\eta_P\\), are varied simultaneously.\n\n\n\n\n\n\n\n\n\nP. Thompson, D.E. Cox, and J.B. Hastings, Rietveld refinement of Debye–Scherrer synchrotron X-ray data from Al2O3. J. Appl. Cryst. 20, 79-83, 1987.\nT. Ida, M. Ando, H. Toraya, Extended pseudo-Voigt function for approximating the Voigt profile. J. Appl. Cryst. 33, 1311-1316, 2000."
  },
  {
    "objectID": "blog/posts/post6/index.html#background",
    "href": "blog/posts/post6/index.html#background",
    "title": "The Pseudo-Voigt Function",
    "section": "",
    "text": "The pseudo-Voigt function represents a significant advancement in spectroscopy, emerging from the long-standing challenge of accurately modeling spectral line shapes. While Gaussian and Lorentzian profiles have been fundamental tools in spectral data analysis for decades, they often fell short in capturing the complexity of spectral signal. In the mid-20th century, researchers began to recognize the inherent limitations of relying solely on Gaussian (normal distribution) and Lorentzian (Cauchy distribution) profiles. Complex spectral lines often result from multiple broadening mechanisms, including:\n\nDoppler broadening, which occurs due to thermal motion of particles\nCollisional broadening, resulting from interactions between particles\nInstrumental broadening introduced by measurement apparatus\n\nWhen Doppler broadening dominates, the shape of a spectral line is best modeled by a Gaussian profile. In contrast, collisional broadening typically leads to a Lorentzian line shape. However, in many cases, the observed line shape results from the interplay of multiple broadening mechanisms. As a result, the pseudo-Voigt function often provides a more accurate representation of the line shape. Indeed, the pseudo-Voigt function can account for both collisional and Doppler broadening effects. Moreover, natural broadening resulting from the finite lifetime of excited states can also be modeled using the pseudo-Voigt function. Therefore, the pseudo-Voigt function emerged as an elegant solution to these challenges. By combining the characteristics of Gaussian and Lorentzian profiles, it offers a more nuanced representation of spectral line shapes.\nThe pseudo-Voigt function was first derived by Thompson, Cox, and Hastings in 1987 [1], in their study describing the application of the Rietveld refinement technique to synchrotron X-ray data collected from a capillary sample of Al₂O₃ using Debye–Scherrer geometry at the Cornell High Energy Synchrotron Source (CHESS). Their analysis showed that individual peak shapes are accurately modeled by a pseudo-Voigt function, in which the Gaussian and Lorentzian half-widths vary, respectively, with the Bragg angle due to instrumental resolution and particle-size broadening."
  },
  {
    "objectID": "blog/posts/post6/index.html#visualizing-the-pseudo-voigt-function",
    "href": "blog/posts/post6/index.html#visualizing-the-pseudo-voigt-function",
    "title": "The Pseudo-Voigt Function",
    "section": "",
    "text": "The following animation illustrates how the Pseudo-Voigt function varies with the mixing parameter \\(\\eta\\). As \\(\\eta\\) increases from 0 to 1, the contribution of the Gaussian component becomes more significant.\n\n\nShow the code\nlibrary(ggplot2)\nlibrary(gganimate)\n\ngaussian &lt;- function(x, x_c, w_G, A, y_0) {\n  y_0 + A / (w_G * sqrt(pi / (4 * log(2)))) * exp(-4 * log(2) * (x - x_c)^2 / w_G^2)\n}\nlorentzian &lt;- function(x, x_c, w_L, A, y_0) {\n  y_0 + A / pi * w_L / ((x - x_c)^2 + w_L^2)\n}\npseudo_voigt &lt;- function(x, x_c, w_L, w_G, A, y_0, eta) {\n  y_0 + A * (eta * lorentzian(x, x_c, w_L, A, y_0) + (1 - eta) * gaussian(x, x_c, w_G, A, y_0))\n}\nx_c &lt;- y_0 &lt;- 0; w_L &lt;- w_G &lt;- A &lt;- 1\neta_values &lt;- seq(0, 1, by = 0.05)\ndf &lt;- data.frame(x = seq(-5, 5, by = 0.1))\nfor (eta in eta_values) {\n  df[[paste0(\"eta = \", eta)]] &lt;- pseudo_voigt(df$x, x_c, w_L, w_G, A, y_0, eta)\n}\ndf_long &lt;- tidyr::pivot_longer(df, cols = -x, names_to = \"eta\", values_to = \"y\")\n\np &lt;- df_long |&gt; \n  ggplot(aes(x = x, y = y, colour = eta)) +\n  geom_line(linewidth = 1) +\n  geom_point(size = 2) +\n  theme_bw(base_size = 15) +\n  labs(x = \"x\", y = \"y\", title = \"The Pseudo-Voigt Function\",\n    subtitle = paste0(\"Varying η (eta) from Lorentzian (η = 0) to Gaussian (η = 1): \", \"{closest_state}\")) +\n  theme(\n    #panel.grid = element_blank(), \n    legend.position = \"none\",\n    plot.title = element_text(hjust = 0, size = 18, face = \"bold\"),\n    plot.subtitle = element_text(hjust = 0),\n    panel.background = element_rect(colour = \"black\", linewidth = 1)\n    )\n\nanim &lt;- p +\n  transition_states(eta, transition_length = 2, state_length = 1) +\n  ease_aes('exponential-in-out') +\n  shadow_mark(alpha = 0.3, size = 0.1)\n\nanimate(\n  anim, \n  nframes = 100, \n  fps = 10, \n  width = 600, \n  height = 400, \n  renderer = gifski_renderer()\n  )\nanim_save(\"voigt_animated.gif\")"
  },
  {
    "objectID": "blog/posts/post6/index.html#visualizing-the-pseudo-voigt-functions",
    "href": "blog/posts/post6/index.html#visualizing-the-pseudo-voigt-functions",
    "title": "The Pseudo-Voigt Function",
    "section": "",
    "text": "The following animation illustrates how the pseudo-Voigt function varies with the mixing parameter \\(\\eta\\). As \\(\\eta\\) increases from 0 to 1, the contribution of the Gaussian component becomes more significant.\n\n\nShow the code\nlibrary(ggplot2)\nlibrary(gganimate)\n\ngaussian &lt;- function(x, x_c, w_G, A, y_0) {\n  y_0 + A / (w_G * sqrt(pi / (4 * log(2)))) * exp(-4 * log(2) * (x - x_c)^2 / w_G^2)\n}\nlorentzian &lt;- function(x, x_c, w_L, A, y_0) {\n  y_0 + A / pi * w_L / ((x - x_c)^2 + w_L^2)\n}\npseudo_voigt &lt;- function(x, x_c, w_L, w_G, A, y_0, eta) {\n  y_0 + A * (eta * gaussian(x, x_c, w_G, A, y_0)  + (1 - eta) * lorentzian(x, x_c, w_L, A, y_0))\n}\n\nx_c &lt;- y_0 &lt;- 0; w_L &lt;- w_G &lt;- A &lt;- 1\neta_values &lt;- seq(0, 1, by = 0.05)\ndf &lt;- data.frame(x = seq(-5, 5, by = 0.1))\nfor (eta in eta_values) {\n  df[[paste0(\"η = \", eta)]] &lt;- pseudo_voigt(df$x, x_c, w_L, w_G, A, y_0, eta)\n}\ndf_long &lt;- tidyr::pivot_longer(df, cols = -x, names_to = \"eta\", values_to = \"y\")\n\np &lt;- df_long |&gt; \n  ggplot(aes(x = x, y = y, colour = eta)) +\n  geom_line(linewidth = 1) +\n  scale_y_continuous(limits = c(0,1)) +\n  geom_point(size = 2) +\n  theme_bw(base_size = 15) +\n  labs(\n    x = \"x\", \n    y = \"y\", \n    title = \"The Pseudo-Voigt Function\",\n    subtitle = paste0(\"Varying η from Lorentzian (η = 0) to Gaussian (η = 1): \", \"{closest_state}\"),\n    caption = \"The mixing parameter η determines the relative contribution of the Lorentzian and Gaussian components.\") +\n  theme(\n    legend.position = \"none\",\n    plot.title = element_text(hjust = 0, size = 18, face = \"bold\"),\n    plot.subtitle = element_text(hjust = 0),\n    panel.background = element_rect(colour = \"black\", linewidth = 1)\n    )\n\nanim &lt;- p +\n  transition_states(eta, transition_length = 2, state_length = 1) +\n  ease_aes('exponential-in-out') +\n  shadow_mark(alpha = 2/10, size = 0.1)\n\nanimate(\n  anim, \n  nframes = 100, \n  fps = 10, \n  width = 600, \n  height = 400, \n  renderer = gifski_renderer()\n  )\nanim_save(\"pv_animation.gif\")\n\n\n\n\n\n\n\nNext, we visualize the normalized extended pseudo-Voigt profiles, illustrating the transition from Gaussian to Lorentzian shapes as the mixing parameters \\(\\eta_L, \\eta_I\\), and \\(\\eta_P\\), are varied simultaneously."
  },
  {
    "objectID": "blog/posts/post6/index.html#references",
    "href": "blog/posts/post6/index.html#references",
    "title": "The Pseudo-Voigt Function",
    "section": "",
    "text": "P. Thompson, D.E. Cox, and J.B. Hastings, Rietveld refinement of Debye–Scherrer synchrotron X-ray data from Al2O3. J. Appl. Cryst. 20, 79-83, 1987.\nT. Ida, M. Ando, H. Toraya, Extended pseudo-Voigt function for approximating the Voigt profile. J. Appl. Cryst. 33, 1311-1316, 2000."
  },
  {
    "objectID": "blog/posts/post7/index.html",
    "href": "blog/posts/post7/index.html",
    "title": "Beyond Standard Boxplot: The Adjusted and Generalized Boxplots",
    "section": "",
    "text": "Photo by Dana Katharina.\n\n\n\n\nProposed by Tukey in 1977 [1], a boxplot is constructed using five key summary statistics: the minimum, first quartile (Q1), median, third quartile (Q3), and maximum. These elements are complemented by whiskers, which typically extend to 1.5 times the interquartile range (IQR = Q3 - Q1) beyond Q1 and Q3, respectively. Data points outside this range are flagged as potential outliers.\n\\[\n\\begin{aligned}\n    \\text{Lower Fence} &= Q_1 - 1.5 \\cdot \\text{IQR} \\\\\n    \\text{Upper Fence} &= Q_3 + 1.5 \\cdot \\text{IQR}\n\\end{aligned}\n\\]\nThe ends of the whiskers are called adjacent values. They are the smallest and largest values not flagged outliers. Moreover, since IQR has a breakdown point of 0.25, it takes more than 25% of the data to be contaminated by outliers for the masking effect to occur. For example, in the Figure below, we show how the quartiles and therefore IQR are significantly influenced by the outliers, highlighting the breakdown point. After replacing 25% of the data with extreme outliers, the IQR increased drastically, from 13 (clean data) to 136 (contaminated data). This demonstrates that with 25% contamination, the IQR becomes unreliable as a measure of spread.\n\n\nShow the code\nset.seed(42)\ndata_clean &lt;- rnorm(100, mean = 50, sd = 10)\nn_outliers &lt;- floor(length(data_clean) * 0.25)\ndata_contaminated &lt;- data_clean\ndata_contaminated[1:n_outliers] &lt;- runif(n_outliers, min = 500, max = 1000)\n\ndata &lt;- tibble(\n  Value = c(data_clean, data_contaminated),\n  Dataset = rep(\n    c(\"Clean Data\", \"Contaminated Data\\n(25% Outliers)\"), \n    each = length(data_clean)\n    )\n  )\n\nsummary_stats &lt;- data %&gt;%\n  group_by(Dataset) %&gt;%\n  summarise(\n    Q1 = quantile(Value, 0.25),\n    Q3 = quantile(Value, 0.75),\n    IQR = Q3 - Q1\n  )\n\nggplot(data, aes(x = Value, y = Dataset, fill = Dataset)) +\n  geom_boxplot(\n    outlier.shape = 21, \n    outlier.size = 2, \n    outlier.alpha = 0.85,\n    width = .3,\n    staplewidth = 0.5) +\n  geom_vline(\n    data = summary_stats, \n    aes(xintercept = Q1, color = Dataset), \n    linetype = \"dashed\") +\n  geom_vline(\n    data = summary_stats, \n    aes(xintercept = Q3, color = Dataset), \n    linetype = \"dashed\") +\n  scale_fill_manual(values = c(\"skyblue\", \"lightcoral\")) +\n  scale_color_manual(values = c(\"orange\", \"green\")) +\n  labs(x = \"\", y = \"\") +\n  theme_bw() +\n  theme(legend.position = \"none\") +\n  facet_wrap(~Dataset, nrow = 2, ncol = 1, scales = \"free\") +\n  theme(\n    strip.background = element_blank(),\n    strip.text.x = element_blank(),\n    panel.grid = element_blank()\n  )\n\n\n\n\n\n\n\n\n\nBoxplots are a widely used tool for visualizing data distributions, particularly when the data exhibit a symmetrical mesokurtic distribution. However, its effectiveness diminishes when dealing with skewed data. In such cases, the boxplot can misrepresent the dataset’s nuances—most notably by classifying an excessive number of larger values as outliers when the distribution is skewed. Recognizing this limitation, Hubert and Vandervieren [2] introduced a refined version of Tukey’s boxplot rule. Their approach incorporates a robust measure of skewness, allowing for a more accurate depiction of asymmetrical distributions while preserving the simplicity and interpretability of Tukey’s boxplot.\n\n\n\nIn 2004, Brys et al. [3] introduced the medcouple (MC), which is a robust statistic used to measure the skewness of a distribution. Unlike traditional measures of skewness that are sensitive to outliers, MC focuses on the median and the IQR, making it less affected by extreme values. It is based on the difference between left and right data spreads relative to the median. MC is bounded between −1 and 1. A value of 0 indicates a perfectly symmetric distribution, while positive values signify a right-tailed (positively skewed) distribution, and negative values correspond to a left-tailed (negatively skewed) distribution. Importantly, this measure is most effective for moderately skewed distributions, particularly when the absolute value of MC is less than or equal to 0.6 (\\(|\\text{MC}| \\leq 0.6\\)). MC is given by:\n\\[\n\\text{MC} = \\text{med} \\left\\{ \\frac{(x_i - m) - (m - x_j)}{x_i - x_j} \\, \\middle| \\, x_i \\geq m \\geq x_j \\right\\}\n\\]\nwhere, \\(x_i\\) and \\(x_j\\) are data points from the sample, \\(m\\) is the median of the sample, and \\(\\text{med}\\{\\cdot\\}\\) denotes the median operator.\nTo address the limitation of Tukey’s boxplot in handling skewed data, Hubert and Vandervieren [2] introduced the adjusted boxplot for skewed distribution. This method modifies the whisker length based on the MC. Specifically, the upper whisker extends further for right-skewed data, and the lower whisker extends further for left-skewed data. This new approch modifies the traditional boxplot fences to account for skewness in the data using the exponential function of MC as follows:\nWhen \\(\\text{MC} \\geq 0\\):\n\\[\n\\begin{aligned}\n    \\text{Lower Fence} &= Q_1 - k \\cdot e^{-4\\text{MC}} \\cdot \\text{IQR} \\\\\n    \\text{Upper Fence} &= Q_3 + k \\cdot e^{3\\text{MC}} \\cdot \\text{IQR}\n\\end{aligned}\n\\]\nWhen \\(\\text{MC} \\leq 0\\):\n\\[\n\\begin{aligned}\n    \\text{Lower Fence} &= Q_1 - k \\cdot e^{-3\\text{MC}} \\cdot \\text{IQR} \\\\\n    \\text{Upper Fence} &= Q_3 + k \\cdot e^{4\\text{MC}} \\cdot \\text{IQR}\n\\end{aligned}\n\\]\nwhere, \\(k\\) is the fence factor. Hubert and Vandervieren [2] have optimized the adjusted boxplot for \\(k = 1.5\\). In the next Figure, we compare Tukey’s boxplot (left) with the adjusted boxplot (right), applied to a skewed data. The adjusted boxplot shows fewer outliers for the skewed data, reflecting its ability to handle skewness better.\n\n\nShow the code\nset.seed(123)\nskewed_data &lt;- data.frame(y = rgamma(500, shape = 2, scale = 2))\np1 &lt;- skewed_data |&gt; \n  ggplot() +\n  aes(x = \"\", y = y, fill = \"lightcoral\") +\n  geom_boxplot(\n    width = 0.3, \n    staplewidth = 0.5, \n    outlier.fill = \"lightcoral\", \n    outlier.shape = 21,\n    outlier.size = 3,\n    outlier.alpha = 1/3) +\n  ylim(0, 15) +\n  labs(\n    x = \"Tukey's Boxplot\", y = \"\") +\n  theme_bw() +\n  theme(\n    panel.grid = element_blank(),\n    legend.position = \"none\",\n    plot.margin = margin(r = 0)\n    )\n\np2 &lt;- skewed_data |&gt; \n  select(y) |&gt; \n  specProc::adjusted_boxplot(box.width = 0.3, staplewidth = 0.5) +\n  scale_fill_manual(values = \"lightblue\") +\n  ylim(0, 15) +\n  labs(x = \"Adjusted Boxplot\", y = \" \") +\n  theme_bw() +\n  theme(\n    panel.grid = element_blank(),\n    legend.position = \"none\",\n    axis.text.x = element_blank(),\n    axis.text.y = element_blank(),\n    plot.margin = margin(l = 0)\n    )\np1 | p2\n\n\n\n\n\n\n\n\n\nThe adjusted boxplot has garnered attention for its ability to handle skewed data, even earning a mention on platforms like Wikipedia. However, it is not without its critiques.\nFor example, Bruffaerts et al. [4] pointed out that the whiskers in the adjusted boxplot are designed to achieve a fixed outlier detection rate of 0.7%. While this threshold may be suitable for some applications, the process becomes cumbersome if one wishes to adopt a different detection rate. It requires re-running the simulations to determine a new tuning constant or fence factor (\\(k\\)). Moreover, for datasets with heavy-tailed distributions, the adjusted boxplot may still misclassify observations, as the whisker adjustment relies solely on skewness and not on tail behavior. Another significant issue lies in its dependence on MC. While MC is effective for large samples, its estimation can be imprecise for small sample sizes.\nThe Figure below illustrates the behavior of MC estimates for small and large sample sizes using density plots. To achieve this, we generated multiple random samples from a skewed Gamma distribution and examined the variability of the MC estimates across these sample sizes. For small sample sizes, the MC estimates exhibit significant variability, reflecting its reduced robustness in such cases. In contrast, as the sample size increases, the MC estimates stabilize and converge to a consistent value, demonstrating its improved precision and reliability with larger data.\n\n\nShow the code\nset.seed(123)\ncalculate_MC &lt;- function(sample_size, num_simulations = 1000) {\n  mc_values &lt;- numeric(num_simulations)\n  for (i in 1:num_simulations) {\n    data &lt;- rgamma(sample_size, shape = 2, scale = 2)  \n    mc_values[i] &lt;- mc(data)                          \n  }\n  return(mc_values)\n}\n\nsmall_sample_size &lt;- 30\nlarge_sample_size &lt;- 1000\nmc_small &lt;- calculate_MC(small_sample_size)\nmc_large &lt;- calculate_MC(large_sample_size)\n\nmc_data &lt;- data.frame(\n  MC = c(mc_small, mc_large),\n  Sample_Size = rep(c(\"Small (n = 30)\", \"Large (n = 1 000)\"), each = length(mc_small))\n)\n\nggplot(mc_data, aes(x = MC, fill = Sample_Size)) +\n  geom_density(alpha = 0.5) +\n  labs(\n    title = \"Medcouple (MC) Estimates for Small and Large Samples\",\n    subtitle = \"MC estimates are more variable for small samples\",\n    x = \"MC\",\n    y = \"Density\",\n    fill = \"Sample size\"\n  ) +\n  scale_fill_manual(values = c(\"purple\", \"green\")) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nIn response to these shortcomings, Bruffaerts et al. [4] proposed an alternative method to address skewness and heavy-tailed distributions. Their approach introduces a rank-preserving transformation that allows the data to conform to a Tukey \\(g\\)-\\(h\\) distribution.\n\n\n\nIn 1977, Tukey introduced a family of distributions through two nonlinear transformations, giving rise to what is now known as Tukey \\(g\\)-\\(h\\) distributions [5,6]. These distributions are widely used in robust statistics and flexible modeling due to their ability to accommodate a range of skewness and kurtosis levels. Tukey \\(g\\)-\\(h\\) distribution is defined by applying two transformations, \\(g\\) (skewness) and \\(h\\) (kurtosis), to a standard normal random variable \\(Z \\sim \\mathcal{N}(0, 1)\\). The resulting transformation is expressed by:\n\\[\nT_{g,h}(Z) = \\frac{e^{gZ}-1}{g} \\cdot e^{h\\frac{Z^2}{2}} \\quad \\text{with} \\quad g \\neq 0, h \\in \\mathbb{R}\n\\]\nBy adjusting \\(g\\) and \\(h\\), the Tukey \\(g\\)-\\(h\\) distribution can model a wide variety of distribution, from symmetric to skewed, and from light-tailed to heavy-tailed distributions.\nThe constants \\(g\\) and \\(h\\), can be estimated from the empirical quantiles as follows:\n\\[\n\\hat{g} = \\frac{1}{z_p} \\text{ln}\\left(-\\frac{Q_p(\\{x_j\\})}{Q_{1-p}(\\{x_j\\})}\\right),\n\\quad\n\\hat{h} = \\frac{2}{z_p^2} \\text{ln}\\left(-\\hat{g}\\frac{Q_p(\\{x_j\\}) \\cdot Q_{1-p}(\\{x_j\\})}{Q_p(\\{x_j\\}) + Q_{1-p}(\\{x_j\\})}\\right)\n\\]where \\(z_p\\) is the quantile of order \\(p\\) of the standard normal distribution. \\(Q_p\\) and \\(Q_{1-p}\\) are the empirical quantiles of order \\(p\\) (\\(0.5&lt;p&lt;1\\)) and \\(1-p\\) of the univariate data \\(X = \\{x_j\\} = \\{x_1, \\cdots, x_n\\}\\).\nThe generalized boxplot as proposed by Bruffaerts et al. [4] begins by applying a rank-preserving transformation to the data. This transformation maps the original observations onto the unit interval (0, 1), maintaining the order of the data points while capturing key distributional features like skewness and tail behavior. Then, an inverse normal transformation is applied, similar to rank-based approaches. This transformed distribution can be fine-tuned using the Tukey \\(g\\)-\\(h\\) distribution, whose quantiles are used to set the boxplot whiskers.\nThe Figures below illustrate a comparison of Tukey’s boxplot, the adjusted boxplot, and the generalized boxplot across three distinct types of data distributions: (i) a normal distribution, (ii) a right-skewed distribution, and (iii) a heavy-tailed distribution. The Figures show how each method performs and adapts to the specific characteristics of these distributions, effectively balancing sensitivity to outliers without over-identifying them.\nFor a symmetric distribution, all three boxplots produce similar results as expected.\n\n\n\n\n\n\n\n\n\nFor a right-skewed distribution, Tukey’s boxplot overestimates the number of outliers, while the adjusted and generalized boxplots provide a better representation of the underlying data distribution. The generalized boxplot, in particular, offers the most flexible approach by accommodating skewness.\n\n\n\n\n\n\n\n\n\nFor a heavy-tailed distribution, Tukey’s boxplot inaccurately identifies an excessive number of outliers due to its assumption of symmetry and limited adaptability to extreme tails. The adjusted boxplot improved upon Tukey’s boxplot by accounting for skewness in the data, yet it still struggles to effectively handle the variability and extreme values inherent in heavy-tailed distributions. In contrast, the generalized boxplot proves to be a more robust and versatile method. Its whiskers and overall box structure more accurately reflect the underlying data distribution, demonstrating its ability to adapt to both skewness and tail heaviness more effectively than the other two methods.\n\n\n\n\n\n\n\n\n\n\n\n\nBy comparing the three boxplot approaches, we show the limitations of Tukey’s boxplot when applied to skewed or heavy-tailed data. While Tukey’s method assumes symmetry and often flags excessive outliers, the adjusted boxplot improves this by incorporating a skewness measure (medcouple). The generalized boxplot provides the most robust solution by flexibly adapting to both skewness and tail heaviness through the Tukey \\(g\\)-\\(h\\) distribution. Overall, the adjusted and generalized boxplots are better suited for non-symmetric data, with the generalized boxplot offering the most robust approach.\n\n\n\n\nTukey, J.W., (1977). Exploratory Data Analysis, Pearson, 1st edition, page 39.\nHubert, M., Vandervieren, E., (2008). An adjusted boxplot for skewed distributions. Computational Statistics and Data Analysis, 52, 5186-5201.\nBrys, G., Hubert, M., Struyf, A., (2004). A robust measure of skewness. Journal of Computational and Graphical Statistics, 13, 996-1017.\nBruffaerts, C., Verardi, V., Vermandele, C., (2014). A generalized boxplot for skewed and heavy-tailed distributions. Statistics & Probability Letters, 95, 110–117.\nTukey, J.W., (1977). Modern techniques in data analysis. NSF‐sponsored regional research conference at Southeastern Massachusetts University, North Dartmouth, MA.\nMartinez, J., Iglewicz, B., (1984). Some properties of the Tukey g and h family of distributions. Communications in Statistics: Theory and Methods, 13, 353–369."
  },
  {
    "objectID": "blog/posts/post7/index.html#tukeys-boxplot",
    "href": "blog/posts/post7/index.html#tukeys-boxplot",
    "title": "Beyond Standard Boxplot: The Adjusted and Generalized Boxplots",
    "section": "",
    "text": "Proposed by Tukey in 1977 [1], a boxplot is constructed using five key summary statistics: the minimum, first quartile (Q1), median, third quartile (Q3), and maximum. These elements are complemented by whiskers, which typically extend to 1.5 times the interquartile range (IQR = Q3 - Q1) beyond Q1 and Q3, respectively. Data points outside this range are flagged as potential outliers.\n\\[\n\\begin{aligned}\n    \\text{Lower Fence} &= Q_1 - 1.5 \\cdot \\text{IQR} \\\\\n    \\text{Upper Fence} &= Q_3 + 1.5 \\cdot \\text{IQR}\n\\end{aligned}\n\\]\nThe ends of the whiskers are called adjacent values. They are the smallest and largest values not flagged outliers. Moreover, since IQR has a breakdown point of 0.25, it takes more than 25% of the data to be contaminated by outliers for the masking effect to occur. For example, in the Figure below, we show how the quartiles and therefore IQR are significantly influenced by the outliers, highlighting the breakdown point. After replacing 25% of the data with extreme outliers, the IQR increased drastically, from 13 (clean data) to 136 (contaminated data). This demonstrates that with 25% contamination, the IQR becomes unreliable as a measure of spread.\n\n\nShow the code\nset.seed(42)\ndata_clean &lt;- rnorm(100, mean = 50, sd = 10)\nn_outliers &lt;- floor(length(data_clean) * 0.25)\ndata_contaminated &lt;- data_clean\ndata_contaminated[1:n_outliers] &lt;- runif(n_outliers, min = 500, max = 1000)\n\ndata &lt;- tibble(\n  Value = c(data_clean, data_contaminated),\n  Dataset = rep(\n    c(\"Clean Data\", \"Contaminated Data\\n(25% Outliers)\"), \n    each = length(data_clean)\n    )\n  )\n\nsummary_stats &lt;- data %&gt;%\n  group_by(Dataset) %&gt;%\n  summarise(\n    Q1 = quantile(Value, 0.25),\n    Q3 = quantile(Value, 0.75),\n    IQR = Q3 - Q1\n  )\n\nggplot(data, aes(x = Value, y = Dataset, fill = Dataset)) +\n  geom_boxplot(\n    outlier.shape = 21, \n    outlier.size = 2, \n    outlier.alpha = 0.85,\n    width = .3,\n    staplewidth = 0.5) +\n  geom_vline(\n    data = summary_stats, \n    aes(xintercept = Q1, color = Dataset), \n    linetype = \"dashed\") +\n  geom_vline(\n    data = summary_stats, \n    aes(xintercept = Q3, color = Dataset), \n    linetype = \"dashed\") +\n  scale_fill_manual(values = c(\"skyblue\", \"lightcoral\")) +\n  scale_color_manual(values = c(\"orange\", \"green\")) +\n  labs(x = \"\", y = \"\") +\n  theme_bw() +\n  theme(legend.position = \"none\") +\n  facet_wrap(~Dataset, nrow = 2, ncol = 1, scales = \"free\") +\n  theme(\n    strip.background = element_blank(),\n    strip.text.x = element_blank(),\n    panel.grid = element_blank()\n  )\n\n\n\n\n\n\n\n\n\nBoxplots are a widely used tool for visualizing data distributions, particularly when the data exhibit a symmetrical mesokurtic distribution. However, its effectiveness diminishes when dealing with skewed data. In such cases, the boxplot can misrepresent the dataset’s nuances—most notably by classifying an excessive number of larger values as outliers when the distribution is skewed. Recognizing this limitation, Hubert and Vandervieren [2] introduced a refined version of Tukey’s boxplot rule. Their approach incorporates a robust measure of skewness, allowing for a more accurate depiction of asymmetrical distributions while preserving the simplicity and interpretability of Tukey’s boxplot."
  },
  {
    "objectID": "blog/posts/post7/index.html#the-skew-adjusted-boxplot",
    "href": "blog/posts/post7/index.html#the-skew-adjusted-boxplot",
    "title": "An Overview of Skew-Adjusted and Generalized Boxplots",
    "section": "",
    "text": "To address the limitations of the standard boxplot in handling skewed data, Hubert and Vandervieren (2008) introduced the skew-adjusted boxplot. This method modifies the whisker length based on the medcouple, a robust skewness statistic. Specifically, the upper whisker extends further for right-skewed data, and the lower whisker extends further for left-skewed data. This adjustment ensures a more accurate representation of the data’s spread while retaining the robustness of the standard boxplot."
  },
  {
    "objectID": "blog/posts/post7/index.html#the-generalized-boxplot",
    "href": "blog/posts/post7/index.html#the-generalized-boxplot",
    "title": "Beyond Standard Boxplot: The Adjusted and Generalized Boxplots",
    "section": "",
    "text": "In 1977, Tukey introduced a family of distributions through two nonlinear transformations, giving rise to what is now known as Tukey \\(g\\)-\\(h\\) distributions [5,6]. These distributions are widely used in robust statistics and flexible modeling due to their ability to accommodate a range of skewness and kurtosis levels. Tukey \\(g\\)-\\(h\\) distribution is defined by applying two transformations, \\(g\\) (skewness) and \\(h\\) (kurtosis), to a standard normal random variable \\(Z \\sim \\mathcal{N}(0, 1)\\). The resulting transformation is expressed by:\n\\[\nT_{g,h}(Z) = \\frac{e^{gZ}-1}{g} \\cdot e^{h\\frac{Z^2}{2}} \\quad \\text{with} \\quad g \\neq 0, h \\in \\mathbb{R}\n\\]\nBy adjusting \\(g\\) and \\(h\\), the Tukey \\(g\\)-\\(h\\) distribution can model a wide variety of distribution, from symmetric to skewed, and from light-tailed to heavy-tailed distributions.\nThe constants \\(g\\) and \\(h\\), can be estimated from the empirical quantiles as follows:\n\\[\n\\hat{g} = \\frac{1}{z_p} \\text{ln}\\left(-\\frac{Q_p(\\{x_j\\})}{Q_{1-p}(\\{x_j\\})}\\right),\n\\quad\n\\hat{h} = \\frac{2}{z_p^2} \\text{ln}\\left(-\\hat{g}\\frac{Q_p(\\{x_j\\}) \\cdot Q_{1-p}(\\{x_j\\})}{Q_p(\\{x_j\\}) + Q_{1-p}(\\{x_j\\})}\\right)\n\\]where \\(z_p\\) is the quantile of order \\(p\\) of the standard normal distribution. \\(Q_p\\) and \\(Q_{1-p}\\) are the empirical quantiles of order \\(p\\) (\\(0.5&lt;p&lt;1\\)) and \\(1-p\\) of the univariate data \\(X = \\{x_j\\} = \\{x_1, \\cdots, x_n\\}\\).\nThe generalized boxplot as proposed by Bruffaerts et al. [4] begins by applying a rank-preserving transformation to the data. This transformation maps the original observations onto the unit interval (0, 1), maintaining the order of the data points while capturing key distributional features like skewness and tail behavior. Then, an inverse normal transformation is applied, similar to rank-based approaches. This transformed distribution can be fine-tuned using the Tukey \\(g\\)-\\(h\\) distribution, whose quantiles are used to set the boxplot whiskers.\nThe Figures below illustrate a comparison of Tukey’s boxplot, the adjusted boxplot, and the generalized boxplot across three distinct types of data distributions: (i) a normal distribution, (ii) a right-skewed distribution, and (iii) a heavy-tailed distribution. The Figures show how each method performs and adapts to the specific characteristics of these distributions, effectively balancing sensitivity to outliers without over-identifying them.\nFor a symmetric distribution, all three boxplots produce similar results as expected.\n\n\n\n\n\n\n\n\n\nFor a right-skewed distribution, Tukey’s boxplot overestimates the number of outliers, while the adjusted and generalized boxplots provide a better representation of the underlying data distribution. The generalized boxplot, in particular, offers the most flexible approach by accommodating skewness.\n\n\n\n\n\n\n\n\n\nFor a heavy-tailed distribution, Tukey’s boxplot inaccurately identifies an excessive number of outliers due to its assumption of symmetry and limited adaptability to extreme tails. The adjusted boxplot improved upon Tukey’s boxplot by accounting for skewness in the data, yet it still struggles to effectively handle the variability and extreme values inherent in heavy-tailed distributions. In contrast, the generalized boxplot proves to be a more robust and versatile method. Its whiskers and overall box structure more accurately reflect the underlying data distribution, demonstrating its ability to adapt to both skewness and tail heaviness more effectively than the other two methods."
  },
  {
    "objectID": "blog/posts/post7/index.html#the-adjusted-boxplot",
    "href": "blog/posts/post7/index.html#the-adjusted-boxplot",
    "title": "Beyond Standard Boxplot: The Adjusted and Generalized Boxplots",
    "section": "",
    "text": "In 2004, Brys et al. [3] introduced the medcouple (MC), which is a robust statistic used to measure the skewness of a distribution. Unlike traditional measures of skewness that are sensitive to outliers, MC focuses on the median and the IQR, making it less affected by extreme values. It is based on the difference between left and right data spreads relative to the median. MC is bounded between −1 and 1. A value of 0 indicates a perfectly symmetric distribution, while positive values signify a right-tailed (positively skewed) distribution, and negative values correspond to a left-tailed (negatively skewed) distribution. Importantly, this measure is most effective for moderately skewed distributions, particularly when the absolute value of MC is less than or equal to 0.6 (\\(|\\text{MC}| \\leq 0.6\\)). MC is given by:\n\\[\n\\text{MC} = \\text{med} \\left\\{ \\frac{(x_i - m) - (m - x_j)}{x_i - x_j} \\, \\middle| \\, x_i \\geq m \\geq x_j \\right\\}\n\\]\nwhere, \\(x_i\\) and \\(x_j\\) are data points from the sample, \\(m\\) is the median of the sample, and \\(\\text{med}\\{\\cdot\\}\\) denotes the median operator.\nTo address the limitation of Tukey’s boxplot in handling skewed data, Hubert and Vandervieren [2] introduced the adjusted boxplot for skewed distribution. This method modifies the whisker length based on the MC. Specifically, the upper whisker extends further for right-skewed data, and the lower whisker extends further for left-skewed data. This new approch modifies the traditional boxplot fences to account for skewness in the data using the exponential function of MC as follows:\nWhen \\(\\text{MC} \\geq 0\\):\n\\[\n\\begin{aligned}\n    \\text{Lower Fence} &= Q_1 - k \\cdot e^{-4\\text{MC}} \\cdot \\text{IQR} \\\\\n    \\text{Upper Fence} &= Q_3 + k \\cdot e^{3\\text{MC}} \\cdot \\text{IQR}\n\\end{aligned}\n\\]\nWhen \\(\\text{MC} \\leq 0\\):\n\\[\n\\begin{aligned}\n    \\text{Lower Fence} &= Q_1 - k \\cdot e^{-3\\text{MC}} \\cdot \\text{IQR} \\\\\n    \\text{Upper Fence} &= Q_3 + k \\cdot e^{4\\text{MC}} \\cdot \\text{IQR}\n\\end{aligned}\n\\]\nwhere, \\(k\\) is the fence factor. Hubert and Vandervieren [2] have optimized the adjusted boxplot for \\(k = 1.5\\). In the next Figure, we compare Tukey’s boxplot (left) with the adjusted boxplot (right), applied to a skewed data. The adjusted boxplot shows fewer outliers for the skewed data, reflecting its ability to handle skewness better.\n\n\nShow the code\nset.seed(123)\nskewed_data &lt;- data.frame(y = rgamma(500, shape = 2, scale = 2))\np1 &lt;- skewed_data |&gt; \n  ggplot() +\n  aes(x = \"\", y = y, fill = \"lightcoral\") +\n  geom_boxplot(\n    width = 0.3, \n    staplewidth = 0.5, \n    outlier.fill = \"lightcoral\", \n    outlier.shape = 21,\n    outlier.size = 3,\n    outlier.alpha = 1/3) +\n  ylim(0, 15) +\n  labs(\n    x = \"Tukey's Boxplot\", y = \"\") +\n  theme_bw() +\n  theme(\n    panel.grid = element_blank(),\n    legend.position = \"none\",\n    plot.margin = margin(r = 0)\n    )\n\np2 &lt;- skewed_data |&gt; \n  select(y) |&gt; \n  specProc::adjusted_boxplot(box.width = 0.3, staplewidth = 0.5) +\n  scale_fill_manual(values = \"lightblue\") +\n  ylim(0, 15) +\n  labs(x = \"Adjusted Boxplot\", y = \" \") +\n  theme_bw() +\n  theme(\n    panel.grid = element_blank(),\n    legend.position = \"none\",\n    axis.text.x = element_blank(),\n    axis.text.y = element_blank(),\n    plot.margin = margin(l = 0)\n    )\np1 | p2\n\n\n\n\n\n\n\n\n\nThe adjusted boxplot has garnered attention for its ability to handle skewed data, even earning a mention on platforms like Wikipedia. However, it is not without its critiques.\nFor example, Bruffaerts et al. [4] pointed out that the whiskers in the adjusted boxplot are designed to achieve a fixed outlier detection rate of 0.7%. While this threshold may be suitable for some applications, the process becomes cumbersome if one wishes to adopt a different detection rate. It requires re-running the simulations to determine a new tuning constant or fence factor (\\(k\\)). Moreover, for datasets with heavy-tailed distributions, the adjusted boxplot may still misclassify observations, as the whisker adjustment relies solely on skewness and not on tail behavior. Another significant issue lies in its dependence on MC. While MC is effective for large samples, its estimation can be imprecise for small sample sizes.\nThe Figure below illustrates the behavior of MC estimates for small and large sample sizes using density plots. To achieve this, we generated multiple random samples from a skewed Gamma distribution and examined the variability of the MC estimates across these sample sizes. For small sample sizes, the MC estimates exhibit significant variability, reflecting its reduced robustness in such cases. In contrast, as the sample size increases, the MC estimates stabilize and converge to a consistent value, demonstrating its improved precision and reliability with larger data.\n\n\nShow the code\nset.seed(123)\ncalculate_MC &lt;- function(sample_size, num_simulations = 1000) {\n  mc_values &lt;- numeric(num_simulations)\n  for (i in 1:num_simulations) {\n    data &lt;- rgamma(sample_size, shape = 2, scale = 2)  \n    mc_values[i] &lt;- mc(data)                          \n  }\n  return(mc_values)\n}\n\nsmall_sample_size &lt;- 30\nlarge_sample_size &lt;- 1000\nmc_small &lt;- calculate_MC(small_sample_size)\nmc_large &lt;- calculate_MC(large_sample_size)\n\nmc_data &lt;- data.frame(\n  MC = c(mc_small, mc_large),\n  Sample_Size = rep(c(\"Small (n = 30)\", \"Large (n = 1 000)\"), each = length(mc_small))\n)\n\nggplot(mc_data, aes(x = MC, fill = Sample_Size)) +\n  geom_density(alpha = 0.5) +\n  labs(\n    title = \"Medcouple (MC) Estimates for Small and Large Samples\",\n    subtitle = \"MC estimates are more variable for small samples\",\n    x = \"MC\",\n    y = \"Density\",\n    fill = \"Sample size\"\n  ) +\n  scale_fill_manual(values = c(\"purple\", \"green\")) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nIn response to these shortcomings, Bruffaerts et al. [4] proposed an alternative method to address skewness and heavy-tailed distributions. Their approach introduces a rank-preserving transformation that allows the data to conform to a Tukey \\(g\\)-\\(h\\) distribution."
  },
  {
    "objectID": "blog/posts/post6/index.html#voigt-profile",
    "href": "blog/posts/post6/index.html#voigt-profile",
    "title": "The Pseudo-Voigt Function",
    "section": "",
    "text": "Fundamentally, the pseudo-Voigt function is an approximation of the more precise Voigt profile (named after German physicist Woldemar Voigt). The Voigt profile (\\(V\\)) represents the exact convolution of Gaussian and Lorentzian profiles, providing a more accurate description of spectral line shapes, particularly when both Doppler and Lorentzian broadening are significant.\n\\[\nV(x;\\sigma,\\gamma) = \\int_{-\\infty}^{\\infty} G(x';\\sigma) L(x - x';\\gamma) \\, dx'\n\\]\nThe Voigt profile can also be expressed using the Faddeeva function \\(\\omega(z)\\), given by:\n\\[\nw(z) = e^{-z^2} \\left( 1 + \\frac{2i}{\\sqrt{\\pi}} \\int_0^z e^{t^2} \\, dt \\right)\n\\] where \\(z\\) is a complex number. Using the Faddeeva function, the Voigt profile is: \\[\nV(x; \\sigma, \\gamma) = \\frac{1}{\\sigma\\sqrt{2\\pi}}\\Re\\left[w\\left( \\frac{x + i\\gamma}{\\sigma\\sqrt{2}} \\right)\\right]\n\\]\nwhere \\(\\Re[w(z)]\\) denotes the real part of the Faddeeva function.\n\n\n\n\n\n\n\n\n\nHowever, the Voigt profile is computationally intensive, requiring efficient numerical methods to accurately evaluate the convolution integral. This complexity has led to the development of approximations, such as the pseudo-Voigt function, which strike a balance between computational efficiency and maintaining acceptable levels of accuracy in the spectral line shape."
  },
  {
    "objectID": "blog/posts/post6/index.html#pseudo-voigt-function",
    "href": "blog/posts/post6/index.html#pseudo-voigt-function",
    "title": "The Pseudo-Voigt Function",
    "section": "",
    "text": "The pseudo-Voigt function (\\(pV\\)) is a linear combination of a Gaussian (\\(G\\)) and a Lorentzian (\\(L\\)) function, and is defined as:\n\\[\npV(x;\\eta) = ηG(x;\\sigma) + (1-η)L(x;\\gamma)\n\\]\n\\[\nG(x;\\sigma) = \\frac{1}{\\sigma \\sqrt{2\\pi}} e^{-\\frac{x^2}{2\\sigma^2}}\n\\]\n\\[\nL(x; \\gamma) = \\frac{\\gamma}{\\pi (x^2 + \\gamma^2)}\n\\]\nwhere, \\(\\eta\\) is a mixing parameter that determines the relative contribution of the Lorentzian (\\(\\eta = 0\\)) and Gaussian (\\(\\eta = 1\\)) components. Here, the parameters \\(\\sigma\\) and \\(\\gamma\\) represent, respectively, the standard deviation of the Gaussian component, related to the full width at half maximum (FWHM) of the Gaussian by \\(w_G = 2\\sqrt{2\\ln 2} \\, \\sigma\\), and the half-width at half maximum (HWHM) of the Lorentzian component, related to the Lorentzian FWHM by \\(w_L = 2\\gamma\\).\n\n\n\n\n\n\n\n\n\nThe following animation illustrates how the pseudo-Voigt function varies with the mixing parameter \\(\\eta\\). As \\(\\eta\\) increases from 0 to 1, the contribution of the Gaussian component becomes more significant. Each case has a FWHM of \\(w_G = w_L = 1\\).\n\n\nShow the code\nlibrary(ggplot2)\nlibrary(gganimate)\n\ngaussian &lt;- function(x, x_c, w_G, A, y_0) {\n  y_0 + A / (w_G * sqrt(pi / (4 * log(2)))) * exp(-4 * log(2) * (x - x_c)^2 / w_G^2)\n}\nlorentzian &lt;- function(x, x_c, w_L, A, y_0) {\n  y_0 + A / pi * w_L / ((x - x_c)^2 + w_L^2)\n}\npseudo_voigt &lt;- function(x, x_c, w_L, w_G, A, y_0, eta) {\n  y_0 + A * (eta * gaussian(x, x_c, w_G, A, y_0)  + (1 - eta) * lorentzian(x, x_c, w_L, A, y_0))\n}\n\nx_c &lt;- y_0 &lt;- 0; w_L &lt;- w_G &lt;- A &lt;- 1\neta_values &lt;- seq(0, 1, by = 0.05)\ndf &lt;- data.frame(x = seq(-5, 5, by = 0.1))\nfor (eta in eta_values) {\n  df[[paste0(\"η = \", eta)]] &lt;- pseudo_voigt(df$x, x_c, w_L, w_G, A, y_0, eta)\n}\ndf_long &lt;- tidyr::pivot_longer(df, cols = -x, names_to = \"eta\", values_to = \"y\")\n\np &lt;- df_long |&gt; \n  ggplot(aes(x = x, y = y, colour = eta)) +\n  geom_line(linewidth = 1) +\n  scale_y_continuous(limits = c(0,1)) +\n  geom_point(size = 2) +\n  theme_bw(base_size = 15) +\n  labs(\n    x = \"x\", \n    y = \"y\", \n    title = \"The Pseudo-Voigt Function\",\n    subtitle = paste0(\"Varying η from Lorentzian (η = 0) to Gaussian (η = 1): \", \"{closest_state}\"),\n    caption = \"The mixing parameter η determines the relative contribution of the Lorentzian and Gaussian components.\") +\n  theme(\n    legend.position = \"none\",\n    plot.title = element_text(hjust = 0, size = 18, face = \"bold\"),\n    plot.subtitle = element_text(hjust = 0),\n    panel.background = element_rect(colour = \"black\", linewidth = 1)\n    )\n\nanim &lt;- p +\n  transition_states(eta, transition_length = 2, state_length = 1) +\n  ease_aes('exponential-in-out') +\n  shadow_mark(alpha = 2/10, size = 0.1)\n\nanimate(\n  anim, \n  nframes = 100, \n  fps = 10, \n  width = 600, \n  height = 400, \n  renderer = gifski_renderer()\n  )"
  },
  {
    "objectID": "blog/posts/post6/index.html#extended-pseudo-voigt-function",
    "href": "blog/posts/post6/index.html#extended-pseudo-voigt-function",
    "title": "The Pseudo-Voigt Function",
    "section": "",
    "text": "In the early 2000s, Ida et al. [2] refined the pseudo-Voigt function by introducing an extended formula designed to more accurately approximate the Voigt profile. This formula is given by:\n\\[\nepV(x;\\eta_L, \\eta_I, \\eta_P) = (1 - \\eta_L - \\eta_I - \\eta_P)G(x;\\sigma) + \\eta_L L(x; \\gamma) + \\eta_I F_I(x;\\gamma_I) + \\eta_P F_P(x;\\gamma_P)\n\\]\nwhere \\(F_I\\) and \\(F_P\\) are intermediate functions that represent the transition between the Lorentzian and Gaussian profiles, respectively. \\(F_I\\) is an irrational function involving a square root, while \\(F_P\\) is the squared hyperbolic secant function. These functions are defined as follows:\n\\[\nF_I(x;\\gamma_I) = \\frac{1}{2\\gamma_I}\\left( 1 + \\left(\\frac{x}{\\gamma_I}\\right)^2 \\right)^{-3/2}\n\\]\n\\[\nF_P(x;\\gamma_P) = \\frac{1}{2\\gamma_P}\\text{sech}^2\\left(\\frac{x}{\\gamma_P}\\right)\n\\]\nThe FWHMs are given by \\(w_I = 2 \\gamma_I \\sqrt{(2^{2/3} - 1)}\\), and \\(w_P = 2 \\gamma_P \\ln{(\\sqrt{2} + 1)}\\). A Gaussian profile is obtained when the mixing parameters are set to \\(\\eta_L = \\eta_I = \\eta_P = 0\\), while the Lorentzian contribution is governed by the parameter \\(\\eta_L\\). Specifically, a pure Lorentzian profile arises when \\(\\eta_L = 1\\). These parameters are constrained to the range from 0 to 1 and must satisfy the condition \\(\\eta_L + \\eta_I + \\eta_P = 1\\). This constraint defines a 2D simplex within a plane in 3D space, representing all possible combinations of the mixing parameters.\n\n\n\n\n\n\n\n\n\nThe animation below is a visual representation of the normalized extended pseudo-Voigt profiles, illustrating the transition from Gaussian to Lorentzian shapes as the mixing parameters \\(\\eta_L, \\eta_I\\), and \\(\\eta_P\\), are varied simultaneously."
  },
  {
    "objectID": "blog/posts/post7/index.html#references",
    "href": "blog/posts/post7/index.html#references",
    "title": "Beyond Standard Boxplot: The Adjusted and Generalized Boxplots",
    "section": "",
    "text": "Tukey, J.W., (1977). Exploratory Data Analysis, Pearson, 1st edition, page 39.\nHubert, M., Vandervieren, E., (2008). An adjusted boxplot for skewed distributions. Computational Statistics and Data Analysis, 52, 5186-5201.\nBrys, G., Hubert, M., Struyf, A., (2004). A robust measure of skewness. Journal of Computational and Graphical Statistics, 13, 996-1017.\nBruffaerts, C., Verardi, V., Vermandele, C., (2014). A generalized boxplot for skewed and heavy-tailed distributions. Statistics & Probability Letters, 95, 110–117.\nTukey, J.W., (1977). Modern techniques in data analysis. NSF‐sponsored regional research conference at Southeastern Massachusetts University, North Dartmouth, MA.\nMartinez, J., Iglewicz, B., (1984). Some properties of the Tukey g and h family of distributions. Communications in Statistics: Theory and Methods, 13, 353–369."
  },
  {
    "objectID": "blog/posts/post7/index.html#conclusions",
    "href": "blog/posts/post7/index.html#conclusions",
    "title": "Beyond Standard Boxplot: The Adjusted and Generalized Boxplots",
    "section": "",
    "text": "By comparing the three boxplot approaches, we show the limitations of Tukey’s boxplot when applied to skewed or heavy-tailed data. While Tukey’s method assumes symmetry and often flags excessive outliers, the adjusted boxplot improves this by incorporating a skewness measure (medcouple). The generalized boxplot provides the most robust solution by flexibly adapting to both skewness and tail heaviness through the Tukey \\(g\\)-\\(h\\) distribution. Overall, the adjusted and generalized boxplots are better suited for non-symmetric data, with the generalized boxplot offering the most robust approach."
  },
  {
    "objectID": "blog/posts/post8/index.html",
    "href": "blog/posts/post8/index.html",
    "title": "Robust Measures of Tail Weight in Distributions",
    "section": "",
    "text": "Photo by Oliver Plattner.\n\n\nIn the world of data analysis, understanding the tail behavior of a distribution is critical for identifying potential outliers. Traditional measures like kurtosis have long been used to capture the weight of the tails of a distribution. However, kurtosis comes with limitations, particularly its sensitivity to outliers and lack of robustness. In this blog post, we will explore two robust measures for tail weight:\n\nQuantile tail weight measure\nMedcouple tail weight measure\n\nLet’s begin with why tail weight matters and examine the limitations of traditional kurtosis. Light-tailed distributions (e.g., normal distribution) have very few extreme values, whereas heavy-tailed distributions (e.g., Cauchy, Student’s t-distribution) contain more extreme values, and outliers occur more frequently. If a measure of tail weight is unreliable or sensitive to extreme observations, the analysis of the underlying data distribution would be inaccurate.\n\n\nKurtosis (denoted \\(\\beta_2\\)), which is derived from the 4th moment of a distribution, measures the degree of peakedness and tail heaviness relative to a normal distribution. For a normal distribution, \\(\\beta_2 = 3\\), however excess kurtosis (\\(\\gamma_2\\)) is often used to center the normal reference value at 0.\n\\[\n\\gamma_2 = \\beta_2 - 3 = \\frac{\\mu_4}{\\mu^2_2} - 3 = \\frac{\\mathbb{E}[(X-\\mu)^4]}{\\sigma^4} - 3\n\\]\nwhere, \\(\\mu\\) is the mean of \\(X\\), and \\(\\sigma^2\\) is its variance.\nThe concept of kurtosis was first introduced by Karl Pearson in the early 1900s as a measure to characterize the shape of probability distributions, particularly their “peakedness” and tail behavior. Pearson later coined the terms platykurtic (\\(\\gamma_2 &lt; 0\\)), mesokurtic (\\(\\gamma_2 = 0\\)), and leptokurtic (\\(\\gamma_2 &gt; 0\\)) to classify distributions based on their kurtosis levels. Despite its usefulness, kurtosis has a notable limitation: it is highly sensitive to outliers. Even a single extreme value can disproportionately inflate the kurtosis, potentially leading to distorted interpretations and misleading conclusions about the data’s underlying distribution.\n\n\nShow the code\nset.seed(123)\nnormal_data &lt;- rnorm(1000)\noutlier_data &lt;- c(normal_data, rep(10, 5))\nkurtosis_normal &lt;- kurtosis(normal_data)\nkurtosis_outlier &lt;- kurtosis(outlier_data)\n\ntibble(\n  value = c(normal_data, outlier_data),\n  type = rep(c(\"normal\", \"with outliers\"), c(1000, 1005))\n  ) %&gt;%\n  ggplot(aes(x = value, fill = type)) +\n  geom_histogram(bins = 30, alpha = 0.6, position = \"identity\") +\n  facet_wrap(vars(type)) +\n  ggsci::scale_fill_jco() +\n  theme_bw() +\n  labs(\n    title = \"Impact of Outliers on Kurtosis\",\n    subtitle = paste(\"Kurtosis (Normal):\", round(kurtosis_normal, 2), \"| Kurtosis (Outliers):\", round(kurtosis_outlier, 2)), \n    x = \"\", \n    y = \"Count\",\n    fill = \"Distribution\",\n    ) +\n  theme(\n    strip.background = element_blank(),\n    strip.text.x = element_blank()\n  )\n\n\n\n\n\n\n\n\n\nNotice how adding a few extreme outliers significantly increases the kurtosis value. This demonstrates the lack of robustness of kurtosis. To overcome the sensitivity of kurtosis, Brys et al. (2006) introduced two robust measures: quantile weight (QW) and medcouple weight (MW). Both rely on robust statistical properties and are less affected by outliers.\n\n\nQW measure evaluates the weight of the tails using quantiles. It is based on the interquartile range (IQR) and extreme quantiles. The numerator focuses on extreme quantiles (tail behavior). The denominator normalizes using the IQR, making the measure robust to outliers. The quantile weights, comprising the left quantile weight (LQW) and the right quantile weight (RQW), and are given by:\n\\[\n\\text{LQW}(p) = -\\frac{Q_{\\frac{1-p}{2}} + Q_{\\frac{p}{2}} - 2Q_{0.25}}{Q_{\\frac{1-p}{2}} - Q_{\\frac{p}{2}}}, \\quad\n\\text{RQW}(q) = -\\frac{Q_{\\frac{1+q}{2}} + Q_{1-\\frac{q}{2}} - 2Q_{0.75}}{Q_{\\frac{1+q}{2}} - Q_{1-\\frac{q}{2}}}\n\\]\nwith \\(0&lt;p&lt;\\frac{1}{2}\\) and \\(\\frac{1}{2}&lt;q&lt;1\\), which define the degree of robustness one is willing to attain.\nThey have a breakdown value of 0.125, meaning that they are resistant to the influence of up to 12.5% of outliers or contaminated data. The concept of quantile weights is derived from quartile skewness, introduced by D.V. Hinkley in 1975. Quartile skewness measures the skewness or asymmetry of a distribution by comparing the differences between the quartiles, which are robust measures of location and scale. Specifically, the quantile weights are calculated when applying quartile skewness to either the left half or the right half of the probability mass, divided at the median of the univariate distribution. The left quantile weight (LQW) is the proportion of the data below the median, divided by the expected proportion (0.5) if the data were normally distributed. The right quantile weight (RQW) is the proportion of the data above the median, divided by 0.5. Values closer to 0 indicate lighter tails compared to the normal distribution, values closer to 1 signify heavier tails compared to the normal distribution, whereas values significantly greater than 1 suggest the presence of outliers or extreme values in the respective tail.\n\n\nShow the code\nquantile_weight &lt;- function(x, p = 0.25, q = 0.75, drop.na = FALSE) {\n  if (missing(x)) {\n    stop(\"Missing 'x' argument.\")\n  }\n  if (!is.numeric(x)) {\n    stop(\"The input 'x' must be a numeric vector.\")\n  }\n  if (!is.logical(drop.na)) {\n    stop(\"The input 'drop.na' must be a logical value (TRUE or FALSE).\")\n  }\n\n  value &lt;- NULL\n  below_med &lt;- NULL\n  above_med &lt;- NULL\n  med &lt;- stats::median(x, na.rm = drop.na)\n\n  left &lt;- tibble::enframe(x) %&gt;%\n    dplyr::mutate(below_med = value &lt;= med) %&gt;%\n    dplyr::filter(below_med) %&gt;%\n    dplyr::pull(value)\n\n  right &lt;- tibble::enframe(x) %&gt;%\n    dplyr::mutate(above_med = value &gt;= med) %&gt;%\n    dplyr::filter(above_med) %&gt;%\n    dplyr::pull(value)\n\n  ql1 &lt;- stats::quantile(left, (1 - p)/2)\n  ql2 &lt;- stats::quantile(left, p/2)\n\n  qr1 &lt;- stats::quantile(right, (1 + q)/2)\n  qr2 &lt;- stats::quantile(right, 1 - (q/2))\n\n  w_tbl &lt;- tibble::tibble(\n    LQW = (-1) * (ql1 + ql2 - 2 * stats::quantile(left, 0.25)) / (ql1 - ql2),\n    RQW = (qr1 + qr2 - 2 * stats::quantile(right, 0.75)) / (qr1 - qr2)\n  )\n\n  return(w_tbl)\n}\n\n\n\n\n\nMW measure uses the medcouple statistic (MC), which is a robust measure of skewness. It focuses on tail asymmetry and provides information about the heaviness of each tail. The medcouple is defined as a scaled median of all pairwise slopes between data points. It is robust to extreme observations and does not require a predefined model. The resulting MW measure evaluates the extent of tail heaviness in a distribution, particularly useful when the data is skewed.\nThe left and right medcouples are a robust measure of tail weight based on the median and the medcouple (Brys et al. 2004), which is a kernel estimator of the cumulative distribution function (CDF). The left and right medcouples are robust to outliers and have a breakdown value of 25%. Specifically, the left medcouple (LMC) measures the skewness in the lower tail of the distribution, while the right medcouple (RMC) measures the skewness in the upper tail. LMC and RMC, are defined as:\n\\[\n\\text{LMC} = -\\text{MC}(x &lt; m), \\quad \\text{RMC} = \\text{MC}(x &gt; m)\n\\]\n\\[\n\\text{with,} \\quad\n\\text{MC} = \\text{med} \\left\\{ \\frac{(x_i - m) - (m - x_j)}{x_i - x_j} \\, \\middle| \\, x_i \\geq m \\geq x_j \\right\\}\n\\]\nwith \\(m\\) the median of \\(X\\). LMC and MC close to 0 indicate a symmetric distribution or light tails. Positive values indicate right-skewness or a heavier right tail, whereas negative values indicate left-skewness or a heavier left tail.\n\n\nShow the code\nmedcouple_weight &lt;- function(x, drop.na = FALSE) {\n  if (missing(x)) {\n    stop(\"Missing 'x' argument.\")\n  }\n  if (!is.numeric(x)) {\n    stop(\"The input 'x' must be a numeric vector.\")\n  }\n  if (!is.logical(drop.na)) {\n    stop(\"The input 'drop.na' must be a logical value (TRUE or FALSE).\")\n  }\n\n  value &lt;- NULL\n  below_med &lt;- NULL\n  above_med &lt;- NULL\n  med &lt;- stats::median(x, na.rm = drop.na)\n\n  left &lt;- tibble::enframe(x) %&gt;%\n    dplyr::mutate(below_med = value &lt;= med) %&gt;%\n    dplyr::filter(below_med) %&gt;%\n    dplyr::pull(value)\n\n  right &lt;- tibble::enframe(x) %&gt;%\n    dplyr::mutate(above_med = value &gt;= med) %&gt;%\n    dplyr::filter(above_med) %&gt;%\n    dplyr::pull(value)\n\n  w_tbl &lt;- tibble::tibble(\n    LMC = (-1) * robustbase::mc(left, na.rm = drop.na),\n    RMC = robustbase::mc(right, na.rm = drop.na)\n  )\n\n  return(w_tbl)\n}\n\n\n\n\n\nThe plots below highlight the results, demonstrating the effectiveness of the QW and MW methods in handling heavy-tailed distributions.\n\n\nShow the code\nset.seed(123)\nheavy_tail_data &lt;- rt(1000, df = 3) \nlight_tail_data &lt;- rnorm(1000)       \nkurt_heavy &lt;- kurtosis(heavy_tail_data)\nkurt_light &lt;- kurtosis(light_tail_data)\n\nqtw_heavy &lt;- quantile_weight(heavy_tail_data)\nqtw_light &lt;- quantile_weight(light_tail_data)\n\nmtw_heavy &lt;- medcouple_weight(heavy_tail_data)\nmtw_light &lt;- medcouple_weight(light_tail_data)\n\ndata_frame(\n  value = c(heavy_tail_data, light_tail_data),\n  type = rep(c(\"Heavy tails\", \"Light tails\"), each = 1000)\n  ) %&gt;%\n  ggplot(aes(x = value, fill = type)) +\n  geom_histogram(bins = 30, alpha = 0.6, position = \"identity\") +\n  facet_wrap(vars(type)) +\n  ggsci::scale_fill_aaas() +\n  theme_bw() +\n  labs(\n    title = \"Impact of heavy-tailed on Kurtosis\",\n    subtitle = paste(\"Kurtosis (Light tail):\", round(kurt_light, 2), \"| Kurtosis (Heavy tail):\", round(kurt_heavy, 2)),\n    x = \"\", y = \"Count\", fill = \"Distribution\"\n    ) +\n  theme(\n    strip.background = element_blank(),\n    strip.text.x = element_blank()\n  )\n\n\n\n\n\n\n\n\n\nIn particular, these methods prove to be robust and efficient when analyzing data characterized by extreme values or pronounced tail behavior.\n\n\nShow the code\ndata &lt;- tibble(\n    method = c(\"γ2\", \"LQW\", \"RQW\", \"LMC\", \"RMC\"),\n    light_tails = c(0.01, 0.10, 0.21, 0.22, 0.22),\n    heavy_tails = c(7.03, 0.34, 0.17, 0.30, 0.34)\n    ) %&gt;%\n    pivot_longer(\n      cols = c(light_tails, heavy_tails), \n      names_to = \"tail_type\", \n      values_to = \"value\"\n      ) %&gt;%\n    mutate(\n      tail_type = case_when(\n        tail_type == \"light_tails\" ~ \"Light tails\",\n        tail_type == \"heavy_tails\" ~ \"Heavy tails\"\n        ),\n      method = factor(method, levels = c(\"γ2\", \"LQW\", \"RQW\", \"LMC\", \"RMC\")),\n      method_group = case_when(\n        method == \"γ2\" ~ \"Classical\",\n        method %in% c(\"LQW\", \"RQW\") ~ \"QW method\",\n        method %in% c(\"LMC\", \"RMC\") ~ \"MC method\"\n        ),\n      method_group = factor(\n        method_group, \n        levels = c(\"Classical\", \"QW method\", \"MC method\")\n        )\n      )\n\nggplot(data, aes(x = value, y = method, color = tail_type)) +\n    geom_point(size = 3, alpha = 0.8) +\n    geom_line(aes(group = method), color = \"gray60\", linewidth = 0.8) +\n    facet_grid(method_group ~ ., scales = \"free_y\", space = \"free_y\") +\n    scale_x_log10(\n      breaks = c(0.01, 0.1, 1, 10),\n      labels = c(\"0.01\", \"0.1\", \"1\", \"10\")\n      ) +\n    scale_color_manual(\n      values = c(\"Light tails\" = \"#2E86AB\", \"Heavy tails\" = \"#A23B72\"),\n      name = \"Tail Type\"\n      ) +\n    labs(\n      title = \"Comparison of Methods for Light vs Heavy-Tailed Distributions\",\n      subtitle = \"Log scale highlights performance differences across methods\",\n      x = \"\",\n      y = \"Statistic\"\n      ) +\n    theme_bw(base_size = 12) +\n    theme(\n      panel.grid = element_blank(),\n      legend.position = \"bottom\",\n      plot.title = element_text(size = 14, face = \"bold\"),\n      plot.subtitle = element_text(size = 12, color = \"gray60\"),\n      axis.title = element_text(size = 12),\n      axis.text = element_text(size = 10),\n      strip.text = element_text(size = 11, face = \"bold\")\n      ) +\n    annotation_logticks(sides = \"b\", size = 0.3)\n\n\n\n\n\n\n\n\n\n\n\n\nMethods\nLight tails\nHeavy tails\n\n\n\n\n\\(\\gamma_2\\)\n0.01\n7.03\n\n\nLQW/RQW\n0.10/ 0.21\n0.34/ 0.17\n\n\nLMC/RMC\n0.22/ 0.22\n0.30/ 0.34\n\n\n\n\n\n\n\nKurtosis, while popular, is not always the best measure of tail weight due to its sensitivity to outliers. Robust alternatives like the quantile tail weight and medcouple tail weight measures provide more reliable insights, particularly in the presence of heavy-tailed or skewed data. Moreover, the LMC and RMC measure do not require the choice of any additional parameter, whereas for the LQW and RQW measures required tuning \\(p\\) and \\(q\\) parameters.\n\n\n\n\nBrys, G., Hubert, M., and Struyf, A. (2006). Robust measures of tail weight. Computational Statistics & Data Analysis, 50, 733-759.\nHinkley, D.V., (1975). On power transformations to symmetry. Biometrika, 62, 101–111."
  },
  {
    "objectID": "blog/posts/post8/index.html#kurtosis",
    "href": "blog/posts/post8/index.html#kurtosis",
    "title": "Robust Measures of Tail Weight in Distributions",
    "section": "",
    "text": "Kurtosis measures the degree of peakedness and tail heaviness relative to a Normal distribution. For a normal distribution, kurtosis = 3. Excess kurtosis (kurtosis - 3) is often used to center the normal reference value at 0. However, kurtosis has a significant weakness: it is highly sensitive to outliers. Even a single extreme observation can disproportionately inflate kurtosis, leading to misleading conclusions.\n\n\nShow the code\nlibrary(tidyverse)\nlibrary(moments)\nset.seed(123)\nnormal_data &lt;- rnorm(1000)\noutlier_data &lt;- c(normal_data, rep(10, 5))\nkurtosis_normal &lt;- kurtosis(normal_data)\nkurtosis_outlier &lt;- kurtosis(outlier_data)\n\ndata_frame(\n  Value = c(normal_data, outlier_data),\n  Type = rep(c(\"Normal Data\", \"With Outliers\"), c(1000, 1005))\n  ) %&gt;%\n  ggplot(aes(x = Value, fill = Type)) +\n  geom_histogram(bins = 30, alpha = 0.6, position = \"identity\") +\n  theme_bw() +\n  labs(\n    title = \"Impact of Outliers on Kurtosis\",\n    subtitle = paste(\n      \"Kurtosis (Normal):\", round(kurtosis_normal, 2), \n      \"| Kurtosis (Outliers):\", round(kurtosis_outlier, 2)\n      ), \n    x = \"\", \n    y = \"Count\"\n    )\n\n\n\n\n\n\n\n\n\nNotice how adding a few extreme outliers significantly increases the kurtosis value. This demonstrates the lack of robustness of kurtosis. To overcome the sensitivity of kurtosis, Brys et al. (2006) introduced two robust measures: Quantile Tail Weight (QTW) and Medcouple Tail Weight (MTW). Both rely on robust statistical properties and are less affected by outliers.\n\n\nThe QTW measure evaluates the weight of the tails using quantiles. It is based on the interquartile range (IQR) and extreme quantiles. The numerator focuses on extreme quantiles (tail behavior). The denominator normalizes using the IQR, making the measure robust to outliers.\n\n\n\nThe MTW measure uses the Medcouple statistic, which is a robust measure of skewness. It focuses on tail asymmetry and provides information about the heaviness of each tail. The Medcouple is defined as a scaled median of all pairwise slopes between data points. It is robust to extreme observations and does not require a predefined model. The resulting MTW measure evaluates the extent of tail heaviness in a distribution, particularly useful when the data is skewed.\n\n\nShow the code\nset.seed(123)\nheavy_tail_data &lt;- rt(1000, df = 3) \nlight_tail_data &lt;- rnorm(1000)       \nkurt_heavy &lt;- kurtosis(heavy_tail_data)\nkurt_light &lt;- kurtosis(light_tail_data)\n\nqtw &lt;- function(x) {\n  (quantile(x, 0.975) - quantile(x, 0.025)) / (quantile(x, 0.75) - quantile(x, 0.25))\n}\nqtw_heavy &lt;- qtw(heavy_tail_data)\nqtw_light &lt;- qtw(light_tail_data)\n\ndata_frame(\n  Value = c(heavy_tail_data, light_tail_data),\n  Type = rep(c(\"Heavy Tails\", \"Light Tails\"), each = 1000)\n  ) %&gt;%\n  ggplot(aes(x = Value, fill = Type)) +\n  geom_histogram(bins = 30, alpha = 0.6, position = \"identity\") +\n  theme_bw() +\n  labs(\n    title = \"Comparing Heavy-Tailed and Light-Tailed Distributions\",\n    subtitle = paste(\n      \"Kurtosis (Heavy):\", round(kurt_heavy, 2),\n      \"| Kurtosis (Light):\", round(kurt_light, 2),\n      \"| QTW (Heavy):\", round(qtw_heavy, 2),\n      \"| QTW (Light):\", round(qtw_light, 2)),\n    x = \"\", \n    y = \"Count\"\n    )"
  },
  {
    "objectID": "blog/posts/post8/index.html#conclusion",
    "href": "blog/posts/post8/index.html#conclusion",
    "title": "Robust Measures of Tail Weight in Distributions",
    "section": "",
    "text": "Kurtosis, while popular, is not always the best measure of tail weight due to its sensitivity to outliers. Robust alternatives like the quantile tail weight and medcouple tail weight measures provide more reliable insights, particularly in the presence of heavy-tailed or skewed data. Moreover, the LMC and RMC measure do not require the choice of any additional parameter, whereas for the LQW and RQW measures required tuning \\(p\\) and \\(q\\) parameters."
  },
  {
    "objectID": "blog/posts/post8/index.html#kurtosis-measure",
    "href": "blog/posts/post8/index.html#kurtosis-measure",
    "title": "Robust Measures of Tail Weight in Distributions",
    "section": "",
    "text": "Kurtosis (denoted \\(\\beta_2\\)), which is derived from the 4th moment of a distribution, measures the degree of peakedness and tail heaviness relative to a normal distribution. For a normal distribution, \\(\\beta_2 = 3\\), however excess kurtosis (\\(\\gamma_2\\)) is often used to center the normal reference value at 0.\n\\[\n\\gamma_2 = \\beta_2 - 3 = \\frac{\\mu_4}{\\mu^2_2} - 3 = \\frac{\\mathbb{E}[(X-\\mu)^4]}{\\sigma^4} - 3\n\\]\nwhere, \\(\\mu\\) is the mean of \\(X\\), and \\(\\sigma^2\\) is its variance.\nThe concept of kurtosis was first introduced by Karl Pearson in the early 1900s as a measure to characterize the shape of probability distributions, particularly their “peakedness” and tail behavior. Pearson later coined the terms platykurtic (\\(\\gamma_2 &lt; 0\\)), mesokurtic (\\(\\gamma_2 = 0\\)), and leptokurtic (\\(\\gamma_2 &gt; 0\\)) to classify distributions based on their kurtosis levels. Despite its usefulness, kurtosis has a notable limitation: it is highly sensitive to outliers. Even a single extreme value can disproportionately inflate the kurtosis, potentially leading to distorted interpretations and misleading conclusions about the data’s underlying distribution.\n\n\nShow the code\nset.seed(123)\nnormal_data &lt;- rnorm(1000)\noutlier_data &lt;- c(normal_data, rep(10, 5))\nkurtosis_normal &lt;- kurtosis(normal_data)\nkurtosis_outlier &lt;- kurtosis(outlier_data)\n\ntibble(\n  value = c(normal_data, outlier_data),\n  type = rep(c(\"normal\", \"with outliers\"), c(1000, 1005))\n  ) %&gt;%\n  ggplot(aes(x = value, fill = type)) +\n  geom_histogram(bins = 30, alpha = 0.6, position = \"identity\") +\n  facet_wrap(vars(type)) +\n  ggsci::scale_fill_jco() +\n  theme_bw() +\n  labs(\n    title = \"Impact of Outliers on Kurtosis\",\n    subtitle = paste(\"Kurtosis (Normal):\", round(kurtosis_normal, 2), \"| Kurtosis (Outliers):\", round(kurtosis_outlier, 2)), \n    x = \"\", \n    y = \"Count\",\n    fill = \"Distribution\",\n    ) +\n  theme(\n    strip.background = element_blank(),\n    strip.text.x = element_blank()\n  )\n\n\n\n\n\n\n\n\n\nNotice how adding a few extreme outliers significantly increases the kurtosis value. This demonstrates the lack of robustness of kurtosis. To overcome the sensitivity of kurtosis, Brys et al. (2006) introduced two robust measures: quantile weight (QW) and medcouple weight (MW). Both rely on robust statistical properties and are less affected by outliers.\n\n\nQW measure evaluates the weight of the tails using quantiles. It is based on the interquartile range (IQR) and extreme quantiles. The numerator focuses on extreme quantiles (tail behavior). The denominator normalizes using the IQR, making the measure robust to outliers. The quantile weights, comprising the left quantile weight (LQW) and the right quantile weight (RQW), and are given by:\n\\[\n\\text{LQW}(p) = -\\frac{Q_{\\frac{1-p}{2}} + Q_{\\frac{p}{2}} - 2Q_{0.25}}{Q_{\\frac{1-p}{2}} - Q_{\\frac{p}{2}}}, \\quad\n\\text{RQW}(q) = -\\frac{Q_{\\frac{1+q}{2}} + Q_{1-\\frac{q}{2}} - 2Q_{0.75}}{Q_{\\frac{1+q}{2}} - Q_{1-\\frac{q}{2}}}\n\\]\nwith \\(0&lt;p&lt;\\frac{1}{2}\\) and \\(\\frac{1}{2}&lt;q&lt;1\\), which define the degree of robustness one is willing to attain.\nThey have a breakdown value of 0.125, meaning that they are resistant to the influence of up to 12.5% of outliers or contaminated data. The concept of quantile weights is derived from quartile skewness, introduced by D.V. Hinkley in 1975. Quartile skewness measures the skewness or asymmetry of a distribution by comparing the differences between the quartiles, which are robust measures of location and scale. Specifically, the quantile weights are calculated when applying quartile skewness to either the left half or the right half of the probability mass, divided at the median of the univariate distribution. The left quantile weight (LQW) is the proportion of the data below the median, divided by the expected proportion (0.5) if the data were normally distributed. The right quantile weight (RQW) is the proportion of the data above the median, divided by 0.5. Values closer to 0 indicate lighter tails compared to the normal distribution, values closer to 1 signify heavier tails compared to the normal distribution, whereas values significantly greater than 1 suggest the presence of outliers or extreme values in the respective tail.\n\n\nShow the code\nquantile_weight &lt;- function(x, p = 0.25, q = 0.75, drop.na = FALSE) {\n  if (missing(x)) {\n    stop(\"Missing 'x' argument.\")\n  }\n  if (!is.numeric(x)) {\n    stop(\"The input 'x' must be a numeric vector.\")\n  }\n  if (!is.logical(drop.na)) {\n    stop(\"The input 'drop.na' must be a logical value (TRUE or FALSE).\")\n  }\n\n  value &lt;- NULL\n  below_med &lt;- NULL\n  above_med &lt;- NULL\n  med &lt;- stats::median(x, na.rm = drop.na)\n\n  left &lt;- tibble::enframe(x) %&gt;%\n    dplyr::mutate(below_med = value &lt;= med) %&gt;%\n    dplyr::filter(below_med) %&gt;%\n    dplyr::pull(value)\n\n  right &lt;- tibble::enframe(x) %&gt;%\n    dplyr::mutate(above_med = value &gt;= med) %&gt;%\n    dplyr::filter(above_med) %&gt;%\n    dplyr::pull(value)\n\n  ql1 &lt;- stats::quantile(left, (1 - p)/2)\n  ql2 &lt;- stats::quantile(left, p/2)\n\n  qr1 &lt;- stats::quantile(right, (1 + q)/2)\n  qr2 &lt;- stats::quantile(right, 1 - (q/2))\n\n  w_tbl &lt;- tibble::tibble(\n    LQW = (-1) * (ql1 + ql2 - 2 * stats::quantile(left, 0.25)) / (ql1 - ql2),\n    RQW = (qr1 + qr2 - 2 * stats::quantile(right, 0.75)) / (qr1 - qr2)\n  )\n\n  return(w_tbl)\n}\n\n\n\n\n\nMW measure uses the medcouple statistic (MC), which is a robust measure of skewness. It focuses on tail asymmetry and provides information about the heaviness of each tail. The medcouple is defined as a scaled median of all pairwise slopes between data points. It is robust to extreme observations and does not require a predefined model. The resulting MW measure evaluates the extent of tail heaviness in a distribution, particularly useful when the data is skewed.\nThe left and right medcouples are a robust measure of tail weight based on the median and the medcouple (Brys et al. 2004), which is a kernel estimator of the cumulative distribution function (CDF). The left and right medcouples are robust to outliers and have a breakdown value of 25%. Specifically, the left medcouple (LMC) measures the skewness in the lower tail of the distribution, while the right medcouple (RMC) measures the skewness in the upper tail. LMC and RMC, are defined as:\n\\[\n\\text{LMC} = -\\text{MC}(x &lt; m), \\quad \\text{RMC} = \\text{MC}(x &gt; m)\n\\]\n\\[\n\\text{with,} \\quad\n\\text{MC} = \\text{med} \\left\\{ \\frac{(x_i - m) - (m - x_j)}{x_i - x_j} \\, \\middle| \\, x_i \\geq m \\geq x_j \\right\\}\n\\]\nwith \\(m\\) the median of \\(X\\). LMC and MC close to 0 indicate a symmetric distribution or light tails. Positive values indicate right-skewness or a heavier right tail, whereas negative values indicate left-skewness or a heavier left tail.\n\n\nShow the code\nmedcouple_weight &lt;- function(x, drop.na = FALSE) {\n  if (missing(x)) {\n    stop(\"Missing 'x' argument.\")\n  }\n  if (!is.numeric(x)) {\n    stop(\"The input 'x' must be a numeric vector.\")\n  }\n  if (!is.logical(drop.na)) {\n    stop(\"The input 'drop.na' must be a logical value (TRUE or FALSE).\")\n  }\n\n  value &lt;- NULL\n  below_med &lt;- NULL\n  above_med &lt;- NULL\n  med &lt;- stats::median(x, na.rm = drop.na)\n\n  left &lt;- tibble::enframe(x) %&gt;%\n    dplyr::mutate(below_med = value &lt;= med) %&gt;%\n    dplyr::filter(below_med) %&gt;%\n    dplyr::pull(value)\n\n  right &lt;- tibble::enframe(x) %&gt;%\n    dplyr::mutate(above_med = value &gt;= med) %&gt;%\n    dplyr::filter(above_med) %&gt;%\n    dplyr::pull(value)\n\n  w_tbl &lt;- tibble::tibble(\n    LMC = (-1) * robustbase::mc(left, na.rm = drop.na),\n    RMC = robustbase::mc(right, na.rm = drop.na)\n  )\n\n  return(w_tbl)\n}\n\n\n\n\n\nThe plots below highlight the results, demonstrating the effectiveness of the QW and MW methods in handling heavy-tailed distributions.\n\n\nShow the code\nset.seed(123)\nheavy_tail_data &lt;- rt(1000, df = 3) \nlight_tail_data &lt;- rnorm(1000)       \nkurt_heavy &lt;- kurtosis(heavy_tail_data)\nkurt_light &lt;- kurtosis(light_tail_data)\n\nqtw_heavy &lt;- quantile_weight(heavy_tail_data)\nqtw_light &lt;- quantile_weight(light_tail_data)\n\nmtw_heavy &lt;- medcouple_weight(heavy_tail_data)\nmtw_light &lt;- medcouple_weight(light_tail_data)\n\ndata_frame(\n  value = c(heavy_tail_data, light_tail_data),\n  type = rep(c(\"Heavy tails\", \"Light tails\"), each = 1000)\n  ) %&gt;%\n  ggplot(aes(x = value, fill = type)) +\n  geom_histogram(bins = 30, alpha = 0.6, position = \"identity\") +\n  facet_wrap(vars(type)) +\n  ggsci::scale_fill_aaas() +\n  theme_bw() +\n  labs(\n    title = \"Impact of heavy-tailed on Kurtosis\",\n    subtitle = paste(\"Kurtosis (Light tail):\", round(kurt_light, 2), \"| Kurtosis (Heavy tail):\", round(kurt_heavy, 2)),\n    x = \"\", y = \"Count\", fill = \"Distribution\"\n    ) +\n  theme(\n    strip.background = element_blank(),\n    strip.text.x = element_blank()\n  )\n\n\n\n\n\n\n\n\n\nIn particular, these methods prove to be robust and efficient when analyzing data characterized by extreme values or pronounced tail behavior.\n\n\nShow the code\ndata &lt;- tibble(\n    method = c(\"γ2\", \"LQW\", \"RQW\", \"LMC\", \"RMC\"),\n    light_tails = c(0.01, 0.10, 0.21, 0.22, 0.22),\n    heavy_tails = c(7.03, 0.34, 0.17, 0.30, 0.34)\n    ) %&gt;%\n    pivot_longer(\n      cols = c(light_tails, heavy_tails), \n      names_to = \"tail_type\", \n      values_to = \"value\"\n      ) %&gt;%\n    mutate(\n      tail_type = case_when(\n        tail_type == \"light_tails\" ~ \"Light tails\",\n        tail_type == \"heavy_tails\" ~ \"Heavy tails\"\n        ),\n      method = factor(method, levels = c(\"γ2\", \"LQW\", \"RQW\", \"LMC\", \"RMC\")),\n      method_group = case_when(\n        method == \"γ2\" ~ \"Classical\",\n        method %in% c(\"LQW\", \"RQW\") ~ \"QW method\",\n        method %in% c(\"LMC\", \"RMC\") ~ \"MC method\"\n        ),\n      method_group = factor(\n        method_group, \n        levels = c(\"Classical\", \"QW method\", \"MC method\")\n        )\n      )\n\nggplot(data, aes(x = value, y = method, color = tail_type)) +\n    geom_point(size = 3, alpha = 0.8) +\n    geom_line(aes(group = method), color = \"gray60\", linewidth = 0.8) +\n    facet_grid(method_group ~ ., scales = \"free_y\", space = \"free_y\") +\n    scale_x_log10(\n      breaks = c(0.01, 0.1, 1, 10),\n      labels = c(\"0.01\", \"0.1\", \"1\", \"10\")\n      ) +\n    scale_color_manual(\n      values = c(\"Light tails\" = \"#2E86AB\", \"Heavy tails\" = \"#A23B72\"),\n      name = \"Tail Type\"\n      ) +\n    labs(\n      title = \"Comparison of Methods for Light vs Heavy-Tailed Distributions\",\n      subtitle = \"Log scale highlights performance differences across methods\",\n      x = \"\",\n      y = \"Statistic\"\n      ) +\n    theme_bw(base_size = 12) +\n    theme(\n      panel.grid = element_blank(),\n      legend.position = \"bottom\",\n      plot.title = element_text(size = 14, face = \"bold\"),\n      plot.subtitle = element_text(size = 12, color = \"gray60\"),\n      axis.title = element_text(size = 12),\n      axis.text = element_text(size = 10),\n      strip.text = element_text(size = 11, face = \"bold\")\n      ) +\n    annotation_logticks(sides = \"b\", size = 0.3)\n\n\n\n\n\n\n\n\n\n\n\n\nMethods\nLight tails\nHeavy tails\n\n\n\n\n\\(\\gamma_2\\)\n0.01\n7.03\n\n\nLQW/RQW\n0.10/ 0.21\n0.34/ 0.17\n\n\nLMC/RMC\n0.22/ 0.22\n0.30/ 0.34"
  },
  {
    "objectID": "blog/posts/post8/index.html#reference",
    "href": "blog/posts/post8/index.html#reference",
    "title": "Robust Measures of Tail Weight in Distributions",
    "section": "",
    "text": "Brys, G., Hubert, M., and Struyf, A. (2006). Robust measures of tail weight. Computational Statistics & Data Analysis, 50, 733-759.\nHinkley, D.V., (1975). On power transformations to symmetry. Biometrika, 62, 101–111."
  },
  {
    "objectID": "projects/index.html#r-packages",
    "href": "projects/index.html#r-packages",
    "title": "R Projects",
    "section": "",
    "text": "category\n\n\npackages\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncategory\n\n\npackages\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncategory\n\n\npackages\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "projects/index.html#r-applications",
    "href": "projects/index.html#r-applications",
    "title": "R Projects",
    "section": "R Applications",
    "text": "R Applications\n\n\n\n\n\n\n\n\n\ncategory\n\n\napps\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blog/posts/post10/index.html",
    "href": "blog/posts/post10/index.html",
    "title": "Tensor Decomposition: PARAFAC and Tucker",
    "section": "",
    "text": "Photo by Corina Ardeleanu.\n\n\n\n\nIn many areas of science, like spectroscopy, data are usually stored as vectors or matrices. But in real life, data often have more than two dimensions. For example, measurements that vary across time, space, and different conditions all at once. To work with these kinds of multi-dimensional datasets, we use multi-linear algebra. It’s the higher-dimensional version of linear algebra. Just as a matrix can be broken down with tools like Singular Value Decomposition (SVD) or Eigenvalue Decomposition (EVD) these larger data structures can also be decomposed with special methods. These decompositions help us find patterns that would be hidden if we only looked at two-dimensional data.\nFor example, Excitation-Emission Matrix (EEM) fluorescence spectroscopy and hyperspectral imaging naturally produce three-dimensional data.\n\n\n\n\n\nIn situation where the collected data are not intrinsically 3-ways but rather tidy (\\(i\\) observations × \\(j\\) variables), we can add for instance “time” as a third mode, in order to have three inputs (\\(i\\) observations × \\(j\\) variables × \\(k\\) time). In other words, by adding an additional modality, we obtain a collection of matrices structured in a table with three entries called third-order tensor. Likewise, if we add “days” as the fourth mode into the previous data (\\(i\\) observations × \\(j\\) variables × \\(k\\) time × \\(l\\) days), we obtain a fourth-order tensor. If “longitude” is added (\\(i\\) observations × \\(j\\) variables × \\(k\\) time × \\(l\\) days × \\(m\\) longitude), we obtain a fifth-order tensor, and so forth. Intuitively we can see that an order-\\(N\\) tensor is the number of modes or ways , or \\(N\\)-dimensional arrays. In the following we will only consider data arranged as third-order tensor  \\(i.e.\\), 3-way data.\n\n\n\n\n\n\n\nLet \\(T\\) be an array with three entries, three matrix representations can be associated with this array:\n\\(T\\) can be seen as 3 different collections of matrices. These collections are called horizontal, vertical and frontal slices. The matrices of each collection have the same dimension.\n\n\n\n\n\n\\(T\\) can be considered successively as three matrices called “representations n mode”. The process of reorganizing \\(T\\) in these three forms is called matricization (or mode-\\(k\\) flattening).\n\n\n\n\n\n\\(T\\) can be seen as three collections of vectors. Each collection is a table with 2 entries, the elements of which are vectors of the same dimension also called “fibers”.\n\n\n\n\n\n\n\n\n\n\n\nPARAFAC (Parallel Factor Analysis) also called Canonical Polyadic (CP) is the simplest and most widely used three-way decomposition. Introduced independently by Harshman and, by Carrol and Chang (who called it Canonical Decomposition or CANDECOMP), PARAFAC represents the most intuitive extension of familiar techniques like Principal Component Analysis (PCA) into the three-dimensional world. PARAFAC is express by:\n\\[\n\\mathbf{X} \\approx \\sum_{r=1}^{R} \\lambda_r \\cdot (\\mathbf{a}_r \\otimes \\mathbf{b}_r \\otimes \\mathbf{c}_r)\n\\]\nThis equation tells us that a 3D tensor \\(\\textbf{X}\\) can be approximated as a sum of \\(R\\) simple components, where each component is the outer product \\((\\otimes)\\) of three vectors: \\(\\textbf{a}_r\\) from mode 1, \\(\\textbf{b}_r\\) from mode 2, and \\(\\textbf{c}_r\\) from mode 3, scaled by weight \\(\\lambda_r\\).\nOne of the defining strengths of PARAFAC is that it applies the same number of components (R) across all modes and, under mild conditions, guarantees a unique decomposition. This property of uniqueness is extremely valuable: unlike many other factorization methods, PARAFAC can recover the “true” underlying factors without ambiguity. As a result, it provides not just a mathematical approximation but an interpretable solution that reflects real-world structure. This makes PARAFAC a powerful tool for scientific discovery, with broad applications in spectroscopy, neuroscience, signal processing, and beyond.\n\n\n\nTucker3 takes a more sophisticated approach, like having an adjustable toolbox instead of a master key. While PARAFAC forces all modes to have the same number of components, Tucker3 allows each mode to have its own optimal number of factors. The Tucker3 decomposition follows this structure:\n\\[\n\\mathbf{X} \\approx \\mathbf{G} \\times_1 \\mathbf{A} \\times_2 \\mathbf{B} \\times_3 \\mathbf{C}\n\\]\nHere, \\(\\textbf{G}\\) is the core tensor that acts as a “weighting grid,” determining how strongly each combination of factors from different modes interacts. The \\(×₁\\), \\(×₂\\), \\(×₃\\) symbols represent tensor contractions. Think of them as sophisticated ways of combining the core tensor with factor matrices \\(\\textbf{A}\\), \\(\\textbf{B}\\), and \\(\\textbf{C}\\).\nUnlike PARAFAC’s simple sum, Tucker3 creates a weighted combination where the core tensor \\(\\textbf{G}\\) contains elements \\(g_{i,j,k}\\) that specify how factors from mode 1 (position \\(i\\)), mode 2 (position \\(j\\)), and mode 3 (position \\(k\\)) combine together. This flexibility allows Tucker3 to capture more complex interaction patterns that PARAFAC might miss.\n\n\n\n\nIn the examples below, we demonstrate a practical implementation of tensor decomposition methods using the Python library TensorLy. In the first example, we built and used a synthetic dataset composed of three Gaussian distribution patterns, illustrating how PARAFAC decomposition can effectively capture and uncovored underlying structures in a noisy multidimensional data. In the second example, we use a published dataset that captures how our tongues move when we speak. Using this dataset, we will compare PARAFAC and Tucker decomposition, highlighting the differences in how each method captures the underlying structures of a complex multidimensional data.\n\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import pearsonr\nfrom mpl_toolkits.mplot3d import Axes3D\nfrom itertools import starmap\nimport math\nimport warnings\nwarnings.filterwarnings('ignore')\n\nFor our tensor decomposition analysis, we rely on TensorLy, a powerful Python library specifically designed for multilinear algebra and tensor operations. TensorLy provides a unified, user-friendly interface for tensor computations while maintaining computational efficiency through optimized backends.\n\nimport tensorly as tl\nfrom tensorly.decomposition import parafac, tucker\nfrom tensorly.cp_tensor import cp_to_tensor\nfrom tensorly.tucker_tensor import tucker_to_tensor\n\nWe also specify the computational backend with tl.set_backend('numpy'), which determines how TensorLy performs its underlying mathematical operations. TensorLy supports multiple backends including NumPy, PyTorch, TensorFlow, and JAX, each offering different advantages depending on the application. For more details see here.\n\nnp.random.seed(123)\ntl.set_backend('numpy')\n\n\n\n\nThe bivariate Gaussian distribution with means \\(\\mu_x,\\mu_y\\), standards deviations \\(\\sigma_x, \\sigma_y\\), and correlation coefficient \\(\\rho\\), is given by:\n\\[\nf(x, y) = \\frac{1}{2\\pi \\sigma_x \\sigma_y \\sqrt{1 - \\rho^2}} \\exp\\left( -\\frac{1}{2(1 - \\rho^2)} \\left[ \\left( \\frac{x - \\mu_x}{\\sigma_x} \\right)^2 - 2\\rho \\left( \\frac{x - \\mu_x}{\\sigma_x} \\right) \\left( \\frac{y - \\mu_y}{\\sigma_y} \\right) + \\left( \\frac{y - \\mu_y}{\\sigma_y} \\right)^2 \\right] \\right)\n\\]\nWe consider the simplified 2D Gaussian distribution when \\(\\rho = 0\\) (i.e., no correlation between \\(x\\) and \\(y\\)):\n\\[\nf(x, y) = \\frac{1}{2\\pi \\sigma_x \\sigma_y} \\exp\\left( - \\frac{1}{2} \\left[ \\left( \\frac{x - \\mu_x}{\\sigma_x} \\right)^2 + \\left( \\frac{y - \\mu_y}{\\sigma_y} \\right)^2 \\right] \\right)\n\\]\n\ndef gaussian_distribution(x, y, mu=(0,0), sigma=(1,1), amplitude=1):\n  alpha = 1 / (2 * math.pi * mu[0] * mu[1])\n  beta_x = ((x - mu[0]) / sigma[0])**2\n  beta_y = ((y - mu[1]) / sigma[1])**2\n  beta = beta_x + beta_y\n  return amplitude * alpha * np.exp(-(1 / 2) * beta)\n\nNow, let’s built three different Gaussian distributions with specific characteristics.\n\nx = np.linspace(-5, 5, 100)\ny = np.linspace(-5, 5, 100)\nX, Y = np.meshgrid(x, y)\n\n# Broad distribution\nbroad = gaussian_distribution(X, Y, (-2, -2), (2, 2), 1)\n\n# Two peaks\npeak1 = gaussian_distribution(X, -Y, (1.5, 1.5), (0.8, 0.8), 1)\npeak2 = gaussian_distribution(-X, Y, (2, 2), (0.8, 0.8), 2)\ntwopeaks = peak1 + peak2 \n\n# Sharp distribution\nsharp = gaussian_distribution(X, Y, (1, 1), (0.2, 0.2), 1.2)\n\nNext, we stack the three distributions into a tensor of dimension \\(100 \\times 100 \\times 3\\).\n\nT = np.stack([broad, twopeaks, sharp], axis=-1)\n\nWe can now visualize the three distributions to verify their shapes and characteristics.\n\nfig = plt.figure(figsize=(15, 5))\n\nfor i in range(3):\n    ax = fig.add_subplot(1, 3, i+1, projection='3d')\n    surf = ax.plot_surface(X, Y, T[:, :, i], cmap='viridis', linewidth=0, antialiased=True)\n    ax.set_xlabel('X-Position')\n    ax.set_ylabel('Y-Position')\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\nplt.close()\n\nFor the purpose of this post, we add some noise into the data at three distinct levels: 0.001, 0.01, and 0.05.\n\nnoisy1_T = T + 0.001 * np.random.randn(*T.shape)\nnoisy2_T = T + 0.01 * np.random.randn(*T.shape)\nnoisy3_T = T + 0.05 * np.random.randn(*T.shape)\n\nThen, we create a list of tensors to decompose, e.g., [noisy1_T, noisy2_T, noisy3_T].\n\ndatasets = [noisy1_T, noisy2_T, noisy3_T]\nnoise_levels = [0.001, 0.01, 0.05]\ntitles = ['Broad', 'Two-peak', 'Sharp']\n\n\ndef plot_tensor(data, noise):\n    fig = plt.figure(figsize=(15, 5))\n    fig.suptitle(f\"Noise level = {noise}\", fontsize=16)\n    \n    for i in range(3):\n        ax = fig.add_subplot(1, 3, i+1, projection='3d')\n        surf = ax.plot_surface(X, Y, data[:, :, i], cmap='viridis', linewidth=0, antialiased=True)\n        ax.set_xlabel('X-Position')\n        ax.set_ylabel('Y-Position')\n\n    plt.tight_layout()\n    plt.show()\n    plt.close()\n\n\nfor _ in map(plot_tensor, datasets, noise_levels):\n    pass\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPARAFAC decomposition is applied iteratively to the list of tensors. For each tensor in datasets, the method extracts three components, producing a set of weights and factor matrices that capture the underlying patterns along each mode:\n\nMode 0 factors: describe the pattern along the rows (X-axis) of the tensor.\nMode 1 factors: describe the pattern along the columns (Y-axis) of the tensor.\nMode 2 factors: correspond to the the different Gaussian distributions.\n\nAll of these results are collected and stored in a list called results, for further analysis and visualization.\n\nresults = list(map(lambda data: parafac(data, 3, init='random', random_state=42), datasets))\n\nWe then reconstruct the tensor from the factorization results and evaluate the quality of the decomposition by calculating the relative reconstruction error. The reconstructed tensor is obtained using the factor matrices and weights returned by the PARAFAC model. The error is computed as the ratio between the Frobenius norm, \\(\\lVert{\\cdot}\\rVert_F\\), of the difference (original tensor minus reconstructed tensor) and the Frobenius norm of the original tensor. This provides a normalized measure of how well the decomposition approximates the data, with lower values indicating a more accurate reconstruction.\n\\[\n\\text{Relative Reconstruction Error} = \\frac{\\lVert\\mathcal{T} - \\hat{\\mathcal{T}}\\rVert_F}{\\lVert \\mathcal{T}\\rVert_F}\n\\]\nWhere \\(\\mathcal{T}\\) is the original tensor and \\(\\lVert \\mathcal{\\hat{T}} \\rVert_F\\) is the reconstructed tensor.\n\ndef compute_error(result, data):\n  weights, factors = result\n  reconstructed_T = cp_to_tensor((weights, factors))\n  return np.linalg.norm(data - reconstructed_T) / np.linalg.norm(data)\n\n\nerrors = list(map(compute_error, results, datasets))\n\n\nfor noise, err in zip(noise_levels, errors):\n    print(f\"Noise level = {noise}, Relative reconstruction error = {err:.4f}\")\n\nNoise level = 0.001, Relative reconstruction error = 0.3160\nNoise level = 0.01, Relative reconstruction error = 0.6662\nNoise level = 0.05, Relative reconstruction error = 0.9623\n\n\nNext, we visualize the decomposed modes obtained from the tensor factorization. Each of the three component represents a distinct pattern captured along a specific mode of the data, and plotting them allows us to interpret the underlying structures identified by the decomposition.\n\ntitles = ['Component 1', 'Component 2', 'Component 3']\n\n\ndef plot_components(result, noise):\n    weights, factors = result\n    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n    fig.suptitle(f\"Noise level = {noise}\", fontsize=16)\n\n    for i, ax in enumerate(axes):\n        component = np.outer(factors[0][:, i], factors[1][:, i])\n        component /= np.max(np.abs(component))\n        line_y = component.sum(axis=0)\n        line_x = component.sum(axis=1)\n        ax.plot(line_y, label='Mode 1 (Y-position)')\n        ax.plot(line_x, label='Mode 0 (X-position)')\n        ax.set_title(titles[i])\n        ax.set_xlabel('Index')\n        ax.set_ylabel('Amplitude (normalized)')\n        ax.legend()\n\n    plt.tight_layout()\n    plt.show()\n    plt.close()\n\n\nfor _ in map(plot_components, results, noise_levels):\n    pass\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntitles = ['Broad', 'Two-peak', 'Sharp']\n\n\ndef plot_reconstruction_error(noisy, reconstructed, noise_level):\n    error_tensor = noisy - reconstructed\n    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n    fig.suptitle(f'Reconstruction Error - Noise level = {noise_level}', fontsize=16)\n    for i, ax in enumerate(axes):\n        im = ax.imshow(error_tensor[:, :, i], cmap='RdBu', aspect='auto')\n        ax.set_title(titles[i])\n        fig.colorbar(im, ax=ax, label='Error')\n\n    plt.tight_layout()\n    plt.show()\n    plt.close()\n\n\nreconstructed_tensors = [cp_to_tensor(res) for res in results]\nfor _ in map(plot_reconstruction_error, datasets, reconstructed_tensors, noise_levels):\n    pass\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIn this example, we use a fascinating dataset from pioneering linguistic research by Ladefoged et al. (1971), that captures how our tongues move when we speak. The study used X-ray imaging to study tongue shapes as five different speakers pronounced various English vowels. By mapping tongue positions onto a defined grid, they created a unique 3D dataset that reveals the biomechanics of human speech.\nThe tensor has dimensions \\(5 \\times 10 \\times 13\\), representing:\n\n5 speakers (different individuals)\n10 vowels (various English vowel sounds)\n13 grid positions (spatial measurements in centimeters)\n\nThis creates a rich, multi-dimensional dataset perfect for demonstrating tensor decomposition techniques. The original study aimed to identify the fundamental patterns underlying tongue movement during speech. The data was preprocessed by centering across vowels, and the researchers found evidence for two definitive components in the speech patterns, with a possible third component that couldn’t be reliably established. This real-world complexity makes it an excellent example for comparing different tensor decomposition methods like PARAFAC and Tucker, as it contains the kind of structural challenges often found in genuine scientific data. For complete experimental details, see the original research paper.\n\n\nCode\nX = np.array([\n    [2.45, 2.40, 2.40, 2.50, 2.45, 2.05, 1.65, 1.00, 0.45, 0.40, 0.55, 0.55, 0.95],\n    [2.55, 2.05, 1.95, 1.90, 1.80, 1.60, 1.30, 0.95, 0.55, 0.65, 0.90, 0.90, 1.05],\n    [2.35, 1.90, 1.80, 1.80, 1.80, 1.55, 1.30, 0.95, 0.65, 0.70, 0.90, 0.85, 1.05],\n    [2.50, 2.05, 1.65, 1.65, 1.55, 1.45, 1.25, 1.05, 0.70, 1.05, 1.20, 1.15, 1.10],\n    [2.05, 1.50, 1.35, 1.50, 1.55, 1.45, 1.30, 1.15, 0.95, 1.40, 1.55, 1.40, 1.25],\n    [1.55, 1.00, 0.85, 1.00, 1.25, 1.35, 1.50, 2.00, 2.10, 2.55, 2.65, 2.35, 2.00],\n    [1.65, 0.90, 0.65, 0.65, 0.75, 0.95, 1.40, 1.90, 2.15, 2.60, 2.70, 2.60, 2.40],\n    [1.90, 1.30, 0.95, 0.85, 0.75, 0.75, 0.95, 1.30, 1.65, 2.15, 2.30, 2.25, 2.30],\n    [2.40, 1.60, 1.45, 1.25, 1.00, 0.95, 0.80, 0.85, 1.10, 1.50, 2.10, 2.00, 1.65],\n    [2.70, 1.95, 1.50, 1.30, 0.90, 0.70, 0.55, 0.55, 0.95, 1.45, 1.80, 1.90, 2.00],\n    [2.95, 2.70, 2.75, 2.75, 2.70, 2.60, 2.25, 1.00, 0.35, 0.15, 0.30, 0.60, 1.15],\n    [2.40, 2.20, 2.25, 2.20, 2.25, 2.15, 1.85, 1.25, 0.75, 0.75, 0.90, 1.05, 1.10],\n    [2.25, 2.45, 2.65, 2.65, 2.40, 2.20, 2.05, 1.55, 0.95, 0.85, 1.10, 1.40, 1.65],\n    [2.00, 1.75, 1.90, 2.30, 2.40, 2.20, 2.00, 1.45, 1.00, 1.05, 1.40, 1.75, 1.80],\n    [1.25, 1.15, 1.30, 1.65, 1.95, 1.90, 1.80, 1.65, 1.40, 1.70, 2.15, 2.45, 2.60],\n    [0.45, 0.25, 0.30, 0.40, 1.15, 1.70, 1.95, 2.30, 2.60, 2.95, 3.30, 3.15, 2.60],\n    [0.40, 0.20, 0.20, 0.30, 0.60, 1.05, 1.35, 1.65, 2.60, 3.05, 3.45, 3.60, 3.40],\n    [1.00, 0.55, 0.55, 0.45, 0.65, 0.80, 1.15, 1.55, 2.25, 2.75, 3.20, 3.35, 3.25],\n    [1.30, 0.70, 0.65, 0.45, 0.65, 0.90, 1.20, 1.45, 1.90, 2.40, 2.85, 2.80, 2.45],\n    [2.15, 1.80, 1.50, 1.05, 0.65, 0.55, 0.65, 0.80, 0.95, 1.55, 2.10, 2.35, 2.60],\n    [2.10, 2.00, 2.15, 2.05, 1.95, 1.80, 1.45, 1.10, 0.75, 0.65, 0.75, 0.80, 0.90],\n    [2.00, 1.70, 1.90, 1.95, 1.90, 1.75, 1.35, 1.15, 0.95, 1.00, 1.10, 0.90, 0.65],\n    [1.95, 1.80, 1.80, 1.95, 1.95, 1.95, 1.65, 1.25, 0.90, 0.85, 1.05, 0.95, 0.90],\n    [1.55, 1.40, 1.50, 1.70, 1.85, 1.80, 1.90, 1.80, 1.75, 1.70, 1.70, 1.40, 1.10],\n    [1.65, 1.25, 1.40, 1.70, 1.90, 1.95, 2.05, 2.10, 1.95, 1.95, 2.15, 2.10, 1.70],\n    [0.95, 0.55, 0.70, 1.15, 1.65, 2.20, 2.65, 2.95, 3.05, 3.20, 3.35, 2.95, 1.90],\n    [1.20, 0.65, 0.45, 0.65, 0.75, 1.00, 1.45, 2.10, 2.40, 2.65, 2.80, 2.55, 1.95],\n    [1.55, 1.45, 1.05, 1.15, 1.05, 1.00, 1.15, 1.45, 1.90, 2.40, 2.70, 2.65, 1.85],\n    [1.80, 1.05, 1.05, 1.05, 1.00, 1.00, 1.15, 1.40, 1.65, 1.95, 2.15, 1.85, 1.50],\n    [2.00, 1.70, 1.40, 1.20, 1.00, 0.85, 0.95, 1.00, 1.10, 1.55, 1.80, 1.70, 1.25],\n    [2.70, 2.60, 2.55, 2.50, 2.45, 2.40, 1.80, 1.35, 0.70, 0.55, 0.75, 0.85, 1.85],\n    [2.25, 1.90, 1.85, 1.90, 2.15, 2.05, 1.85, 1.65, 1.35, 1.40, 1.50, 1.90, 1.80],\n    [2.25, 2.20, 2.30, 2.25, 2.30, 2.20, 1.70, 1.45, 0.90, 0.90, 1.10, 1.25, 1.85],\n    [1.90, 1.50, 1.40, 1.40, 1.65, 1.75, 1.75, 1.85, 1.60, 1.80, 1.90, 1.65, 1.50],\n    [1.70, 1.20, 1.05, 1.05, 1.55, 1.70, 1.80, 1.90, 1.85, 2.10, 2.35, 2.40, 2.25],\n    [1.05, 0.90, 0.45, 0.60, 1.45, 2.05, 2.90, 2.90, 3.00, 3.20, 3.35, 2.95, 2.15],\n    [0.90, 0.40, 0.45, 0.55, 1.30, 1.80, 2.30, 2.80, 3.10, 3.40, 3.45, 3.00, 2.40],\n    [2.00, 1.30, 1.05, 0.90, 0.95, 0.90, 1.25, 1.65, 1.80, 2.30, 2.60, 2.60, 1.90],\n    [2.15, 1.70, 1.45, 1.30, 1.30, 1.25, 1.20, 1.35, 1.45, 1.95, 2.20, 2.25, 1.95],\n    [2.95, 2.30, 2.05, 1.80, 1.70, 1.45, 1.00, 0.80, 0.80, 1.15, 1.55, 1.90, 1.40],\n    [3.00, 2.45, 2.30, 2.20, 2.10, 1.45, 1.15, 0.80, 0.40, 0.60, 0.45, 0.40, 0.85],\n    [2.40, 2.10, 1.95, 1.90, 1.80, 1.45, 1.10, 0.90, 0.70, 0.95, 0.95, 0.75, 1.10],\n    [2.50, 2.40, 2.20, 2.05, 2.05, 1.70, 1.30, 0.95, 0.65, 0.95, 1.00, 0.85, 1.20],\n    [2.25, 2.10, 1.95, 1.90, 1.90, 1.55, 1.15, 1.00, 0.90, 1.10, 1.05, 0.90, 1.25],\n    [1.70, 1.95, 2.05, 2.10, 1.95, 1.50, 1.15, 1.15, 1.10, 1.30, 1.30, 1.20, 1.45],\n    [1.40, 0.85, 1.05, 1.30, 1.55, 1.55, 1.65, 2.00, 2.40, 2.75, 2.80, 2.60, 2.35],\n    [1.10, 0.70, 0.70, 0.90, 1.15, 1.00, 1.20, 1.80, 2.40, 2.75, 2.80, 2.35, 2.05],\n    [1.80, 1.05, 0.75, 0.70, 0.70, 0.55, 0.60, 1.20, 1.85, 2.40, 2.45, 2.25, 2.40],\n    [1.90, 1.25, 1.05, 0.90, 0.95, 0.65, 0.65, 1.25, 1.85, 2.35, 2.35, 2.05, 2.30],\n    [2.70, 2.05, 1.65, 1.40, 1.15, 0.60, 0.40, 0.50, 0.60, 1.15, 1.40, 1.60, 1.65]\n])\n\n\nLet’s walk through the essential steps to convert our raw data into a proper 3D tensor for analysis. First, we transpose our original data matrix. This rearranges our data so that the spatial measurements (13 grid positions) become the first dimension, which will be important for our tensor structure.\n\nX = X.T\n\nNext, we’re specifying how to organize our data into a 3D structure.\n\n13 = Grid positions (spatial measurements along the tongue)\n10 = Vowel sounds (different English vowels)\n5 = Speakers (different individuals)\n\nFinally, the .reshape() function reorganizes the 637 total elements \\((13 \\times 49)\\) into our desired 3D structure while preserving the original data values. We now have a clean 3D tensor where X_tensor[i, j, k] represents the tongue measurement at grid position i, for vowel j, spoken by speaker k.\n\ntensor_dims = [13, 10, 5]\nX_tensor = X.reshape(tensor_dims)\n\n\n\nIn the following, we first proceed with one important step. We systematically test different ranks (complexity levels) to find the best PARAFAC decomposition for our tongue shape data. We test multiple ranks from 2 to 50, asking: “How many components do we need to best represent our data?”\nIn doing so, we follow a systematic decomposition loop for each rank. We decompose the tensor into factor matrices using PARAFAC, then reconstruct the original tensor from these factors. We measure the error between the original and reconstructed data, and store all results for comparison. PARAFAC breaks down our 3D tensor into three factor matrices: a grid positions factor (13 × rank), a vowels factor (10 × rank), and a speakers factor (5 × rank). Think of this as finding the fundamental “building blocks” that, when combined, recreate the original tongue movement patterns.\n\ndef parafac_modeling(X_tensor, max_rank=50):\n    \"\"\"\n    PARAFAC modeling\n    \n    Parameters:\n    -----------\n    X_tensor : numpy.ndarray\n        Input tensor to decompose\n    max_rank : intenger, default=50\n        Maximum rank\n    \n    Returns:\n    --------\n    dict : Results dictionary with PARAFAC decomposition results\n    \"\"\"\n    ranks_to_test = list(range(2, max_rank + 1))\n    results = {}\n    \n    for rank in ranks_to_test:\n        try:\n            factors = parafac(\n              X_tensor, \n              rank=rank, \n              init='random', \n              n_iter_max=200, \n              random_state=42\n              )\n              \n            reconstructed = tl.cp_to_tensor(factors)\n            error = tl.norm(X_tensor - reconstructed)\n            rel_error = error / tl.norm(X_tensor)\n            \n            results[rank] = {\n                'factors': factors,\n                'reconstructed': reconstructed,\n                'error': error,\n                'rel_error': rel_error\n            }\n            \n        except Exception as e:\n            print(f\" Models failed...\")\n            results[rank] = None\n            \n    return results\n\n\n\nCode\ndef optimal_rank(model, criteria='elbow'):\n    \"\"\"\n    Analyze and suggest optimal rank based on different criteria\n    \"\"\"\n    ranks = []\n    rel_errors = []\n    \n    for rank, result in model.items():\n        if result is not None:\n            ranks.append(rank)\n            rel_errors.append(result['rel_error'])\n    \n    ranks = np.array(ranks)\n    rel_errors = np.array(rel_errors)\n    \n    best_rank = ranks[np.argmin(rel_errors)]\n    best_error = np.min(rel_errors)\n    print(f\" Best overall rank: {best_rank} (error: {best_error*100:.3f}%)\")\n    \n    if len(ranks) &gt; 3:\n        first_diff = np.diff(rel_errors)\n        second_diff = np.diff(first_diff)\n        if len(second_diff) &gt; 0:\n            second_diff_ranks = ranks[2:len(second_diff)+2]\n            small_second_diff_mask = np.abs(second_diff) &lt; 0.001\n            if np.any(small_second_diff_mask):\n                elbow_rank = second_diff_ranks[small_second_diff_mask][0]\n                elbow_error = rel_errors[ranks == elbow_rank][0]\n                print(f\" Elbow point: Rank {elbow_rank} (error: {elbow_error*100:.3f}%)\")\n            else:\n                max_reduction_idx = np.argmax(np.abs(first_diff))\n                if max_reduction_idx &lt; len(ranks) - 1:\n                    elbow_rank = ranks[max_reduction_idx + 1]\n                    elbow_error = rel_errors[max_reduction_idx + 1]\n                    print(f\" Largest improvement at: Rank {elbow_rank} (error: {elbow_error*100:.3f}%)\")\n    \n    print(\"\\n Recommendations:\")\n    \n    good_ranks = ranks[rel_errors &lt; 0.05]\n    if len(good_ranks) &gt; 0:\n        practical_rank = good_ranks[0]  # First rank with &lt; 5% error\n        practical_error = rel_errors[ranks == practical_rank][0]\n        print(f\"   • For &lt; 5% error: Rank {practical_rank} (error: {practical_error*100:.3f}%)\")\n    \n    excellent_ranks = ranks[rel_errors &lt; 0.01]\n    if len(excellent_ranks) &gt; 0:\n        excellent_rank = excellent_ranks[0]\n        excellent_error = rel_errors[ranks == excellent_rank][0]\n        print(f\"   • For &lt; 1% error: Rank {excellent_rank} (error: {excellent_error*100:.3f}%)\")\n    \n    tensor_size = np.prod([13, 10, 5])\n    for rank in [5, 10, 15, 20]:\n        if rank in ranks:\n            params = rank * (13 + 10 + 5)\n            compression_ratio = tensor_size / params\n            error = rel_errors[ranks == rank][0]\n            print(f\"   • Rank {rank}: {params} params, {compression_ratio:.1f}x compression, {error*100:.2f}% error\")\n    \n    return ranks, rel_errors\n\n\n\n\nCode\ndef rank_analysis(model, save_fig=False):\n    \"\"\"\n    Rank analysis showing reconstruction error vs rank\n    \"\"\"\n    ranks = []\n    rel_errors = []\n    abs_errors = []\n    \n    for rank, result in model.items():\n        if result is not None:\n            ranks.append(rank)\n            rel_errors.append(result['rel_error'])\n            abs_errors.append(result['error'])\n    \n    ranks = np.array(ranks)\n    rel_errors = np.array(rel_errors)\n    abs_errors = np.array(abs_errors)\n    \n    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n    \n    axes[0, 0].plot(ranks, rel_errors * 100, 'bo-', linewidth=2, markersize=4)\n    axes[0, 0].set_xlabel('Rank')\n    axes[0, 0].set_ylabel('Relative Error (%)')\n    axes[0, 0].set_title('Reconstruction Error vs Rank')\n    axes[0, 0].grid(True, alpha=0.3)\n    axes[0, 0].set_ylim(bottom=0)\n    \n    min_error_idx = np.argmin(rel_errors)\n    axes[0, 0].annotate(\n      f'Min: Rank {ranks[min_error_idx]}\\nError: {rel_errors[min_error_idx]*100:.2f}%',\n      xy=(ranks[min_error_idx], rel_errors[min_error_idx]*100),\n      xytext=(ranks[min_error_idx]+5, rel_errors[min_error_idx]*100+2),\n      arrowprops=dict(arrowstyle='-&gt;', color='red'),\n      fontsize=9, color='red'\n      )\n    \n    axes[0, 1].semilogy(ranks, rel_errors * 100, 'ro-', linewidth=2, markersize=4)\n    axes[0, 1].set_xlabel('Rank')\n    axes[0, 1].set_ylabel('Relative Error (%) [Log Scale]')\n    axes[0, 1].set_title('Error vs Rank (Log Scale)')\n    axes[0, 1].grid(True, alpha=0.3)\n    \n    error_reduction = np.diff(rel_errors) / rel_errors[:-1] * -100\n    axes[1, 0].bar(ranks[1:], error_reduction, alpha=0.7, color='green')\n    axes[1, 0].set_xlabel('Rank')\n    axes[1, 0].set_ylabel('Error Reduction Rate (%)')\n    axes[1, 0].set_title('Error Improvement Rate (Rank i vs Rank i-1)')\n    axes[1, 0].grid(True, alpha=0.3)\n    axes[1, 0].axhline(y=0, color='black', linestyle='-', alpha=0.5)\n    \n    explained_variance = (1 - rel_errors**2) * 100\n    axes[1, 1].plot(ranks, explained_variance, 'mo-', linewidth=2, markersize=4)\n    axes[1, 1].set_xlabel('Rank')\n    axes[1, 1].set_ylabel('Explained Variance (%)')\n    axes[1, 1].set_title('Explained Variance vs Rank')\n    axes[1, 1].grid(True, alpha=0.3)\n    axes[1, 1].set_ylim([0, 100])\n    \n    for threshold in [90, 95, 99]:\n        axes[1, 1].axhline(y=threshold, color='gray', linestyle='--', alpha=0.5)\n        axes[1, 1].text(ranks[-10], threshold+1, f'{threshold}%', fontsize=8, alpha=0.7)\n    plt.tight_layout()\n    \n    if save_fig:\n        plt.savefig('rank_analysis.png', dpi=300, bbox_inches='tight')\n    \n    plt.show()\n    \n    return fig, axes\n\n\nWe obtain four-panel visualization that gives a clear picture of how well different ranks capture tongue shape patterns. The blue curve shows the classic elbow effect: reconstruction error drops quickly between ranks 2–10, then levels off, and finally approaches zero around ranks 40–45. In other words, using more than ~40 ranks lets us reconstruct the data almost perfectly. The log-scale view zooms in on the details, showing that most of the improvement happens early (ranks 2–15), with only small, gradual gains after that. The green bars highlight the “value added” by each rank. Big improvements show up at low ranks (2–10), with a few extra bumps around ranks 20–30. Occasionally, the bars dip below zero, which just means the algorithm stalled briefly due to some numerical hiccups.\nFrom a practical standpoint, choosing a rank between 7–20 is a sweet spot. It captures the meaningful tongue movement patterns without overfitting. Going beyond 40 may look like perfect reconstruction, but in reality, it’s more likely fitting noise than real speech dynamics.\n\nparafac_mod = parafac_modeling(X_tensor, max_rank=50)\nfig, axes = rank_analysis(model=parafac_mod)\n\n\n\n\n\n\n\n\n\nranks, rel_errors = optimal_rank(model=parafac_mod)\n\n Best overall rank: 50 (error: 0.000%)\n Elbow point: Rank 7 (error: 7.084%)\n\n Recommendations:\n   • For &lt; 5% error: Rank 11 (error: 4.580%)\n   • For &lt; 1% error: Rank 24 (error: 0.973%)\n   • Rank 5: 140 params, 4.6x compression, 8.91% error\n   • Rank 10: 280 params, 2.3x compression, 5.27% error\n   • Rank 15: 420 params, 1.5x compression, 2.91% error\n   • Rank 20: 560 params, 1.2x compression, 1.90% error\n\n\n\n\n\nFor comparison, we also applied Tucker3 decomposition to the tongue shape data. Unlike PARAFAC, where a single rank is chosen for all modes, Tucker3 allows the number of components in each mode to be varied independently. We therefore tested different combinations of model sizes, systematically varying the dimensions of the grid positions, vowels, and speakers modes (e.g., [5, 8, 4] and other configurations).\nAs in the PARAFAC modeling, each Tucker3 model was fit by decomposing the tensor into three factor matrices and a core tensor. The factor matrices represent the main axes of variation within each mode, while the core tensor captures how these axes interact across grid positions, vowels, and speakers. After reconstruction, the error was computed and stored for each configuration, enabling a direct comparison to PARAFAC.\n\ndef tucker_modeling(X_tensor, max_rank=[10, 10, 10]):\n    \"\"\"\n    Tucker3 modeling\n    \n    Parameters:\n    -----------\n    X_tensor : numpy.ndarray\n        Input tensor to decompose\n    max_rank : list, default=[4, 4, 3]\n        Maximum rank for each mode [mode1, mode2, mode3]\n    \n    Returns:\n    --------\n    dict : Results dictionary with Tucker decomposition results\n    \"\"\"\n    ranks_to_test = []\n    for r1 in range(2, max_rank[0] + 1):\n        for r2 in range(2, max_rank[1] + 1):\n            for r3 in range(2, max_rank[2] + 1):\n                ranks_to_test.append([r1, r2, r3])\n    \n    results = {}\n    successful_count = 0\n    \n    for rank_combo in ranks_to_test:\n        try:\n            factors = tucker(\n                X_tensor, \n                rank=rank_combo,\n                init='random', \n                n_iter_max=200, \n                random_state=42\n            )\n            \n            reconstructed = tl.tucker_to_tensor(factors)\n            error = tl.norm(X_tensor - reconstructed)\n            rel_error = error / tl.norm(X_tensor)\n\n            core_params = np.prod(rank_combo) \n            factor_params = sum(X_tensor.shape[i] * rank_combo[i] for i in range(3))\n            total_params = core_params + factor_params\n            \n            key = str(rank_combo)\n            results[key] = {\n                'ranks': rank_combo,\n                'factors': factors,\n                'reconstructed': reconstructed,\n                'error': error,\n                'rel_error': rel_error,\n                'core_params': core_params,\n                'factor_params': factor_params,\n                'total_params': total_params\n            }\n            \n            successful_count += 1\n            \n        except Exception as e:\n            print(f\" Models failed - {str(e)[:30]}...\")\n            results[str(rank_combo)] = None\n            \n    return results\n\n\n\nCode\ndef tucker_results(model):\n    \"\"\"Analyze Tucker decomposition results\"\"\"\n    successful_results = {k: v for k, v in model.items() if v is not None}\n    \n    if not successful_results:\n        print(\" No successful Tucker decompositions to analyze\")\n        return\n    \n    best_key = min(successful_results.keys(), key=lambda k:\n      successful_results[k]['rel_error'])\n    \n    best_result = successful_results[best_key]\n    \n    print(f\" Best configuration: {best_result['ranks']}\")\n    print(f\"   - Relative error: {best_result['rel_error']*100:.3f}%\")\n    print(f\"   - Total parameters: {best_result['total_params']}\")\n    print(f\"   - Core parameters: {best_result['core_params']}\")\n    print(f\"   - Factor parameters: {best_result['factor_params']}\")\n    print(f\"\\n Top 5 configurations:\")\n    sorted_results = sorted(successful_results.items(), key=lambda x: x[1]['rel_error'])\n    \n    for i, (key, result) in enumerate(sorted_results[:5]):\n        compression_ratio = np.prod(tensor_dims) / result['total_params']\n        print(\n          f\"   {i+1}. {result['ranks']}: {result['rel_error']*100:.3f}% error, \"\n          f\"{result['total_params']} params, {compression_ratio:.1f}x compression\"\n          )\n    \n    print(f\"\\n Recommendations:\")\n    for threshold in [0.05, 0.01, 0.005]:\n        good_configs = [(k, v) for k, v in successful_results.items() \n                       if v['rel_error'] &lt; threshold]\n        if good_configs:\n            best_config = min(good_configs, key=lambda x: x[1]['total_params'])\n            result = best_config[1]\n            print(\n              f\"   • For &lt;{threshold*100:.1f}% error: {result['ranks']} \"\n              f\"({result['rel_error']*100:.3f}% error, {result['total_params']} params)\"\n              )\n\n\n\n\nCode\ndef plot_tucker_results(model):\n    \"\"\"Create comprehensive Tucker visualization\"\"\"\n    import matplotlib.pyplot as plt\n  \n    successful_results = {k: v for k, v in model.items() if v is not None}\n    \n    if not successful_results:\n        print(\" No successful decompositions to plot\")\n        return\n    \n    configs = []\n    rel_errors = []\n    total_params = []\n    core_params = []\n    factor_params = []\n    compression_ratios = []\n    \n    tensor_size = tensor_dims[0] * tensor_dims[1] * tensor_dims[2]\n    \n    for key, result in successful_results.items():\n        configs.append(key)\n        rel_errors.append(result['rel_error'] * 100)\n        total_params.append(result['total_params'])\n        core_params.append(result['core_params'])\n        factor_params.append(result['factor_params'])\n        compression_ratios.append(tensor_size / result['total_params'])\n    \n    rel_errors = np.array(rel_errors)\n    total_params = np.array(total_params)\n    compression_ratios = np.array(compression_ratios)\n    \n    fig = plt.figure(figsize=(18, 12))\n    \n    ax1 = plt.subplot(2, 3, 1)\n    sorted_indices = np.argsort(rel_errors)\n    top_20 = sorted_indices[:min(20, len(sorted_indices))]\n    \n    bars = plt.bar(range(len(top_20)), rel_errors[top_20], alpha=0.7, color='skyblue')\n    plt.xlabel('Configuration Rank')\n    plt.ylabel('Relative Error (%)')\n    plt.title('Tucker: Top 20 Configurations by Error')\n    plt.xticks(range(len(top_20)), [configs[i] for i in top_20], rotation=45, ha='right')\n    plt.grid(True, alpha=0.3)\n    \n    if len(top_20) &gt; 0:\n        bars[0].set_color('gold')\n        plt.text(0, rel_errors[top_20[0]] + 0.1, 'Best', ha='center', fontweight='bold')\n    \n    ax2 = plt.subplot(2, 3, 2)\n    scatter = plt.scatter(total_params, rel_errors, c=compression_ratios, \n                         cmap='viridis', alpha=0.7, s=60)\n    plt.xlabel('Total Parameters')\n    plt.ylabel('Relative Error (%)')\n    plt.title('Tucker: Error vs Model Complexity')\n    plt.grid(True, alpha=0.3)\n    \n    cbar = plt.colorbar(scatter)\n    cbar.set_label('Compression Ratio')\n    \n    pareto_indices = _find_pareto_front(total_params, rel_errors)\n    plt.plot(\n      total_params[pareto_indices], \n      rel_errors[pareto_indices], \n      'r-', \n      linewidth=2, \n      alpha=0.8, \n      label='Pareto Front'\n      )\n    plt.legend()\n    \n    ax3 = plt.subplot(2, 3, 3)\n    top_10_by_error = sorted_indices[:10]\n    \n    core_params_top = [core_params[i] for i in top_10_by_error]\n    factor_params_top = [factor_params[i] for i in top_10_by_error]\n    configs_top = [configs[i] for i in top_10_by_error]\n    \n    x_pos = range(len(top_10_by_error))\n    p1 = plt.bar(\n      x_pos, \n      core_params_top, \n      alpha=0.8, \n      color='coral', \n      label='Core Parameters'\n      )\n    p2 = plt.bar(\n      x_pos, \n      factor_params_top, \n      bottom=core_params_top, \n      alpha=0.8, \n      color='lightblue', \n      label='Factor Parameters'\n      )\n    \n    plt.xlabel('Configuration')\n    plt.ylabel('Number of Parameters')\n    plt.title('Tucker: Parameter Breakdown (Top 10)')\n    plt.xticks(x_pos, configs_top, rotation=45, ha='right')\n    plt.legend()\n    plt.grid(True, alpha=0.3)\n    \n    ax4 = plt.subplot(2, 3, 4)\n    plt.scatter(compression_ratios, rel_errors, alpha=0.7, color='green', s=60)\n    plt.xlabel('Compression Ratio')\n    plt.ylabel('Relative Error (%)')\n    plt.title('Tucker: Compression vs Accuracy Trade-off')\n    plt.grid(True, alpha=0.3)\n    \n    high_compression = compression_ratios &gt; 1.0\n    if np.any(high_compression):\n        best_compression_idx = np.argmax(compression_ratios[high_compression])\n        best_idx = np.where(high_compression)[0][best_compression_idx]\n        plt.annotate(\n          f'{configs[best_idx]}\\n{compression_ratios[best_idx]:.1f}x',\n          xy=(compression_ratios[best_idx], rel_errors[best_idx]),\n          xytext=(compression_ratios[best_idx]+0.1, rel_errors[best_idx]+1),\n          arrowprops=dict(arrowstyle='-&gt;', color='red'),\n          fontsize=9, \n          color='red'\n          )\n    \n    ax5 = plt.subplot(2, 3, 5)\n    plt.hist(rel_errors, bins=20, alpha=0.7, color='purple', edgecolor='black')\n    plt.xlabel('Relative Error (%)')\n    plt.ylabel('Frequency')\n    plt.title('Tucker: Error Distribution')\n    plt.axvline(\n      np.mean(rel_errors), \n      color='red', \n      linestyle='--', \n      label=f'Mean: {np.mean(rel_errors):.2f}%'\n      )\n    plt.axvline(\n      np.median(rel_errors), \n      color='orange', \n      linestyle='--', \n      label=f'Median: {np.median(rel_errors):.2f}%'\n      )\n    plt.legend()\n    plt.grid(True, alpha=0.3)\n    ax6 = plt.subplot(2, 3, 6)\n\n    if len(successful_results) &gt; 10:\n        rank_data = []\n        for key, result in successful_results.items():\n            r1, r2, r3 = result['ranks']\n            rank_data.append([r1, r2, r3, result['rel_error']])\n        \n        rank_data = np.array(rank_data)\n        unique_r1 = sorted(set(rank_data[:, 0]))\n        unique_r2 = sorted(set(rank_data[:, 1]))\n        \n        if len(unique_r1) &gt; 1 and len(unique_r2) &gt; 1:\n            heatmap_data = np.full((len(unique_r2), len(unique_r1)), np.nan)\n            \n            for i, r2 in enumerate(unique_r2):\n                for j, r1 in enumerate(unique_r1):\n                    mask = (rank_data[:, 0] == r1) & (rank_data[:, 1] == r2)\n                    if np.any(mask):\n                        heatmap_data[i, j] = np.min(rank_data[mask, 3]) * 100\n            \n            im = plt.imshow(heatmap_data, cmap='RdYlBu_r', aspect='auto')\n            plt.colorbar(im, label='Relative Error (%)')\n            plt.xlabel('Mode 1 Rank')\n            plt.ylabel('Mode 2 Rank')\n            plt.title('Tucker: Error Heatmap (Mode 1 vs Mode 2)')\n            plt.xticks(range(len(unique_r1)), unique_r1)\n            plt.yticks(range(len(unique_r2)), unique_r2)\n        else:\n            plt.text(\n              0.5, \n              0.5, \n              'Insufficient data\\nfor heatmap', \n               ha='center', \n               va='center', \n               transform=ax6.transAxes, fontsize=12\n               )\n    else:\n        plt.text(\n          0.5, \n          0.5, \n          'Too few configurations\\nfor heatmap', \n          ha='center', \n          va='center', \n          transform=ax6.transAxes, \n          fontsize=12\n          )\n    \n    plt.tight_layout()\n    plt.suptitle('Tucker Decomposition Analysis', fontsize=16, y=0.98)\n    plt.show()\n\ndef _find_pareto_front(params, errors):\n    \"\"\"Find Pareto front for parameter-error trade-off\"\"\"\n    sorted_indices = np.argsort(params)\n    pareto_indices = []\n    min_error_so_far = float('inf')\n    for idx in sorted_indices:\n        if errors[idx] &lt; min_error_so_far:\n            pareto_indices.append(idx)\n            min_error_so_far = errors[idx]\n    \n    return np.array(pareto_indices)\n    \"\"\"Analyze Tucker decomposition results\"\"\"\n    successful_results = {k: v for k, v in results.items() if v is not None}\n    \n    if not successful_results:\n        print(\" No successful Tucker decompositions to analyze\")\n        return\n    \n    best_key = min(successful_results.keys(), key=lambda k:\n      successful_results[k]['rel_error'])\n    best_result = successful_results[best_key]\n    \n    print(f\" Best configuration: {best_result['ranks']}\")\n    print(f\"   - Relative error: {best_result['rel_error']*100:.3f}%\")\n    print(f\"   - Total parameters: {best_result['total_params']}\")\n    print(f\"   - Core parameters: {best_result['core_params']}\")\n    print(f\"   - Factor parameters: {best_result['factor_params']}\")\n    \n    print(f\"\\n Top 5 configurations:\")\n    sorted_results = sorted(successful_results.items(), key=lambda x:\n      x[1]['rel_error'])\n    \n    for i, (key, result) in enumerate(sorted_results[:5]):\n        compression_ratio = np.prod(tensor_dims) / result['total_params']\n        print(\n          f\"   {i+1}. {result['ranks']}: {result['rel_error']*100:.3f}% error, \"\n          f\"{result['total_params']} params, {compression_ratio:.1f}x compression\"\n          )\n    \n    print(f\"\\n Recommendations:\")\n    for threshold in [0.05, 0.01, 0.005]:\n        good_configs = [(k, v) for k, v in successful_results.items() \n                       if v['rel_error'] &lt; threshold]\n        if good_configs:\n            best_config = min(good_configs, key=lambda x: x[1]['total_params'])\n            result = best_config[1]\n            print(\n              f\"   • For &lt;{threshold*100:.1f}% error: {result['ranks']} \"\n              f\"({result['rel_error']*100:.3f}% error, {result['total_params']} params)\"\n              )\n\n\nLikewise, the six-panel visualization below provides insights into Tucker decomposition performance across different core tensor configurations.\nThe top-left panel showcases the twenty best-performing configurations, with reconstruction errors clustered impressively between 0.5-2.0%. This immediately tells us that Tucker decomposition can achieve excellent accuracy for our tongue shape data when properly configured. The scatter plot of error versus model complexity illustrates the trade-off between accuracy and efficiency. The red Pareto Front marks the “optimal” configurations, where we cannot improve both accuracy and efficiency at the same time. In other words, if we want a lower error, we must accept higher complexity, and vice versa.\nThe stacked bars break down model complexity into two parts: parameters from the core tensor (orange) and those from the factor matrices (blue). This shows where the computational cost comes from in each model. The green dots compare compression ratio against reconstruction error. Configurations above the diagonal line achieve good compression while still keeping the error low, which makes them especially useful in practice.\nFinally, the histogram shows the overall distribution of errors. Most Tucker configurations fall between 8–10% error, with a median of 8.7% and a mean of 9.2%. A few standout models perform much better, achieving far lower error rates. The color-coded grid provides another view, showing how reconstruction error changes across rank combinations for the first two modes. This makes it easier to detect “sweet spots” where the model balances accuracy and efficiency most effectively.\n\ntucker_mod = tucker_modeling(X_tensor)\nplot_tucker_results(model = tucker_mod)\n\n\n\n\n\n\n\n\n\ntucker_results(model = tucker_mod)\n\n Best configuration: [10, 10, 10]\n   - Relative error: 1.348%\n   - Total parameters: 1280\n   - Core parameters: 1000\n   - Factor parameters: 280\n\n Top 5 configurations:\n   1. [10, 10, 10]: 1.348% error, 1280 params, 0.5x compression\n   2. [10, 10, 9]: 1.348% error, 1175 params, 0.6x compression\n   3. [10, 10, 8]: 1.348% error, 1070 params, 0.6x compression\n   4. [10, 10, 5]: 1.348% error, 755 params, 0.9x compression\n   5. [10, 10, 6]: 1.348% error, 860 params, 0.8x compression\n\n Recommendations:\n   • For &lt;5.0% error: [5, 9, 5] (4.703% error, 405 params)\n\n\n\n\n\nAfter optimizing both decomposition methods, we can now directly compare their performance on our tongue shape data. The comparison reveals distinct characteristics and trade-offs between the two approaches.\nThe original tensor slices in the top row provide our baseline reference, showing the complex spatial patterns of tongue positions across different speakers and vowels. Both PARAFAC and Tucker successfully capture the general structure of these patterns, but with notable differences in their approach and results.\nPARAFAC, configured with rank 7, demonstrates its signature strength in providing interpretable factor loadings across all three modes. The factor plots show clear, distinct patterns for each component, with the grid position factors revealing systematic spatial relationships and the vowel factors capturing acoustic-articulatory connections. The reconstruction achieves a 7.08% relative error with just 196 parameters, making it remarkably parameter-efficient. The error map shows relatively uniform reconstruction quality across the tensor space. Meanwhile, Tucker decomposition, using core dimensions [5, 9, 5], takes a fundamentally different approach with its more flexible structure. The factor matrices show more complex patterns, reflecting Tucker’s ability to capture asymmetric relationships between modes. With 405 parameters, Tucker achieves a superior 4.70% relative error, demonstrating the power of its additional flexibility. The core tensor visualization shows the internal structure that Tucker uses to combine these factors, something absent in PARAFAC’s simpler multiplicative model.\nThe direct comparison panels at the bottom quantify these trade-offs clearly. PARAFAC wins on parameter efficiency with 196 versus 405 parameters, translating to better compression ratios. However, Tucker delivers superior reconstruction accuracy with nearly 30% lower relative error. The reconstruction difference heatmap highlights where these methods disagree most strongly, typically in regions with complex multimodal interactions.\n\n\nCode\ndef compare_optimized_models(X_tensor, parafac_rank, tucker_ranks):\n    \"\"\"\n    Compare optimized PARAFAC and Tucker models with comprehensive analysis\n    \n    Parameters:\n    -----------\n    X_tensor : numpy.ndarray\n        Input tensor to decompose\n    parafac_rank : int\n        Optimal rank for PARAFAC\n    tucker_ranks : list\n        Optimal ranks for Tucker [mode1, mode2, mode3]\n    \"\"\"\n    results = {}\n    \n    # PARAFAC Decomposition\n    try:\n        parafac_factors = parafac(\n            X_tensor, \n            rank=parafac_rank, \n            init='random', \n            n_iter_max=200,\n            random_state=42\n        )\n        \n        X_parafac_reconstructed = tl.cp_to_tensor(parafac_factors)\n        parafac_error = tl.norm(X_tensor - X_parafac_reconstructed)\n        parafac_rel_error = parafac_error / tl.norm(X_tensor)\n        parafac_params = parafac_rank * sum(X_tensor.shape)\n        \n        results['PARAFAC'] = {\n            'success': True,\n            'factors': parafac_factors,\n            'reconstructed': X_parafac_reconstructed,\n            'error': parafac_error,\n            'rel_error': parafac_rel_error,\n            'rank': parafac_rank,\n            'params': parafac_params,\n            'method': 'PARAFAC'\n        }\n        \n    except Exception as e:\n        results['PARAFAC'] = {'success': False, 'error': str(e)}\n    \n    # Tucker Decomposition\n    try:\n        tucker_factors = tucker(\n            X_tensor, \n            rank=tucker_ranks, \n            init='random',\n            n_iter_max=200, \n            random_state=42\n        )\n        \n        X_tucker_reconstructed = tl.tucker_to_tensor(tucker_factors)\n        tucker_error = tl.norm(X_tensor - X_tucker_reconstructed)\n        tucker_rel_error = tucker_error / tl.norm(X_tensor)\n        \n        core_params = np.prod(tucker_ranks)\n        factor_params = sum(X_tensor.shape[i] * tucker_ranks[i] for i in range(3))\n        tucker_params = core_params + factor_params\n        \n        results['Tucker'] = {\n            'success': True,\n            'factors': tucker_factors,\n            'reconstructed': X_tucker_reconstructed,\n            'error': tucker_error,\n            'rel_error': tucker_rel_error,\n            'ranks': tucker_ranks,\n            'params': tucker_params,\n            'core_params': core_params,\n            'factor_params': factor_params,\n            'method': 'Tucker'\n        }\n        \n    except Exception as e:\n        results['Tucker'] = {'success': False, 'error': str(e)}\n    \n    _visualization(results, X_tensor)\n    _summary(results)\n    \n    return results\n\ndef _visualization(results, X_tensor):\n    \"\"\"Create comprehensive comparison visualization\"\"\"\n    successful_methods = [method for method, result in results.items() if result['success']]\n    \n    if not successful_methods:\n        return\n    \n    fig = plt.figure(figsize=(20, 14))\n    \n    # Original tensor slices\n    for i in range(min(5, X_tensor.shape[2])):\n        ax = plt.subplot(4, 6, i+1)\n        im = plt.imshow(X_tensor[:, :, i], cmap='viridis', aspect='auto')\n        plt.title(f'Original Tensor\\nSlice {i+1}', fontsize=10)\n        plt.colorbar(im, shrink=0.6)\n        if i == 0:\n            plt.ylabel('Mode 1', fontsize=9)\n        plt.xlabel('Mode 2', fontsize=9)\n    \n    # Tensor statistics\n    ax = plt.subplot(4, 6, 6)\n    ax.text(0.1, 0.8, f'Shape: {X_tensor.shape}', fontsize=12, transform=ax.transAxes)\n    ax.text(0.1, 0.6, f'Elements: {np.prod(X_tensor.shape)}', fontsize=12, transform=ax.transAxes)\n    ax.text(0.1, 0.4, f'Norm: {tl.norm(X_tensor):.3f}', fontsize=12, transform=ax.transAxes)\n    ax.text(0.1, 0.2, f'Range: [{X_tensor.min():.3f}, {X_tensor.max():.3f}]', fontsize=12, transform=ax.transAxes)\n    ax.set_title('Tensor Stats', fontsize=10)\n    ax.axis('off')\n    \n    # PARAFAC Analysis\n    if 'PARAFAC' in successful_methods:\n        result = results['PARAFAC']\n        factors = result['factors'][1]\n        colors = ['blue', 'red', 'green', 'orange']\n        \n        # PARAFAC factor matrices\n        for mode in range(3):\n            ax = plt.subplot(4, 6, 7 + mode)\n            n_comps_show = min(4, factors[mode].shape[1])\n            for comp in range(n_comps_show):\n                plt.plot(factors[mode][:, comp], color=colors[comp], \n                        label=f'Comp {comp+1}', marker='o', markersize=3, linewidth=1.5)\n            plt.title(f'PARAFAC Mode {mode+1}', fontsize=10)\n            plt.xlabel('Index', fontsize=9)\n            plt.ylabel('Factor Value', fontsize=9)\n            if mode == 0:\n                plt.legend(fontsize=7)\n            plt.grid(True, alpha=0.3)\n        \n        # PARAFAC reconstruction\n        ax = plt.subplot(4, 6, 10)\n        recon_slice = result['reconstructed'][:, :, 0]\n        im = plt.imshow(recon_slice, cmap='viridis', aspect='auto')\n        plt.title(f'PARAFAC Recon', fontsize=10)\n        plt.colorbar(im, shrink=0.6)\n        \n        # PARAFAC error\n        ax = plt.subplot(4, 6, 11)\n        error_slice = np.abs(X_tensor[:, :, 0] - recon_slice)\n        im = plt.imshow(error_slice, cmap='Reds', aspect='auto')\n        plt.title(f'PARAFAC Error', fontsize=10)\n        plt.colorbar(im, shrink=0.6)\n        \n        # PARAFAC stats\n        ax = plt.subplot(4, 6, 12)\n        ax.text(0.1, 0.8, f\"Rank: {result['rank']}\", fontsize=12, transform=ax.transAxes)\n        ax.text(0.1, 0.6, f\"Error: {result['rel_error']*100:.3f}%\", fontsize=12, transform=ax.transAxes)\n        ax.text(0.1, 0.4, f\"Params: {result['params']}\", fontsize=12, transform=ax.transAxes)\n        compression = np.prod(X_tensor.shape) / result['params']\n        ax.text(0.1, 0.2, f\"Compression: {compression:.2f}x\", fontsize=12, transform=ax.transAxes)\n        ax.set_title('PARAFAC Stats', fontsize=10)\n        ax.axis('off')\n    \n    # Tucker Analysis\n    if 'Tucker' in successful_methods:\n        result = results['Tucker']\n        factors = result['factors'][1]\n        \n        # Tucker factor matrices\n        for mode in range(3):\n            ax = plt.subplot(4, 6, 13 + mode)\n            n_comps_show = min(4, factors[mode].shape[1])\n            for comp in range(n_comps_show):\n                plt.plot(factors[mode][:, comp], color=colors[comp], \n                        label=f'Comp {comp+1}', marker='s', markersize=3, linewidth=1.5)\n            plt.title(f'Tucker Mode {mode+1}', fontsize=10)\n            plt.xlabel('Index', fontsize=9)\n            plt.ylabel('Factor Value', fontsize=9)\n            if mode == 0:\n                plt.legend(fontsize=7)\n            plt.grid(True, alpha=0.3)\n        \n        # Tucker core tensor\n        ax = plt.subplot(4, 6, 16)\n        core = result['factors'][0]\n        core_slice = core[:, :, 0] if core.ndim == 3 else core\n        im = plt.imshow(core_slice, cmap='RdBu_r', aspect='auto')\n        plt.title('Tucker Core', fontsize=10)\n        plt.colorbar(im, shrink=0.6)\n        \n        # Tucker reconstruction\n        ax = plt.subplot(4, 6, 17)\n        recon_slice = result['reconstructed'][:, :, 0]\n        im = plt.imshow(recon_slice, cmap='viridis', aspect='auto')\n        plt.title(f'Tucker Recon', fontsize=10)\n        plt.colorbar(im, shrink=0.6)\n        \n        # Tucker stats\n        ax = plt.subplot(4, 6, 18)\n        ax.text(0.1, 0.8, f\"Ranks: {result['ranks']}\", fontsize=12, transform=ax.transAxes)\n        ax.text(0.1, 0.6, f\"Error: {result['rel_error']*100:.3f}%\", fontsize=12, transform=ax.transAxes)\n        ax.text(0.1, 0.4, f\"Params: {result['params']}\", fontsize=12, transform=ax.transAxes)\n        compression = np.prod(X_tensor.shape) / result['params']\n        ax.text(0.1, 0.2, f\"Compression: {compression:.2f}x\", fontsize=12, transform=ax.transAxes)\n        ax.set_title('Tucker Stats', fontsize=10)\n        ax.axis('off')\n    \n    # Direct Comparison\n    if len(successful_methods) &gt;= 2:\n        # Error comparison\n        ax = plt.subplot(4, 6, 19)\n        methods = [result['method'] for result in results.values() if result['success']]\n        errors = [result['rel_error'] for result in results.values() if result['success']]\n        colors_comp = ['skyblue', 'lightcoral'][:len(methods)]\n        \n        bars = plt.bar(methods, errors, color=colors_comp, alpha=0.7)\n        plt.title('Error Comparison', fontsize=10)\n        plt.ylabel('Relative Error', fontsize=9)\n        plt.xticks(rotation=45)\n        \n        for bar, error in zip(bars, errors):\n            plt.text(\n                bar.get_x() + bar.get_width()/2, \n                bar.get_height() + 0.002,\n                f'{error:.4f}', \n                ha='center', \n                va='bottom', \n                fontsize=8\n            )\n        \n        # Parameter comparison\n        ax = plt.subplot(4, 6, 20)\n        params = [result['params'] for result in results.values() if result['success']]\n        bars = plt.bar(methods, params, color=colors_comp, alpha=0.7)\n        plt.title('Parameters', fontsize=10)\n        plt.ylabel('# Parameters', fontsize=9)\n        plt.xticks(rotation=45)\n        \n        for bar, param in zip(bars, params):\n            plt.text(\n                bar.get_x() + bar.get_width()/2, \n                bar.get_height() + max(params)*0.02,\n                f'{param}', \n                ha='center', \n                va='bottom', \n                fontsize=8\n            )\n        \n        # Reconstruction difference\n        ax = plt.subplot(4, 6, 21)\n        if len(successful_methods) == 2:\n            parafac_recon = results['PARAFAC']['reconstructed'][:, :, 0] if 'PARAFAC' in results and results['PARAFAC']['success'] else None\n            tucker_recon = results['Tucker']['reconstructed'][:, :, 0] if 'Tucker' in results and results['Tucker']['success'] else None\n            \n            if parafac_recon is not None and tucker_recon is not None:\n                diff = np.abs(parafac_recon - tucker_recon)\n                im = plt.imshow(diff, cmap='plasma', aspect='auto')\n                plt.title('Reconstruction\\nDifference', fontsize=10)\n                plt.colorbar(im, shrink=0.6)\n            else:\n                plt.text(0.5, 0.5, 'Cannot compute\\ndifference', ha='center', va='center', \n                        transform=ax.transAxes, fontsize=10)\n        else:\n            plt.text(0.5, 0.5, 'Need both methods\\nfor comparison', ha='center', va='center', \n                    transform=ax.transAxes, fontsize=10)\n        \n        # Compression comparison\n        ax = plt.subplot(4, 6, 22)\n        compressions = [np.prod(X_tensor.shape) / result['params'] for result in results.values() if result['success']]\n        bars = plt.bar(methods, compressions, color=colors_comp, alpha=0.7)\n        plt.title('Compression Ratio', fontsize=10)\n        plt.ylabel('Compression', fontsize=9)\n        plt.xticks(rotation=45)\n        \n        for bar, comp in zip(bars, compressions):\n            plt.text(\n                bar.get_x() + bar.get_width()/2, \n                bar.get_height() + max(compressions)*0.02,\n                f'{comp:.2f}x', \n                ha='center', \n                va='bottom', \n                fontsize=8\n            )\n        \n        # Explained variance comparison\n        ax = plt.subplot(4, 6, 23)\n        explained_vars = [(1 - result['rel_error']**2) * 100 for result in results.values() if result['success']]\n        bars = plt.bar(methods, explained_vars, color=colors_comp, alpha=0.7)\n        plt.title('Explained Variance', fontsize=10)\n        plt.ylabel('Variance (%)', fontsize=9)\n        plt.xticks(rotation=45)\n        plt.ylim([0, 100])\n        \n        for bar, var in zip(bars, explained_vars):\n            plt.text(\n                bar.get_x() + bar.get_width()/2, \n                bar.get_height() + 2,\n                f'{var:.1f}%', \n                ha='center', \n                va='bottom', \n                fontsize=8\n            )\n    \n    plt.tight_layout()\n    plt.suptitle('PARAFAC vs Tucker: Comparison Results', fontsize=16, y=0.98)\n    plt.show()\n    plt.close()\n\ndef _summary(results):\n    \"\"\"Print minimal final summary\"\"\"\n    successful_methods = [method for method, result in results.items() if result['success']]\n    \n    if successful_methods:\n        best_method = min(successful_methods, key=lambda m: results[m]['rel_error'])\n        best_result = results[best_method]\n        \n        print(f\"Best performing method: {best_method}\")\n        print(f\"  • Relative error: {best_result['rel_error']*100:.3f}%\")\n        print(f\"  • Model parameters: {best_result['params']}\")\n\n\n\nresults = compare_optimized_models(\n    X_tensor, \n    parafac_rank=7, \n    tucker_ranks=[5, 9, 5]\n    )\n\nBest performing method: Tucker\n  • Relative error: 4.703%\n  • Model parameters: 405\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAlthrough, both methods achieve high explained variance (over 95%), but Tucker’s additional parameters allow it to capture subtle patterns that PARAFAC’s constrained structure cannot represent. In other word, PARAFAC offers cleaner, more interpretable results, while Tucker provides superior reconstruction quality for applications where accuracy trumps interpretability.\nThe comparison suggests that for our tongue shape data, both methods successfully identify meaningful patterns, but serve different analytical purposes depending on whether the priority is understanding the underlying structure or achieving the most accurate data representation."
  },
  {
    "objectID": "blog/posts/post10/index.html#introduction",
    "href": "blog/posts/post10/index.html#introduction",
    "title": "A Gentle Introduction to Allan Deviation Measure",
    "section": "",
    "text": "Traditional second-moment measures of dispersion like standard deviation are often inadequate for quantifying stability over varying time intervals, particularly for non-stationary time series data in which noise processes that change over time. This limitation led to the development of the Allan deviation in the 1960s by David Allan, then working at the National Bureau of Standards (now NIST). His work was driven by the need to compare and improve the stability of atomic clocks and frequency standards, crucial for navigation, telecommunications, and fundamental physics experiments.\nThe standard deviation, \\(s\\), is given by:\n\\[\ns = \\sqrt{s^2}\n\\]\nwhere \\(s^2\\) is the standard variance and is defined as:\n\\[\ns^2 = \\frac{1}{N-1}\\sum_{i=1}^N(y_i-\\bar{y})^2\n\\]\nwith \\(y_i\\) are the \\(N\\) fractional frequency values, and \\(\\bar{y} = \\frac{1}{N}\\sum_{i=1}^N y_i\\) is the average frequency.\nThe use of standard deviation as a measure of dispersion in non-stationary noises, such as flicker FM noise, reveals significant limitations, as shown in the figure below.\n\n\n\n\n\n\n\n\n\nWe can see that standard deviation increases with sample size. This phenomenon arises because standard deviation relies on deviations from the mean, which is inadequate for non-stationary time series. In contrast, employing Allan variance mitigates these issues by utilizing first differences of fractional frequency values, thus providing a more reliable metric for frequency stability.\nAllan variance serves as a robust tool for assessing frequency stability across varying averaging times, effectively distinguishing between different noise processes. Its square root, known as Allan deviation, is particularly valued for its interpretability and ease of comparison, making it a foundational measure in fields such as metrology, geophysics, and laser spectroscopy.\n\n\nThe phase and frequency fluctuations of a frequency source can often be well-described by one or more power law noise processes having a spectral characteristic of \\(S(f) \\approx f^{\\alpha}\\) where \\(f\\) is the Fourier or sideband frequency in hertz and \\(\\alpha\\) is the power law exponent. Note that power law noise doesn’t necessarily have to have an integer \\(\\alpha\\), mixtures of noise types are possible.\n\n\n\nNoise\nColor\n\\(\\alpha\\) (Phase)\n\\(\\alpha\\) (Frequency)\n\n\n\n\nWhite\nWhite\n2\n0\n\n\nFlicker\nPink\n1\n-1\n\n\nRandom Walk\nBrown\n0\n-2\n\n\n\nThe more divergent noise types are sometimes referred to by their color. White noise has a flat spectral density (by analogy to white light). Flicker noise has an \\(f^{-1}\\) spectral density, and is called pink or red (more energy toward lower frequencies). Random walk has \\(f^{-2}\\) spectral density is called brown, and \\(f^{-3}\\) (flicker walk) noise is called black. Because the white, flicker, and random walk noise types can apply to either phase or frequency data, these three noise types, along with phase-frequency conversions, will cover all five common noises. Note that those conversions change the exponent by 2, and that white frequency noise is the same as random walk phase noise, since both have \\(\\alpha = 0\\).\n\nWhite frequency noise: This noise arises from random fluctuations in the frequency and is common in many physical systems.\nFlicker frequency noise: Steady noise level independent of the averaging time.\nRandom walk frequency noise: This type of noise often results from cumulative effects, such as thermal expansion or mechanical drift, that grow over time.\nWhite phase noise: Very short-term fluctuations in the phase.\nFlicker phase noise: This noise has a slower decrease with averaging time compared to white phase noise.\n\n\n\n\nExamples of the four most common of these noises (i.e., \\(\\alpha = 0, 1, 2\\) and \\(3\\)) are shown below."
  },
  {
    "objectID": "blog/posts/post10/index.html#allan-variance",
    "href": "blog/posts/post10/index.html#allan-variance",
    "title": "A Gentle Introduction to Allan Deviation Measure",
    "section": "",
    "text": "Each type of noise dominates at different time scales and affects the stability of a frequency source differently. The Allan variance is uniquely suited to distinguish these noise types because it analyzes stability as a function of the averaging time \\(\\tau\\). Mathematically, the Allan variance, \\(\\sigma_y^2(\\tau)\\), is defined as:\n\\[\n\\sigma_y^2(\\tau) = \\frac{1}{2(N-1)}\\sum_{i=1}^{N-1}(\\bar{y}_{i+1}-\\bar{y}_i)^2\n\\]\nwhere, \\(\\bar{y}_i\\) is the time-series of fractional frequency at averaging time, and \\(N\\) is the total number of averaging intervals.\nIn terms of time-series of phase data, the Allan variance is calculated as:\n\\[\n\\sigma_y^2(\\tau = m\\tau_0) = \\frac{1}{2(m\\tau_0)^2(N-2)}\\sum_{i=1}^{N-2}(x_{i+2}-2x_{i+1}+x_i)^2\n\\]\nwhere \\(x_i\\) is the \\(i\\)-th of the \\(N\\) phase values spaced by the measurement interval \\(\\tau\\), \\(m\\) is the averaging factor and \\(\\tau_0\\) is the basic measurement interval.\nJust as with standard deviation, the Allan deviation, \\(\\sigma_y(\\tau)\\), is defined as the squared root of the Allan variance and is given by:\n\\[\n\\sigma_y(\\tau) = \\sqrt{\\sigma^2_y(\\tau)}\n\\]\nIt computes the variance between consecutive overlapping time intervals in the time series, with each pair of adjacent intervals contributing a single term to the computation. Although it is well suited for characterizing white noise and random walk frequency noise, it does not use all available data because it relies on non-overlapping intervals. This results in reduced statistical efficiency for longer time series. For white FM noise, the Allan variance is equivalent to the ordinary variance. However, it has the advantage of converging to a finite value for more divergent types of noise, such as flicker noise, regardless of the number of samples. The confidence interval of an estimate of the Allan variance depends on the type of noise and is typically estimated as \\(±\\frac{\\sigma_y(\\tau)}{\\sqrt{N}}\\).\n\n\nThe maximal overlap Allan deviation is a refinement of the classical Allan deviation that uses all possible overlapping pairs of adjacent intervals for the computation. It maximizes the overlap by considering every possible pair of adjacent intervals of duration \\(\\tau\\), rather than restricting to non-overlapping intervals. Consequently, the maximal overlap Allan deviation increases statistical efficiency since it uses the entire dataset for each \\(\\tau\\) while providing smoother and more reliable results, especially for longer datasets or when the signal has non-white noise components. The overlapped Allan deviation is the most common measure of time-domain frequency stability. The accronym AVAR has come to be used mainly for this form of the Allan variance, and ADEV for its square root.\nThe maximal overlap Allan variance is given by:\n\\[\n\\sigma_y^2 = \\frac{1}{2m^2(N-2m+1)}\\sum_{j=1}^{N-2m+1} \\left( \\sum_{i=j}^{j+m-1}(\\bar{y}_{i+m}-\\bar{y}_i) \\right )^2\n\\]\nIn terms of phase data, the overlapping Allan variance can be estimated from a set of \\(N\\) time measurements as:\n\\[\n\\sigma^2_y(m\\tau_0) = { 1 \\over 2 (m \\tau_0 )^2 (N-2m) }\n\\sum_{n=1}^{N-2m} ( {x}_{n+2m} - 2x_{n+1m} + x_{n} )^2\n\\]\n\n\n\nThe modified Allan variance is another common time domain measure of frequency stability that address the inability to separate white phase modulation from flicker phase modulation using traditional Allan variance. It is estimated from a set of \\(N\\) frequency measurements for averaging time \\(\\tau = m\\tau_0\\), by the expression:\n\\[\n\\sigma_y^2(\\tau) = \\frac{1}{2m^4(N-3m+2)}\\sum_{j=1}^{N-3m+2} \\left[ \\sum_{i=j}^{i+m-1} \\left( \\sum_{k=i}^{i+m-1}(y_{k+m} - y_k) \\right) \\right]^2\n\\]\nIn terms of phase data,\n\\[\n\\sigma^2_y(m\\tau_0) = { 1 \\over 2 (m \\tau_0 )^2 (N-3m+1) }\n\\sum_{j=1}^{N-3m+1} \\left[\n\\sum_{i=j}^{j+m-1} {x}_{i+2m} - 2x_{i+m} + x_{i} \\right]^2\n\\] The modified Allan variance is the same as the normal Allan variance for \\(m\\) = 1. It includes an additional phase averaging operation, and has the advantage of being able to distinguish between white and flicker PM noise. It offers the highest statistical efficiency of the three methods, capturing long-range correlations in the data, making it more robust for non-stationary or highly correlated signals. However, the computational complexity is higher due to the extensive overlap and inclusion of all interval combinations.\n\n\n\n\n\n\n\n\n\nFeature\nAllan variance\nOverlapping Allan variance\nModified Allan variance\n\n\n\n\nOverlap\nNo\nAdjacent intervals only\nAll intervals of length \\(\\tau\\)\n\n\nEfficiency\nLow\nHigher than classical\nHighest\n\n\nComplexity\nLow\nModerate\nHigh\n\n\nNoise Sensitivity\nGood for simple noise models\nBetter for colored noise\nBest for complex noise structures\n\n\nUse Cases\nSimple analysis\nReliable characterization\nAdvanced analysis with long-range correlations\n\n\n\n\n\n\nBelow, we demonstrate how to plot the Allan deviation for three common noise types: white noise, flicker noise, and random walk noise.\n\n\n\n\n\n\n\n\n\nThe plotted Allan deviations allow for a direct comparison of the stability characteristics of these noise types. Each noise type has a distinct slope and pattern, providing valuable insights into the underlying system dynamics:\n\nWhite Noise: Characterized by a slope of \\(\\sigma_y(\\tau) \\propto \\tau^{-1/2}​\\). This rapid decrease reflects uncorrelated, short-term fluctuations. White noise is often the baseline noise type for many systems.\nFlicker Noise: Exhibits a slope of \\(\\sigma_y(\\tau) \\propto \\tau^0\\), meaning the Allan deviation is constant regardless of averaging time. This indicates long-term correlated instability, which is common in clocks and oscillators.\nRandom Walk Noise: Displays a slope of \\(\\sigma_y(\\tau) \\propto \\tau^{+1/2}\\). The increase in Allan deviation with \\(\\tau\\) reflects cumulative drift over time, which is characteristic of systems with strong correlations or random-walk-like behavior."
  },
  {
    "objectID": "blog/posts/post11/index.html",
    "href": "blog/posts/post11/index.html",
    "title": "How to Accept Column Names in R",
    "section": "",
    "text": "In R, it’s common to work with data frames and their columns. However, when writing functions that accept column names, you might encounter a situation where you want to allow users to specify these names in various formats. For example, let’s consider the following function:\n\nsuppressMessages(library(tidyverse))\ndata(mtcars)\n\n\nplot_fun &lt;- function(data, x, y, color.var = NULL, type = \"scatter\") {\n  \n  if (!x %in% names(data)) {\n    stop(paste(\"Column\", x, \"not found in data\"))\n  }\n  \n  if (!y %in% names(data)) {\n    stop(paste(\"Column\", y, \"not found in data\"))\n  }\n  \n  if (!is.null(color.var)) {\n    if (!color.var %in% names(data)) {\n    stop(paste(\"Column\", color.var, \"not found in data\"))\n  }\n    data &lt;- data |&gt; mutate(across(all_of(color.var), as.factor))\n    p &lt;- ggplot(data, aes(x = .data[[x]], y = .data[[y]], fill = .data[[color.var]]))\n  } else {\n    p &lt;- ggplot(data, aes(x = .data[[x]], y = .data[[y]]))\n  }\n  \n  if (type == \"scatter\") {\n    p &lt;- p + geom_point(size = 4, color = \"black\", shape = 21)\n  } else if (type == \"line\") {\n    p &lt;- p + geom_line()\n  } else if (type == \"bar\") {\n    p &lt;- p + geom_col()\n  } else {\n    stop(\"type must be 'scatter', 'line', or 'bar'\")\n  }\n  \n  p &lt;- p + labs(x = x, y = y, title = paste(y, \"vs\", x))\n  return(p)\n}\n\n\nmtcars %&gt;% plot_fun(\"mpg\", \"hp\", \"cyl\")\n\n\n\n\n\n\n\n\n\n\n\nTo handle this in a more tidyverse-friendly way, we can use non-standard evaluation (NSE) with rlang:\n\nlibrary(rlang)\n\n\nAttaching package: 'rlang'\n\n\nThe following objects are masked from 'package:purrr':\n\n    %@%, flatten, flatten_chr, flatten_dbl, flatten_int, flatten_lgl,\n    flatten_raw, invoke, splice\n\n\n\nplot_fun &lt;- function(data, x, y, color.var = NULL, type = \"scatter\") {\n  \n  # Capture unquoted arguments\n  x_enquo &lt;- enquo(x)\n  y_enquo &lt;- enquo(y)\n  color_enquo &lt;- enquo(color.var)\n  \n  # Convert to string names for validation and labels\n  x_name &lt;- as_name(x_enquo)\n  y_name &lt;- as_name(y_enquo)\n  \n  \n  if (!x_name %in% names(data)) {\n    stop(paste(\"Column\", x_name, \"not found in data\"))\n  }\n  if (!y_name %in% names(data)) {\n    stop(paste(\"Column\", y_name, \"not found in data\"))\n  }\n  \n  if (!quo_is_null(color_enquo)) {\n    color_name &lt;- as_name(color_enquo)\n    if (!color_name %in% names(data)) {\n      stop(paste(\"Column\", color_name, \"not found in data\"))\n    }\n    data &lt;- data |&gt; mutate(across({{ color.var }}, as.factor))\n    p &lt;- ggplot(data, aes(x = {{ x }}, y = {{ y }}, fill = {{ color.var }}))\n  } else {\n    p &lt;- ggplot(data, aes(x = {{ x }}, y = {{ y }}))\n  }\n  \n  if (type == \"scatter\") {\n    p &lt;- p + geom_point(size = 4, color = \"black\", shape = 21)\n  } else if (type == \"line\") {\n    p &lt;- p + geom_line()\n  } else if (type == \"bar\") {\n    p &lt;- p + geom_col()\n  } else {\n    stop(\"type must be 'scatter', 'line', or 'bar'\")\n  }\n  \n  p &lt;- p + labs(x = x_name, y = y_name, title = paste(y_name, \"vs\", x_name))\n  return(p)\n}\n\n\nmtcars %&gt;% plot_fun(mpg, hp, cyl)\n\n\n\n\n\n\n\n\nThis approach captures the input as a symbol and then converts it to a string, allowing for flexible input formats while maintaining clarity in your code."
  },
  {
    "objectID": "blog/posts/post11/index.html#the-challenge",
    "href": "blog/posts/post11/index.html#the-challenge",
    "title": "How to Accept Column Names in Different Formats in R",
    "section": "",
    "text": "In R, it’s common to work with data frames and their columns. However, when writing functions that accept column names, you might encounter a situation where you want to allow users to specify these names in various formats. For example, let’s consider a function fun &lt;- function(var, iter = 100) that need to accept column names in different ways:\n\nAs unquoted names: var = timestamp\nWith quotes: var = \"Timestamp\"\nWith backticks: var =`time stamp`\n\nWe can solve this manually by using substitute() to capture the raw input and then process it. However, this can lead to some confusion, especially when dealing with different quoting styles.\n1. substitute() - Capturing the Raw Input\nsubstitute(var)\nsubstitute() captures exactly what the user typed, without evaluating it:\n\nIf user typed: var = timestamp → returns a symbol/name object\nIf user typed: time_col = \"time Stamp\" → returns the actual string\nIf user typed: var =`time stamp` → returns a symbol/name object with backticks\n\n2. is.character() - Checking the Type\nis.character(substitute(var))\nThis checks if what was passed is already a character string:\n\n\"timestamp → TRUE (it’s already a string)\ntimestamp or time stamp → FALSE (they’re symbols/names)\n\n3. The Conditional Logic\nvar_str &lt;- if (is.character(substitute(time_col))) {\n    var              # If already a string, use it directly\n  } else {\n    deparse(substitute(var))  # If a symbol, convert to string\n  }\nThis logic allows us to handle both cases: if the input is a character string, we use it directly; if it’s a symbol or name, we convert it to a string using deparse().\n4. gsub() - Cleaning Up Backticks\nvar_str &lt;- gsub(\"`\", \"\", var_str)\nThis step removes any backticks from the string, ensuring that we have a clean column name to work with.\n\n\nLet’s trace through different inputs:\nInput 1: var = \"timestamp\"\nsubstitute(var) → \"timestamp\"\nis.character(\"timestamp\") → TRUE\ntime_col_str ← \"timestamp\ngsub(\"`\", \"\", \"timestamp\") → \"timestamp\"\nInput 2: var = timestamp\nsubstitute(var) → timestamp (symbol)\nis.character(timestamp) → FALSE\ndeparse(timestamp) → \"timestamp\"\nvar_str ← \"Timestamp\"\ngsub(\"`\", \"\", \"timestamp\") → \"timestamp\"\nInput 3: var =`time stamp```` r substitute(var) →time stamp(symbol with backticks) is.character(time stamp) → FALSE deparse(time stamp) → \"time stamp\" var_str ← \"time stamp\" gsub(\"“,”“,”time stamp“) →”time stamp”\n\n## Tidyverse NSE Approach\nTo handle this in a more tidyverse-friendly way, we can use non-standard evaluation (NSE) with `rlang`:\n\n``` r\n# Option 1: Using enquo() + as_label()\ntime_col_str &lt;- as_label(enquo(time_col))\n\n# Option 2: Using ensym() + as_string() (what I used originally)\ntime_col_sym &lt;- ensym(time_col)\ntime_col_str &lt;- as_string(time_col_sym)\nThis approach captures the input as a symbol and then converts it to a string, allowing for flexible input formats while maintaining clarity in your code."
  },
  {
    "objectID": "blog/posts/post11/index.html#manual-approach",
    "href": "blog/posts/post11/index.html#manual-approach",
    "title": "How to Accept Column Names in R",
    "section": "",
    "text": "In R, it’s common to work with data frames and their columns. However, when writing functions that accept column names, you might encounter a situation where you want to allow users to specify these names in various formats. For example, let’s consider the following function:\n\nsuppressMessages(library(tidyverse))\ndata(mtcars)\n\n\nplot_fun &lt;- function(data, x, y, color.var = NULL, type = \"scatter\") {\n  \n  if (!x %in% names(data)) {\n    stop(paste(\"Column\", x, \"not found in data\"))\n  }\n  \n  if (!y %in% names(data)) {\n    stop(paste(\"Column\", y, \"not found in data\"))\n  }\n  \n  if (!is.null(color.var)) {\n    if (!color.var %in% names(data)) {\n    stop(paste(\"Column\", color.var, \"not found in data\"))\n  }\n    data &lt;- data |&gt; mutate(across(all_of(color.var), as.factor))\n    p &lt;- ggplot(data, aes(x = .data[[x]], y = .data[[y]], fill = .data[[color.var]]))\n  } else {\n    p &lt;- ggplot(data, aes(x = .data[[x]], y = .data[[y]]))\n  }\n  \n  if (type == \"scatter\") {\n    p &lt;- p + geom_point(size = 4, color = \"black\", shape = 21)\n  } else if (type == \"line\") {\n    p &lt;- p + geom_line()\n  } else if (type == \"bar\") {\n    p &lt;- p + geom_col()\n  } else {\n    stop(\"type must be 'scatter', 'line', or 'bar'\")\n  }\n  \n  p &lt;- p + labs(x = x, y = y, title = paste(y, \"vs\", x))\n  return(p)\n}\n\n\nmtcars %&gt;% plot_fun(\"mpg\", \"hp\", \"cyl\")"
  },
  {
    "objectID": "blog/posts/post11/index.html#tidyverse-nse-approach",
    "href": "blog/posts/post11/index.html#tidyverse-nse-approach",
    "title": "How to Accept Column Names in R",
    "section": "",
    "text": "To handle this in a more tidyverse-friendly way, we can use non-standard evaluation (NSE) with rlang:\n\nlibrary(rlang)\n\n\nAttaching package: 'rlang'\n\n\nThe following objects are masked from 'package:purrr':\n\n    %@%, flatten, flatten_chr, flatten_dbl, flatten_int, flatten_lgl,\n    flatten_raw, invoke, splice\n\n\n\nplot_fun &lt;- function(data, x, y, color.var = NULL, type = \"scatter\") {\n  \n  # Capture unquoted arguments\n  x_enquo &lt;- enquo(x)\n  y_enquo &lt;- enquo(y)\n  color_enquo &lt;- enquo(color.var)\n  \n  # Convert to string names for validation and labels\n  x_name &lt;- as_name(x_enquo)\n  y_name &lt;- as_name(y_enquo)\n  \n  \n  if (!x_name %in% names(data)) {\n    stop(paste(\"Column\", x_name, \"not found in data\"))\n  }\n  if (!y_name %in% names(data)) {\n    stop(paste(\"Column\", y_name, \"not found in data\"))\n  }\n  \n  if (!quo_is_null(color_enquo)) {\n    color_name &lt;- as_name(color_enquo)\n    if (!color_name %in% names(data)) {\n      stop(paste(\"Column\", color_name, \"not found in data\"))\n    }\n    data &lt;- data |&gt; mutate(across({{ color.var }}, as.factor))\n    p &lt;- ggplot(data, aes(x = {{ x }}, y = {{ y }}, fill = {{ color.var }}))\n  } else {\n    p &lt;- ggplot(data, aes(x = {{ x }}, y = {{ y }}))\n  }\n  \n  if (type == \"scatter\") {\n    p &lt;- p + geom_point(size = 4, color = \"black\", shape = 21)\n  } else if (type == \"line\") {\n    p &lt;- p + geom_line()\n  } else if (type == \"bar\") {\n    p &lt;- p + geom_col()\n  } else {\n    stop(\"type must be 'scatter', 'line', or 'bar'\")\n  }\n  \n  p &lt;- p + labs(x = x_name, y = y_name, title = paste(y_name, \"vs\", x_name))\n  return(p)\n}\n\n\nmtcars %&gt;% plot_fun(mpg, hp, cyl)\n\n\n\n\n\n\n\n\nThis approach captures the input as a symbol and then converts it to a string, allowing for flexible input formats while maintaining clarity in your code."
  },
  {
    "objectID": "blog/posts/post9/index.html",
    "href": "blog/posts/post9/index.html",
    "title": "Covariance vs Correlation in PCA: What’s the Difference?",
    "section": "",
    "text": "Photo by Colin Watts.\n\n\n\n\nPrincipal Component Analysis (PCA) is a linear dimensionality reduction technique that identifies orthogonal directions (principal components) in the data along which variance is maximized. It projects high-dimensional data into a lower-dimensional space while preserving as much variability (information) as possible. Mathematically, PCA solves an eigenvalue problem on the covariance (or correlation) matrix of the data. When performing PCA, one must decide whether to apply it to the covariance matrix or the correlation matrix. This decision affects how variables with different scales contribute to the principal components. This post offers a technical explanation with a mathematical derivation to show why the covariance matrix of standardized data equals the correlation matrix of the original data. We’ll back this up with an example in R.\n\n\n\nLet \\(\\mathbf{X} \\in \\mathbb{R}^{n \\times p}\\) be a data matrix with \\(n\\) observations and \\(p\\) variables.\n\n\\(\\mu_j = \\frac{1}{n} \\sum_{i=1}^n X_{ij}\\) is the sample mean of variable \\(j\\)\n\\(\\sigma_j = \\sqrt{\\frac{1}{n-1} \\sum_{i=1}^n (X_{ij} - \\mu_j)^2}\\) is its sample standard deviation\n\nThe centered data matrix is \\(\\tilde{\\mathbf{X}} = \\mathbf{X} - \\mathbf{1}_n \\mu^T\\) where \\(\\mu \\in \\mathbb{R}^p\\) is the vector of column means.\nThe covariance matrix of \\(\\mathbf{X}\\) is defined as:\n\\[\n\\textbf{Cov}(\\mathbf{X}) = \\frac{1}{n-1} \\tilde{\\mathbf{X}}^T \\tilde{\\mathbf{X}}\n\\]\nThe standardized data (i.e., \\(\\mu = 0\\), \\(\\sigma = 1\\)) is given by:\n\\[\nZ_{ij} = \\frac{X_{ij} - \\mu_j}{\\sigma_j} = \\frac{\\tilde{\\mathbf{X}}}{\\sigma_j} \\quad \\text{or} \\quad \\mathbf{Z} = \\tilde{\\mathbf{X}} \\mathbf{D}^{-1}\n\\]\nwhere \\(\\mathbf{D} = \\text{diag}(\\sigma_1, \\dots, \\sigma_p)\\)\nThen,\n\\[\n\\begin{align*} \\textbf{Cov}(\\mathbf{Z}) &= \\frac{1}{n-1} \\mathbf{Z}^T \\mathbf{Z} \\\\\n&= \\frac{1}{n-1} \\left[\\tilde{\\mathbf{X}} \\mathbf{D}^{-1}\\right]^T \\tilde{\\mathbf{X}} \\mathbf{D}^{-1}\\\\\n&= \\mathbf{D}^{-1}\\frac{1}{n-1}\\tilde{\\mathbf{X}}^T \\tilde{\\mathbf{X}} \\mathbf{D}^{-1}\\\\\n&= \\mathbf{D}^{-1} \\textbf{Cov}(\\tilde{\\mathbf{X}}) \\mathbf{D}^{-1}\\\\\n\\end{align*}\n\\]\nNow, to compute the correlation matrix of \\(\\mathbf{X}\\), we normalize each covariance term by the product of the standard deviations of variables \\(i\\) and \\(j\\):\n\\[\n\\begin{align*}\n\\textbf{Cor}(\\mathbf{X})_{ij} &= \\frac{\\textbf{Cov}(\\tilde{\\mathbf{X}})_{ij}}{\\sigma_i\\sigma_j} \\\\ &= \\mathbf{D}^{-1} \\textbf{Cov}(\\tilde{\\mathbf{X}}) \\mathbf{D}^{-1}\n\\end{align*}\n\\]\nThus: \\(\\boxed{\\textbf{Cov}(\\mathbf{Z}) = \\textbf{Cor}(\\mathbf{X})} \\quad \\blacksquare\\)\nThis demonstrates that standardizing the data transforms the covariance matrix into the correlation matrix of the original variables.\nPCA seeks directions (\\(\\mathbf{v}\\)) that maximize the variance of the projected data:\n\\[\n\\max_{\\mathbf{v} \\in \\mathbb{R}^p} \\quad \\mathbf{v}^T \\Sigma \\mathbf{v} \\quad \\text{subject to } \\|\\mathbf{v}\\| = 1\n\\]\nIf \\(\\Sigma\\) is the covariance matrix of \\(\\mathbf{X}\\), this favors variables with large variance. If \\(\\Sigma\\) is the correlation matrix (i.e., the covariance of standardized \\(\\mathbf{X}\\)), all variables contribute equally. Indeed, the fact that the covariance matrix is unbounded (and scale-dependent), while the correlation matrix is bounded, \\([-1, 1]\\), and scale-free, is a fundamental reason why correlation-based PCA is preferred when variables are measured on different scales or units.\n\n\n\nSimulate correlated variables with different scales:\n\nssh = suppressPackageStartupMessages\nssh(library(DT))\nssh(library(tidyverse))\nssh(library(FactoMineR))\n\n\ndata(diamonds, \"ggplot2\")\n\nStandardize the data:\n\nZ &lt;- diamonds |&gt; \n  select(where(is.numeric)) |&gt; \n  scale()\n\nCovariance matrix of the standardized data:\n\ncov_std &lt;- Z |&gt; \n  cov() |&gt;\n  round(3)\n\ncov_std |&gt; datatable()\n\n\n\n\n\nCorrelation matrix of the raw data:\n\ncor_raw &lt;- diamonds |&gt; \n  select(where(is.numeric)) |&gt; \n  cor() |&gt;\n  round(3)\n\ncor_raw |&gt; datatable()\n\n\n\n\n\nAre they equal?\n\nall.equal(cov_std, cor_raw)  # Should return TRUE\n\n[1] TRUE\n\n\nSince the diamonds dataset contains variables with heterogeneous scales, we perform PCA on the correlation matrix by using standardized data \\(Z\\). In the FactoMineR::PCA() function, we set the scale.unit argument to FALSE because our input data has already been standardized.\n\nset.seed(123)\npca_mod &lt;- PCA(Z, scale.unit = FALSE)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe covariance matrix of standardized data is mathematically equal to the correlation matrix of raw data. This equivalence means that applying PCA to standardized data is functionally identical to applying PCA directly to the correlation matrix of the raw data. This distinction is critical because the covariance matrix is scale-dependent and unbounded, which can cause variables with larger numerical ranges to dominate the principal components. In contrast, the correlation matrix is bounded between -1 and 1, ensuring that all variables contribute equally regardless of their original scale. Therefore, you must use correlation-based PCA, when your data consists of variables at different scales."
  },
  {
    "objectID": "blog/posts/post9/index.html#introduction",
    "href": "blog/posts/post9/index.html#introduction",
    "title": "Covariance vs Correlation in PCA: What’s the Difference?",
    "section": "",
    "text": "Principal Component Analysis (PCA) is a linear dimensionality reduction technique that identifies orthogonal directions (principal components) in the data along which variance is maximized. It projects high-dimensional data into a lower-dimensional space while preserving as much variability (information) as possible. Mathematically, PCA solves an eigenvalue problem on the covariance (or correlation) matrix of the data. When performing PCA, one must decide whether to apply it to the covariance matrix or the correlation matrix. This decision affects how variables with different scales contribute to the principal components. This post offers a technical explanation with a mathematical derivation to show why the covariance matrix of standardized data equals the correlation matrix of the original data. We’ll back this up with an example in R."
  },
  {
    "objectID": "blog/posts/post9/index.html#definitions",
    "href": "blog/posts/post9/index.html#definitions",
    "title": "Covariance vs Correlation in PCA: What’s the Difference?",
    "section": "",
    "text": "Let \\(\\mathbf{X} \\in \\mathbb{R}^{n \\times p}\\) be a data matrix with \\(n\\) observations and \\(p\\) variables.\n\n\\(\\mu_j = \\frac{1}{n} \\sum_{i=1}^n X_{ij}\\) is the sample mean of variable \\(j\\)\n\\(\\sigma_j = \\sqrt{\\frac{1}{n-1} \\sum_{i=1}^n (X_{ij} - \\mu_j)^2}\\) is its sample standard deviation\n\nThe centered data matrix is \\(\\tilde{\\mathbf{X}} = \\mathbf{X} - \\mathbf{1}_n \\mu^T\\) where \\(\\mu \\in \\mathbb{R}^p\\) is the vector of column means.\nThe covariance matrix of \\(\\mathbf{X}\\) is defined as:\n\\[\n\\textbf{Cov}(\\mathbf{X}) = \\frac{1}{n-1} \\tilde{\\mathbf{X}}^T \\tilde{\\mathbf{X}}\n\\]\nThe standardized data (i.e., \\(\\mu = 0\\), \\(\\sigma = 1\\)) is given by:\n\\[\nZ_{ij} = \\frac{X_{ij} - \\mu_j}{\\sigma_j} = \\frac{\\tilde{\\mathbf{X}}}{\\sigma_j} \\quad \\text{or} \\quad \\mathbf{Z} = \\tilde{\\mathbf{X}} \\mathbf{D}^{-1}\n\\]\nwhere \\(\\mathbf{D} = \\text{diag}(\\sigma_1, \\dots, \\sigma_p)\\)\nThen,\n\\[\n\\begin{align*} \\textbf{Cov}(\\mathbf{Z}) &= \\frac{1}{n-1} \\mathbf{Z}^T \\mathbf{Z} \\\\\n&= \\frac{1}{n-1} \\left[\\tilde{\\mathbf{X}} \\mathbf{D}^{-1}\\right]^T \\tilde{\\mathbf{X}} \\mathbf{D}^{-1}\\\\\n&= \\mathbf{D}^{-1}\\frac{1}{n-1}\\tilde{\\mathbf{X}}^T \\tilde{\\mathbf{X}} \\mathbf{D}^{-1}\\\\\n&= \\mathbf{D}^{-1} \\textbf{Cov}(\\tilde{\\mathbf{X}}) \\mathbf{D}^{-1}\\\\\n\\end{align*}\n\\]\nNow, to compute the correlation matrix of \\(\\mathbf{X}\\), we normalize each covariance term by the product of the standard deviations of variables \\(i\\) and \\(j\\):\n\\[\n\\begin{align*}\n\\textbf{Cor}(\\mathbf{X})_{ij} &= \\frac{\\textbf{Cov}(\\tilde{\\mathbf{X}})_{ij}}{\\sigma_i\\sigma_j} \\\\ &= \\mathbf{D}^{-1} \\textbf{Cov}(\\tilde{\\mathbf{X}}) \\mathbf{D}^{-1}\n\\end{align*}\n\\]\nThus: \\(\\boxed{\\textbf{Cov}(\\mathbf{Z}) = \\textbf{Cor}(\\mathbf{X})}\\)\nThis demonstrates that standardizing the data transforms the covariance matrix into the correlation matrix of the original variables.\nPCA seeks directions (\\(\\mathbf{v}\\)) that maximize the variance of the projected data:\n\\[\n\\max_{\\mathbf{v} \\in \\mathbb{R}^p} \\quad \\mathbf{v}^T \\Sigma \\mathbf{v} \\quad \\text{subject to } \\|\\mathbf{v}\\| = 1\n\\]\nIf \\(\\Sigma\\) is the covariance matrix of \\(\\mathbf{X}\\), this favors variables with large variance. If \\(\\Sigma\\) is the correlation matrix (i.e., the covariance of standardized \\(\\mathbf{X}\\)), all variables contribute equally in unitless form. Consequently, correlation-based PCA (or standardize data first) is used when variable scales differ."
  },
  {
    "objectID": "blog/posts/post9/index.html#why-this-matters-in-pca",
    "href": "blog/posts/post9/index.html#why-this-matters-in-pca",
    "title": "Covariance vs Correlation in PCA: What’s the Difference?",
    "section": "",
    "text": "PCA seeks directions (\\(\\mathbf{v}\\)) that maximize the variance of the projected data: (\\(\\max\\_{\\mathbf{v} \\in \\mathbb{R}^p} \\quad \\mathbf{v}^T \\Sigma \\mathbf{v} \\quad \\text{subject to } \\|\\mathbf{v}\\| = 1\\))\nIf (\\(\\Sigma\\)) is the covariance matrix of (\\(\\mathbf{X}\\)), this favors variables with large variance (and large units).\nIf (\\(\\Sigma\\)) is the correlation matrix (or covariance of standardized data), all variables contribute equally in unitless form.\nUse correlation-based PCA (or standardize data first) when variable scales differ."
  },
  {
    "objectID": "blog/posts/post9/index.html#r-example",
    "href": "blog/posts/post9/index.html#r-example",
    "title": "Covariance vs Correlation in PCA: What’s the Difference?",
    "section": "",
    "text": "# Simulate correlated variables with different scales\nset.seed(123)\nx1 &lt;- rnorm(100, mean = 50, sd = 10)\nx2 &lt;- 2 * x1 + rnorm(100, mean = 0, sd = 5)\ndata &lt;- data.frame(x1, x2)\n\n# Covariance and correlation of raw data\ncov_raw &lt;- cov(data)\ncor_raw &lt;- cor(data)\n\n# Standardize the data\nZ &lt;- scale(data)\n\n# Covariance of standardized data\ncov_std &lt;- cov(Z)\n\n# Compare\ncat(\"Covariance of standardized data =\\n\")\n\nCovariance of standardized data =\n\nprint(cov_std)\n\n         x1       x2\nx1 1.000000 0.965907\nx2 0.965907 1.000000\n\ncat(\"\\nCorrelation of raw data =\\n\")\n\n\nCorrelation of raw data =\n\nprint(cor_raw)\n\n         x1       x2\nx1 1.000000 0.965907\nx2 0.965907 1.000000\n\n# Are they equal?\nall.equal(cov_std, cor_raw)  # Should return TRUE\n\n[1] TRUE"
  },
  {
    "objectID": "blog/posts/post9/index.html#conclusion",
    "href": "blog/posts/post9/index.html#conclusion",
    "title": "Covariance vs Correlation in PCA: What’s the Difference?",
    "section": "",
    "text": "The covariance matrix of standardized data is mathematically equal to the correlation matrix of raw data. This equivalence means that applying PCA to standardized data is functionally identical to applying PCA directly to the correlation matrix of the raw data. This distinction is critical because the covariance matrix is scale-dependent and unbounded, which can cause variables with larger numerical ranges to dominate the principal components. In contrast, the correlation matrix is bounded between -1 and 1, ensuring that all variables contribute equally regardless of their original scale. Therefore, you must use correlation-based PCA, when your data consists of variables at different scales."
  },
  {
    "objectID": "blog/posts/post9/index.html#example",
    "href": "blog/posts/post9/index.html#example",
    "title": "Covariance vs Correlation in PCA: What’s the Difference?",
    "section": "",
    "text": "Simulate correlated variables with different scales:\n\nssh = suppressPackageStartupMessages\nssh(library(DT))\nssh(library(tidyverse))\nssh(library(FactoMineR))\n\n\ndata(diamonds, \"ggplot2\")\n\nStandardize the data:\n\nZ &lt;- diamonds |&gt; \n  select(where(is.numeric)) |&gt; \n  scale()\n\nCovariance matrix of the standardized data:\n\ncov_std &lt;- Z |&gt; \n  cov() |&gt;\n  round(3)\n\ncov_std |&gt; datatable()\n\n\n\n\n\nCorrelation matrix of the raw data:\n\ncor_raw &lt;- diamonds |&gt; \n  select(where(is.numeric)) |&gt; \n  cor() |&gt;\n  round(3)\n\ncor_raw |&gt; datatable()\n\n\n\n\n\nAre they equal?\n\nall.equal(cov_std, cor_raw)  # Should return TRUE\n\n[1] TRUE\n\n\nSince the diamonds dataset contains variables with heterogeneous scales, we perform PCA on the correlation matrix by using standardized data \\(Z\\). In the FactoMineR::PCA() function, we set the scale.unit argument to FALSE because our input data has already been standardized.\n\nset.seed(123)\npca_mod &lt;- PCA(Z, scale.unit = FALSE)"
  },
  {
    "objectID": "blog/posts/post9/index.html#proof",
    "href": "blog/posts/post9/index.html#proof",
    "title": "Covariance vs Correlation in PCA: What’s the Difference?",
    "section": "",
    "text": "Let \\(\\mathbf{X} \\in \\mathbb{R}^{n \\times p}\\) be a data matrix with \\(n\\) observations and \\(p\\) variables.\n\n\\(\\mu_j = \\frac{1}{n} \\sum_{i=1}^n X_{ij}\\) is the sample mean of variable \\(j\\)\n\\(\\sigma_j = \\sqrt{\\frac{1}{n-1} \\sum_{i=1}^n (X_{ij} - \\mu_j)^2}\\) is its sample standard deviation\n\nThe centered data matrix is \\(\\tilde{\\mathbf{X}} = \\mathbf{X} - \\mathbf{1}_n \\mu^T\\) where \\(\\mu \\in \\mathbb{R}^p\\) is the vector of column means.\nThe covariance matrix of \\(\\mathbf{X}\\) is defined as:\n\\[\n\\textbf{Cov}(\\mathbf{X}) = \\frac{1}{n-1} \\tilde{\\mathbf{X}}^T \\tilde{\\mathbf{X}}\n\\]\nThe standardized data (i.e., \\(\\mu = 0\\), \\(\\sigma = 1\\)) is given by:\n\\[\nZ_{ij} = \\frac{X_{ij} - \\mu_j}{\\sigma_j} = \\frac{\\tilde{\\mathbf{X}}}{\\sigma_j} \\quad \\text{or} \\quad \\mathbf{Z} = \\tilde{\\mathbf{X}} \\mathbf{D}^{-1}\n\\]\nwhere \\(\\mathbf{D} = \\text{diag}(\\sigma_1, \\dots, \\sigma_p)\\)\nThen,\n\\[\n\\begin{align*} \\textbf{Cov}(\\mathbf{Z}) &= \\frac{1}{n-1} \\mathbf{Z}^T \\mathbf{Z} \\\\\n&= \\frac{1}{n-1} \\left[\\tilde{\\mathbf{X}} \\mathbf{D}^{-1}\\right]^T \\tilde{\\mathbf{X}} \\mathbf{D}^{-1}\\\\\n&= \\mathbf{D}^{-1}\\frac{1}{n-1}\\tilde{\\mathbf{X}}^T \\tilde{\\mathbf{X}} \\mathbf{D}^{-1}\\\\\n&= \\mathbf{D}^{-1} \\textbf{Cov}(\\tilde{\\mathbf{X}}) \\mathbf{D}^{-1}\\\\\n\\end{align*}\n\\]\nNow, to compute the correlation matrix of \\(\\mathbf{X}\\), we normalize each covariance term by the product of the standard deviations of variables \\(i\\) and \\(j\\):\n\\[\n\\begin{align*}\n\\textbf{Cor}(\\mathbf{X})_{ij} &= \\frac{\\textbf{Cov}(\\tilde{\\mathbf{X}})_{ij}}{\\sigma_i\\sigma_j} \\\\ &= \\mathbf{D}^{-1} \\textbf{Cov}(\\tilde{\\mathbf{X}}) \\mathbf{D}^{-1}\n\\end{align*}\n\\]\nThus: \\(\\boxed{\\textbf{Cov}(\\mathbf{Z}) = \\textbf{Cor}(\\mathbf{X})} \\quad \\blacksquare\\)\nThis demonstrates that standardizing the data transforms the covariance matrix into the correlation matrix of the original variables.\nPCA seeks directions (\\(\\mathbf{v}\\)) that maximize the variance of the projected data:\n\\[\n\\max_{\\mathbf{v} \\in \\mathbb{R}^p} \\quad \\mathbf{v}^T \\Sigma \\mathbf{v} \\quad \\text{subject to } \\|\\mathbf{v}\\| = 1\n\\]\nIf \\(\\Sigma\\) is the covariance matrix of \\(\\mathbf{X}\\), this favors variables with large variance. If \\(\\Sigma\\) is the correlation matrix (i.e., the covariance of standardized \\(\\mathbf{X}\\)), all variables contribute equally. Indeed, the fact that the covariance matrix is unbounded (and scale-dependent), while the correlation matrix is bounded, \\([-1, 1]\\), and scale-free, is a fundamental reason why correlation-based PCA is preferred when variables are measured on different scales or units."
  },
  {
    "objectID": "blog/posts/post10/index.html#create-synthetic-tensor",
    "href": "blog/posts/post10/index.html#create-synthetic-tensor",
    "title": "Tensor Decomposition Using the PARAFAC2 Algorithm",
    "section": "Create synthetic tensor",
    "text": "Create synthetic tensor\n\n# Set parameters\ntrue_rank = 3\nI, J, K = 30, 40, 20\nnoise_rate = 0.1\nnp.random.seed(0)\n\n# Generate random matrices\nA_factor_matrix = np.random.uniform(1, 2, size=(I, true_rank))\nB_factor_matrix = np.random.uniform(size=(J, true_rank))\nC_factor_matrix = np.random.uniform(size=(K, true_rank))\n\n# Normalised factor matrices\nA_normalised = A_factor_matrix / la.norm(A_factor_matrix, axis=0)\nB_normalised = B_factor_matrix / la.norm(B_factor_matrix, axis=0)\nC_normalised = C_factor_matrix / la.norm(C_factor_matrix, axis=0)\n\n# Generate the shifted factor matrix\nB_factor_matrices = [np.roll(B_factor_matrix, shift=i, axis=0) for i in range(I)]\nBs_normalised = [np.roll(B_normalised, shift=i, axis=0) for i in range(I)]\n\n# Construct the tensor\ntensor = np.einsum(\n    \"ir,ijr,kr-&gt;ijk\", A_factor_matrix, B_factor_matrices, C_factor_matrix\n    )\n\n# Add noise\nnoise = np.random.standard_normal(tensor.shape)\nnoise /= np.linalg.norm(noise)\nnoise *= noise_rate * np.linalg.norm(tensor)\ntensor += noise"
  },
  {
    "objectID": "blog/posts/post10/index.html#fit-a-parafac2-tensor",
    "href": "blog/posts/post10/index.html#fit-a-parafac2-tensor",
    "title": "Tensor Decomposition Using the PARAFAC2 Algorithm",
    "section": "Fit a PARAFAC2 tensor",
    "text": "Fit a PARAFAC2 tensor\n\nbest_err = np.inf\ndecomposition = None\n\nfor run in range(10):\n    trial_decomposition, trial_errs = parafac2(\n        tensor,\n        true_rank,\n        return_errors=True,\n        tol=1e-8,\n        n_iter_max=500,\n        random_state=run,\n    )\n    if best_err &gt; trial_errs[-1]:\n        best_err = trial_errs[-1]\n        err = trial_errs\n        decomposition = trial_decomposition\n\n\nprint(f\"Best model error: {best_err}\")\n\nBest model error: 0.09204695261873093\n\n\n\nest_tensor = tl.parafac2_tensor.parafac2_to_tensor(decomposition)\nest_weights, (est_A, est_B, est_C) = tl.parafac2_tensor.apply_parafac2_projections(\n    decomposition\n    )"
  },
  {
    "objectID": "blog/posts/post10/index.html#compute-performance-metrics",
    "href": "blog/posts/post10/index.html#compute-performance-metrics",
    "title": "Tensor Decomposition Using the PARAFAC2 Algorithm",
    "section": "Compute performance metrics",
    "text": "Compute performance metrics\n\nreconstruction_error = la.norm(est_tensor - tensor)\nrecovery_rate = 1 - reconstruction_error / la.norm(tensor)\n\nprint(\n    f\"{recovery_rate:2.0%} of the data is explained by the model, which is expected with noise rate: {noise_rate}\"\n)\n\n91% of the data is explained by the model, which is expected with noise rate: 0.1\n\n# To evaluate how well the original structure is recovered, we calculate the tucker congruence coefficient.\n\nest_A, est_projected_Bs, est_C = tl.parafac2_tensor.apply_parafac2_projections(\n    decomposition\n)[1]\n\nsign = np.sign(est_A)\nest_A = np.abs(est_A)\nest_projected_Bs = sign[:, np.newaxis] * est_projected_Bs\n\nest_A_normalised = est_A / la.norm(est_A, axis=0)\nest_Bs_normalised = [est_B / la.norm(est_B, axis=0) for est_B in est_projected_Bs]\nest_C_normalised = est_C / la.norm(est_C, axis=0)\n\nB_corr = (\n    np.array(est_Bs_normalised).reshape(-1, true_rank).T\n    @ np.array(Bs_normalised).reshape(-1, true_rank)\n    / len(est_Bs_normalised)\n)\nA_corr = est_A_normalised.T @ A_normalised\nC_corr = est_C_normalised.T @ C_normalised\n\ncorr = A_corr * B_corr * C_corr\npermutation = linear_sum_assignment(\n    -corr\n)  # Old versions of scipy does not support maximising, from scipy v1.4, you can pass `corr` and `maximize=True` instead of `-corr` to maximise the sum.\n\ncongruence_coefficient = np.mean(corr[permutation])\nprint(f\"Average tucker congruence coefficient: {congruence_coefficient}\")\n\nAverage tucker congruence coefficient: 0.9945618721597632"
  },
  {
    "objectID": "blog/posts/post10/index.html#visualize-the-components",
    "href": "blog/posts/post10/index.html#visualize-the-components",
    "title": "Tensor Decomposition Using the PARAFAC2 Algorithm",
    "section": "Visualize the components",
    "text": "Visualize the components\n\n# Find the best permutation so that we can plot the estimated components on top of the true components\npermutation = np.argmax(A_corr * B_corr * C_corr, axis=0)\n\n\n# Create plots of each component vector for each mode\n# (We just look at one of the B_i matrices)\n\nfig, axes = plt.subplots(true_rank, 3, figsize=(15, 3 * true_rank + 1))\ni = 0  # What slice, B_i, we look at for the B mode\n\nfor r in range(true_rank):\n\n    # Plot true and estimated components for mode A\n    axes[r][0].plot((A_normalised[:, r]), label=\"True\")\n    axes[r][0].plot((est_A_normalised[:, permutation[r]]), \"--\", label=\"Estimated\")\n\n    # Labels for the different components\n    axes[r][0].set_ylabel(f\"Component {r}\")\n\n    # Plot true and estimated components for mode C\n    axes[r][2].plot(C_normalised[:, r])\n    axes[r][2].plot(est_C_normalised[:, permutation[r]], \"--\")\n\n    # Plot true components for mode B\n    axes[r][1].plot(Bs_normalised[i][:, r])\n\n    # Get the signs so that we can flip the B mode factor matrices\n    A_sign = np.sign(est_A_normalised)\n\n    # Plot estimated components for mode B (after sign correction)\n    axes[r][1].plot(A_sign[i, r] * est_Bs_normalised[i][:, permutation[r]], \"--\")\n\n# Titles for the different modes\naxes[0][0].set_title(\"A mode\")\naxes[0][2].set_title(\"C mode\")\naxes[0][1].set_title(f\"B mode (slice {i})\")\n\n# Create a legend for the entire figure\nhandles, labels = axes[r][0].get_legend_handles_labels()\nfig.legend(handles, labels, loc=\"upper center\", ncol=2)\n\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "blog/posts/post10/index.html#setup-python-virtual-environment",
    "href": "blog/posts/post10/index.html#setup-python-virtual-environment",
    "title": "Tensor Decomposition Using the PARAFAC2 Algorithm",
    "section": "",
    "text": "library(reticulate)\n\n\n# virtualenv_create(\"r-reticulate\")\n# virtualenv_install(\n#   \"r-reticulate\", \n#   packages = c(\"numpy\", \"pandas\", \"matplotlib\", \"scipy\", \"statsmodels\")\n#   )\n\n\nuse_virtualenv(\"r-reticulate\", required = TRUE)"
  },
  {
    "objectID": "blog/posts/post10/index.html#python-libraries",
    "href": "blog/posts/post10/index.html#python-libraries",
    "title": "Tensor Decomposition Using the PARAFAC2 Algorithm",
    "section": "Python libraries",
    "text": "Python libraries\n\nimport numpy as np\nimport tensorly as tl\nimport numpy.linalg as la\nimport matplotlib.pyplot as plt\nimport tensorly.tenalg as tg\nfrom scipy.optimize import linear_sum_assignment\nfrom tensorly.decomposition import tucker, parafac, parafac2, non_negative_tucker"
  },
  {
    "objectID": "blog/posts/post10/index.html#rd-oder-tensor",
    "href": "blog/posts/post10/index.html#rd-oder-tensor",
    "title": "Tensor Decomposition: PARAFAC, PARAFC2 and Tucker3",
    "section": "3rd-Oder Tensor",
    "text": "3rd-Oder Tensor\nMatrices are of bilinear form, likewise 3-way data are of trilinear form. This defines a mathematical framework which makes it possible to manipulate and study 3-way data through the algebraic and geometric properties of trilinear form. For instance, lets consider \\(T = t_{ijk}\\) and \\(T’= t’_{ijk}\\) two tensors of equal dimension \\(IJK\\), then addition and scalar multiplication are naturally defined as follows \\(T ± T’ = t_{ijk} ± t’_{ijk}\\) and, \\(𝜆T = 𝜆t_{ijk}\\), where \\(\\lambda\\) is a scalar. As with matrices (two-entry arrays) and using these two operations, all three-entry arrays of the same dimension have a finite-dimensional vector space structure of \\(IJK\\) dimension. \n\nMatrix or vector representations\nLet \\(T\\) be an array with three entries, three matrix representations can be associated with this array:\n\n\\(T\\) can be seen as 3 different collections of matrices. These collections are called horizontal, vertical and frontal slices. The matrices of each collection have the same dimension.\n\n\\(T\\) can be considered successively as three matrices called “representations n mode”. The process of reorganizing \\(T\\) in these three forms is called matricization (or mode-\\(k\\) flattening).\n\n\\(T\\) can be seen as three collections of vectors. Each collection is a table with 2 entries, the elements of which are vectors of the same dimension also called “fibers”."
  },
  {
    "objectID": "blog/posts/post10/index.html#tensor-decomposition",
    "href": "blog/posts/post10/index.html#tensor-decomposition",
    "title": "Tensor Decomposition: PARAFAC and Tucker",
    "section": "",
    "text": "PARAFAC (Parallel Factor Analysis) also called Canonical Polyadic (CP) is the simplest and most widely used three-way decomposition. Introduced independently by Harshman and, by Carrol and Chang (who called it Canonical Decomposition or CANDECOMP), PARAFAC represents the most intuitive extension of familiar techniques like Principal Component Analysis (PCA) into the three-dimensional world. PARAFAC is express by:\n\\[\n\\mathbf{X} \\approx \\sum_{r=1}^{R} \\lambda_r \\cdot (\\mathbf{a}_r \\otimes \\mathbf{b}_r \\otimes \\mathbf{c}_r)\n\\]\nThis equation tells us that a 3D tensor \\(\\textbf{X}\\) can be approximated as a sum of \\(R\\) simple components, where each component is the outer product \\((\\otimes)\\) of three vectors: \\(\\textbf{a}_r\\) from mode 1, \\(\\textbf{b}_r\\) from mode 2, and \\(\\textbf{c}_r\\) from mode 3, scaled by weight \\(\\lambda_r\\).\nOne of the defining strengths of PARAFAC is that it applies the same number of components (R) across all modes and, under mild conditions, guarantees a unique decomposition. This property of uniqueness is extremely valuable: unlike many other factorization methods, PARAFAC can recover the “true” underlying factors without ambiguity. As a result, it provides not just a mathematical approximation but an interpretable solution that reflects real-world structure. This makes PARAFAC a powerful tool for scientific discovery, with broad applications in spectroscopy, neuroscience, signal processing, and beyond.\n\n\n\nTucker3 takes a more sophisticated approach, like having an adjustable toolbox instead of a master key. While PARAFAC forces all modes to have the same number of components, Tucker3 allows each mode to have its own optimal number of factors. The Tucker3 decomposition follows this structure:\n\\[\n\\mathbf{X} \\approx \\mathbf{G} \\times_1 \\mathbf{A} \\times_2 \\mathbf{B} \\times_3 \\mathbf{C}\n\\]\nHere, \\(\\textbf{G}\\) is the core tensor that acts as a “weighting grid,” determining how strongly each combination of factors from different modes interacts. The \\(×₁\\), \\(×₂\\), \\(×₃\\) symbols represent tensor contractions. Think of them as sophisticated ways of combining the core tensor with factor matrices \\(\\textbf{A}\\), \\(\\textbf{B}\\), and \\(\\textbf{C}\\).\nUnlike PARAFAC’s simple sum, Tucker3 creates a weighted combination where the core tensor \\(\\textbf{G}\\) contains elements \\(g_{i,j,k}\\) that specify how factors from mode 1 (position \\(i\\)), mode 2 (position \\(j\\)), and mode 3 (position \\(k\\)) combine together. This flexibility allows Tucker3 to capture more complex interaction patterns that PARAFAC might miss."
  },
  {
    "objectID": "blog/posts/post10/index.html#background",
    "href": "blog/posts/post10/index.html#background",
    "title": "Tensor Decomposition: PARAFAC and Tucker",
    "section": "",
    "text": "In many areas of science, like spectroscopy, data are usually stored as vectors or matrices. But in real life, data often have more than two dimensions. For example, measurements that vary across time, space, and different conditions all at once. To work with these kinds of multi-dimensional datasets, we use multi-linear algebra. It’s the higher-dimensional version of linear algebra. Just as a matrix can be broken down with tools like Singular Value Decomposition (SVD) or Eigenvalue Decomposition (EVD) these larger data structures can also be decomposed with special methods. These decompositions help us find patterns that would be hidden if we only looked at two-dimensional data.\nFor example, Excitation-Emission Matrix (EEM) fluorescence spectroscopy and hyperspectral imaging naturally produce three-dimensional data.\n\n\n\n\n\nIn situation where the collected data are not intrinsically 3-ways but rather tidy (\\(i\\) observations × \\(j\\) variables), we can add for instance “time” as a third mode, in order to have three inputs (\\(i\\) observations × \\(j\\) variables × \\(k\\) time). In other words, by adding an additional modality, we obtain a collection of matrices structured in a table with three entries called third-order tensor. Likewise, if we add “days” as the fourth mode into the previous data (\\(i\\) observations × \\(j\\) variables × \\(k\\) time × \\(l\\) days), we obtain a fourth-order tensor. If “longitude” is added (\\(i\\) observations × \\(j\\) variables × \\(k\\) time × \\(l\\) days × \\(m\\) longitude), we obtain a fifth-order tensor, and so forth. Intuitively we can see that an order-\\(N\\) tensor is the number of modes or ways , or \\(N\\)-dimensional arrays. In the following we will only consider data arranged as third-order tensor  \\(i.e.\\), 3-way data.\n\n\n\n\n\n\n\nLet \\(T\\) be an array with three entries, three matrix representations can be associated with this array:\n\\(T\\) can be seen as 3 different collections of matrices. These collections are called horizontal, vertical and frontal slices. The matrices of each collection have the same dimension.\n\n\n\n\n\n\\(T\\) can be considered successively as three matrices called “representations n mode”. The process of reorganizing \\(T\\) in these three forms is called matricization (or mode-\\(k\\) flattening).\n\n\n\n\n\n\\(T\\) can be seen as three collections of vectors. Each collection is a table with 2 entries, the elements of which are vectors of the same dimension also called “fibers”."
  },
  {
    "objectID": "blog/posts/post10/index.html#modeling",
    "href": "blog/posts/post10/index.html#modeling",
    "title": "Tensor Decomposition: PARAFAC and Tucker",
    "section": "",
    "text": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import pearsonr\nimport warnings\nwarnings.filterwarnings('ignore')\n\nFor our tensor decomposition analysis, we rely on TensorLy, a powerful Python library specifically designed for multilinear algebra and tensor operations. TensorLy provides a unified, user-friendly interface for tensor computations while maintaining computational efficiency through optimized backends.\n\nimport tensorly as tl\nfrom tensorly.decomposition import parafac, tucker\n\nWe also specify the computational backend with tl.set_backend('numpy'), which determines how TensorLy performs its underlying mathematical operations. TensorLy supports multiple backends including NumPy, PyTorch, TensorFlow, and JAX, each offering different advantages depending on the application. For more details see here.\n\nnp.random.seed(42)\ntl.set_backend('numpy')\n\n\n\n\nFor this post, we use a fascinating dataset from pioneering linguistic research by Ladefoged et al. (1971), that captures how our tongues move when we speak. The study used X-ray imaging to study tongue shapes as five different speakers pronounced various English vowels. By mapping tongue positions onto a defined grid, they created a unique 3D dataset that reveals the biomechanics of human speech.\nThe tensor has dimensions \\(5 \\times 10 \\times 13\\), representing:\n\n5 speakers (different individuals)\n10 vowels (various English vowel sounds)\n13 grid positions (spatial measurements in centimeters)\n\nThis creates a rich, multi-dimensional dataset perfect for demonstrating tensor decomposition techniques. The original study aimed to identify the fundamental patterns underlying tongue movement during speech. The data was preprocessed by centering across vowels, and the researchers found evidence for two definitive components in the speech patterns, with a possible third component that couldn’t be reliably established. This real-world complexity makes it an excellent example for comparing different tensor decomposition methods like PARAFAC and Tucker, as it contains the kind of structural challenges often found in genuine scientific data. For complete experimental details, see the original research paper.\n\n\nCode\nX = np.array([\n    [2.45, 2.40, 2.40, 2.50, 2.45, 2.05, 1.65, 1.00, 0.45, 0.40, 0.55, 0.55, 0.95],\n    [2.55, 2.05, 1.95, 1.90, 1.80, 1.60, 1.30, 0.95, 0.55, 0.65, 0.90, 0.90, 1.05],\n    [2.35, 1.90, 1.80, 1.80, 1.80, 1.55, 1.30, 0.95, 0.65, 0.70, 0.90, 0.85, 1.05],\n    [2.50, 2.05, 1.65, 1.65, 1.55, 1.45, 1.25, 1.05, 0.70, 1.05, 1.20, 1.15, 1.10],\n    [2.05, 1.50, 1.35, 1.50, 1.55, 1.45, 1.30, 1.15, 0.95, 1.40, 1.55, 1.40, 1.25],\n    [1.55, 1.00, 0.85, 1.00, 1.25, 1.35, 1.50, 2.00, 2.10, 2.55, 2.65, 2.35, 2.00],\n    [1.65, 0.90, 0.65, 0.65, 0.75, 0.95, 1.40, 1.90, 2.15, 2.60, 2.70, 2.60, 2.40],\n    [1.90, 1.30, 0.95, 0.85, 0.75, 0.75, 0.95, 1.30, 1.65, 2.15, 2.30, 2.25, 2.30],\n    [2.40, 1.60, 1.45, 1.25, 1.00, 0.95, 0.80, 0.85, 1.10, 1.50, 2.10, 2.00, 1.65],\n    [2.70, 1.95, 1.50, 1.30, 0.90, 0.70, 0.55, 0.55, 0.95, 1.45, 1.80, 1.90, 2.00],\n    [2.95, 2.70, 2.75, 2.75, 2.70, 2.60, 2.25, 1.00, 0.35, 0.15, 0.30, 0.60, 1.15],\n    [2.40, 2.20, 2.25, 2.20, 2.25, 2.15, 1.85, 1.25, 0.75, 0.75, 0.90, 1.05, 1.10],\n    [2.25, 2.45, 2.65, 2.65, 2.40, 2.20, 2.05, 1.55, 0.95, 0.85, 1.10, 1.40, 1.65],\n    [2.00, 1.75, 1.90, 2.30, 2.40, 2.20, 2.00, 1.45, 1.00, 1.05, 1.40, 1.75, 1.80],\n    [1.25, 1.15, 1.30, 1.65, 1.95, 1.90, 1.80, 1.65, 1.40, 1.70, 2.15, 2.45, 2.60],\n    [0.45, 0.25, 0.30, 0.40, 1.15, 1.70, 1.95, 2.30, 2.60, 2.95, 3.30, 3.15, 2.60],\n    [0.40, 0.20, 0.20, 0.30, 0.60, 1.05, 1.35, 1.65, 2.60, 3.05, 3.45, 3.60, 3.40],\n    [1.00, 0.55, 0.55, 0.45, 0.65, 0.80, 1.15, 1.55, 2.25, 2.75, 3.20, 3.35, 3.25],\n    [1.30, 0.70, 0.65, 0.45, 0.65, 0.90, 1.20, 1.45, 1.90, 2.40, 2.85, 2.80, 2.45],\n    [2.15, 1.80, 1.50, 1.05, 0.65, 0.55, 0.65, 0.80, 0.95, 1.55, 2.10, 2.35, 2.60],\n    [2.10, 2.00, 2.15, 2.05, 1.95, 1.80, 1.45, 1.10, 0.75, 0.65, 0.75, 0.80, 0.90],\n    [2.00, 1.70, 1.90, 1.95, 1.90, 1.75, 1.35, 1.15, 0.95, 1.00, 1.10, 0.90, 0.65],\n    [1.95, 1.80, 1.80, 1.95, 1.95, 1.95, 1.65, 1.25, 0.90, 0.85, 1.05, 0.95, 0.90],\n    [1.55, 1.40, 1.50, 1.70, 1.85, 1.80, 1.90, 1.80, 1.75, 1.70, 1.70, 1.40, 1.10],\n    [1.65, 1.25, 1.40, 1.70, 1.90, 1.95, 2.05, 2.10, 1.95, 1.95, 2.15, 2.10, 1.70],\n    [0.95, 0.55, 0.70, 1.15, 1.65, 2.20, 2.65, 2.95, 3.05, 3.20, 3.35, 2.95, 1.90],\n    [1.20, 0.65, 0.45, 0.65, 0.75, 1.00, 1.45, 2.10, 2.40, 2.65, 2.80, 2.55, 1.95],\n    [1.55, 1.45, 1.05, 1.15, 1.05, 1.00, 1.15, 1.45, 1.90, 2.40, 2.70, 2.65, 1.85],\n    [1.80, 1.05, 1.05, 1.05, 1.00, 1.00, 1.15, 1.40, 1.65, 1.95, 2.15, 1.85, 1.50],\n    [2.00, 1.70, 1.40, 1.20, 1.00, 0.85, 0.95, 1.00, 1.10, 1.55, 1.80, 1.70, 1.25],\n    [2.70, 2.60, 2.55, 2.50, 2.45, 2.40, 1.80, 1.35, 0.70, 0.55, 0.75, 0.85, 1.85],\n    [2.25, 1.90, 1.85, 1.90, 2.15, 2.05, 1.85, 1.65, 1.35, 1.40, 1.50, 1.90, 1.80],\n    [2.25, 2.20, 2.30, 2.25, 2.30, 2.20, 1.70, 1.45, 0.90, 0.90, 1.10, 1.25, 1.85],\n    [1.90, 1.50, 1.40, 1.40, 1.65, 1.75, 1.75, 1.85, 1.60, 1.80, 1.90, 1.65, 1.50],\n    [1.70, 1.20, 1.05, 1.05, 1.55, 1.70, 1.80, 1.90, 1.85, 2.10, 2.35, 2.40, 2.25],\n    [1.05, 0.90, 0.45, 0.60, 1.45, 2.05, 2.90, 2.90, 3.00, 3.20, 3.35, 2.95, 2.15],\n    [0.90, 0.40, 0.45, 0.55, 1.30, 1.80, 2.30, 2.80, 3.10, 3.40, 3.45, 3.00, 2.40],\n    [2.00, 1.30, 1.05, 0.90, 0.95, 0.90, 1.25, 1.65, 1.80, 2.30, 2.60, 2.60, 1.90],\n    [2.15, 1.70, 1.45, 1.30, 1.30, 1.25, 1.20, 1.35, 1.45, 1.95, 2.20, 2.25, 1.95],\n    [2.95, 2.30, 2.05, 1.80, 1.70, 1.45, 1.00, 0.80, 0.80, 1.15, 1.55, 1.90, 1.40],\n    [3.00, 2.45, 2.30, 2.20, 2.10, 1.45, 1.15, 0.80, 0.40, 0.60, 0.45, 0.40, 0.85],\n    [2.40, 2.10, 1.95, 1.90, 1.80, 1.45, 1.10, 0.90, 0.70, 0.95, 0.95, 0.75, 1.10],\n    [2.50, 2.40, 2.20, 2.05, 2.05, 1.70, 1.30, 0.95, 0.65, 0.95, 1.00, 0.85, 1.20],\n    [2.25, 2.10, 1.95, 1.90, 1.90, 1.55, 1.15, 1.00, 0.90, 1.10, 1.05, 0.90, 1.25],\n    [1.70, 1.95, 2.05, 2.10, 1.95, 1.50, 1.15, 1.15, 1.10, 1.30, 1.30, 1.20, 1.45],\n    [1.40, 0.85, 1.05, 1.30, 1.55, 1.55, 1.65, 2.00, 2.40, 2.75, 2.80, 2.60, 2.35],\n    [1.10, 0.70, 0.70, 0.90, 1.15, 1.00, 1.20, 1.80, 2.40, 2.75, 2.80, 2.35, 2.05],\n    [1.80, 1.05, 0.75, 0.70, 0.70, 0.55, 0.60, 1.20, 1.85, 2.40, 2.45, 2.25, 2.40],\n    [1.90, 1.25, 1.05, 0.90, 0.95, 0.65, 0.65, 1.25, 1.85, 2.35, 2.35, 2.05, 2.30],\n    [2.70, 2.05, 1.65, 1.40, 1.15, 0.60, 0.40, 0.50, 0.60, 1.15, 1.40, 1.60, 1.65]\n])\n\n\nLet’s walk through the essential steps to convert our raw data into a proper 3D tensor for analysis. First, we transpose our original data matrix. This rearranges our data so that the spatial measurements (13 grid positions) become the first dimension, which will be important for our tensor structure.\n\nX = X.T\n\nNext, we’re specifying how to organize our data into a 3D structure.\n\n13 = Grid positions (spatial measurements along the tongue)\n10 = Vowel sounds (different English vowels)\n5 = Speakers (different individuals)\n\nFinally, the .reshape() function reorganizes the 637 total elements \\((13 \\times 49)\\) into our desired 3D structure while preserving the original data values. We now have a clean 3D tensor where X_tensor[i, j, k] represents the tongue measurement at grid position i, for vowel j, spoken by speaker k.\n\ntensor_dims = [13, 10, 5]\nX_tensor = X.reshape(tensor_dims)\n\n\n\n\nIn the following, we first proceed with one important step. We systematically test different ranks (complexity levels) to find the best PARAFAC decomposition for our tongue shape data. We test multiple ranks from 2 to 50, asking: “How many components do we need to best represent our data?”\nIn doing so, we follow a systematic decomposition loop for each rank. We decompose the tensor into factor matrices using PARAFAC, then reconstruct the original tensor from these factors. We measure the error between the original and reconstructed data, and store all results for comparison. PARAFAC breaks down our 3D tensor into three factor matrices: a grid positions factor (13 × rank), a vowels factor (10 × rank), and a speakers factor (5 × rank). Think of this as finding the fundamental “building blocks” that, when combined, recreate the original tongue movement patterns.\n\ndef parafac_modeling(X_tensor, max_rank=50):\n    \"\"\"\n    PARAFAC modeling\n    \n    Parameters:\n    -----------\n    X_tensor : numpy.ndarray\n        Input tensor to decompose\n    max_rank : intenger, default=50\n        Maximum rank\n    \n    Returns:\n    --------\n    dict : Results dictionary with PARAFAC decomposition results\n    \"\"\"\n    ranks_to_test = list(range(2, max_rank + 1))\n    results = {}\n    \n    for rank in ranks_to_test:\n        try:\n            factors = parafac(\n              X_tensor, \n              rank=rank, \n              init='random', \n              n_iter_max=200, \n              random_state=42\n              )\n              \n            reconstructed = tl.cp_to_tensor(factors)\n            error = tl.norm(X_tensor - reconstructed)\n            rel_error = error / tl.norm(X_tensor)\n            \n            results[rank] = {\n                'factors': factors,\n                'reconstructed': reconstructed,\n                'error': error,\n                'rel_error': rel_error\n            }\n            \n        except Exception as e:\n            print(f\" Models failed...\")\n            results[rank] = None\n            \n    return results\n\n\n\nCode\ndef optimal_rank(model, criteria='elbow'):\n    \"\"\"\n    Analyze and suggest optimal rank based on different criteria\n    \"\"\"\n    ranks = []\n    rel_errors = []\n    \n    for rank, result in model.items():\n        if result is not None:\n            ranks.append(rank)\n            rel_errors.append(result['rel_error'])\n    \n    ranks = np.array(ranks)\n    rel_errors = np.array(rel_errors)\n    \n    best_rank = ranks[np.argmin(rel_errors)]\n    best_error = np.min(rel_errors)\n    print(f\" Best overall rank: {best_rank} (error: {best_error*100:.3f}%)\")\n    \n    if len(ranks) &gt; 3:\n        first_diff = np.diff(rel_errors)\n        second_diff = np.diff(first_diff)\n        if len(second_diff) &gt; 0:\n            second_diff_ranks = ranks[2:len(second_diff)+2]\n            small_second_diff_mask = np.abs(second_diff) &lt; 0.001\n            if np.any(small_second_diff_mask):\n                elbow_rank = second_diff_ranks[small_second_diff_mask][0]\n                elbow_error = rel_errors[ranks == elbow_rank][0]\n                print(f\" Elbow point: Rank {elbow_rank} (error: {elbow_error*100:.3f}%)\")\n            else:\n                max_reduction_idx = np.argmax(np.abs(first_diff))\n                if max_reduction_idx &lt; len(ranks) - 1:\n                    elbow_rank = ranks[max_reduction_idx + 1]\n                    elbow_error = rel_errors[max_reduction_idx + 1]\n                    print(f\" Largest improvement at: Rank {elbow_rank} (error: {elbow_error*100:.3f}%)\")\n    \n    print(\"\\n Recommendations:\")\n    \n    good_ranks = ranks[rel_errors &lt; 0.05]\n    if len(good_ranks) &gt; 0:\n        practical_rank = good_ranks[0]  # First rank with &lt; 5% error\n        practical_error = rel_errors[ranks == practical_rank][0]\n        print(f\"   • For &lt; 5% error: Rank {practical_rank} (error: {practical_error*100:.3f}%)\")\n    \n    excellent_ranks = ranks[rel_errors &lt; 0.01]\n    if len(excellent_ranks) &gt; 0:\n        excellent_rank = excellent_ranks[0]\n        excellent_error = rel_errors[ranks == excellent_rank][0]\n        print(f\"   • For &lt; 1% error: Rank {excellent_rank} (error: {excellent_error*100:.3f}%)\")\n    \n    tensor_size = np.prod([13, 10, 5])\n    for rank in [5, 10, 15, 20]:\n        if rank in ranks:\n            params = rank * (13 + 10 + 5)\n            compression_ratio = tensor_size / params\n            error = rel_errors[ranks == rank][0]\n            print(f\"   • Rank {rank}: {params} params, {compression_ratio:.1f}x compression, {error*100:.2f}% error\")\n    \n    return ranks, rel_errors\n\n\n\n\nCode\ndef rank_analysis(model, save_fig=False):\n    \"\"\"\n    Rank analysis showing reconstruction error vs rank\n    \"\"\"\n    ranks = []\n    rel_errors = []\n    abs_errors = []\n    \n    for rank, result in model.items():\n        if result is not None:\n            ranks.append(rank)\n            rel_errors.append(result['rel_error'])\n            abs_errors.append(result['error'])\n    \n    ranks = np.array(ranks)\n    rel_errors = np.array(rel_errors)\n    abs_errors = np.array(abs_errors)\n    \n    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n    \n    axes[0, 0].plot(ranks, rel_errors * 100, 'bo-', linewidth=2, markersize=4)\n    axes[0, 0].set_xlabel('Rank')\n    axes[0, 0].set_ylabel('Relative Error (%)')\n    axes[0, 0].set_title('Reconstruction Error vs Rank')\n    axes[0, 0].grid(True, alpha=0.3)\n    axes[0, 0].set_ylim(bottom=0)\n    \n    min_error_idx = np.argmin(rel_errors)\n    axes[0, 0].annotate(\n      f'Min: Rank {ranks[min_error_idx]}\\nError: {rel_errors[min_error_idx]*100:.2f}%',\n      xy=(ranks[min_error_idx], rel_errors[min_error_idx]*100),\n      xytext=(ranks[min_error_idx]+5, rel_errors[min_error_idx]*100+2),\n      arrowprops=dict(arrowstyle='-&gt;', color='red'),\n      fontsize=9, color='red'\n      )\n    \n    axes[0, 1].semilogy(ranks, rel_errors * 100, 'ro-', linewidth=2, markersize=4)\n    axes[0, 1].set_xlabel('Rank')\n    axes[0, 1].set_ylabel('Relative Error (%) [Log Scale]')\n    axes[0, 1].set_title('Error vs Rank (Log Scale)')\n    axes[0, 1].grid(True, alpha=0.3)\n    \n    error_reduction = np.diff(rel_errors) / rel_errors[:-1] * -100\n    axes[1, 0].bar(ranks[1:], error_reduction, alpha=0.7, color='green')\n    axes[1, 0].set_xlabel('Rank')\n    axes[1, 0].set_ylabel('Error Reduction Rate (%)')\n    axes[1, 0].set_title('Error Improvement Rate (Rank i vs Rank i-1)')\n    axes[1, 0].grid(True, alpha=0.3)\n    axes[1, 0].axhline(y=0, color='black', linestyle='-', alpha=0.5)\n    \n    explained_variance = (1 - rel_errors**2) * 100\n    axes[1, 1].plot(ranks, explained_variance, 'mo-', linewidth=2, markersize=4)\n    axes[1, 1].set_xlabel('Rank')\n    axes[1, 1].set_ylabel('Explained Variance (%)')\n    axes[1, 1].set_title('Explained Variance vs Rank')\n    axes[1, 1].grid(True, alpha=0.3)\n    axes[1, 1].set_ylim([0, 100])\n    \n    for threshold in [90, 95, 99]:\n        axes[1, 1].axhline(y=threshold, color='gray', linestyle='--', alpha=0.5)\n        axes[1, 1].text(ranks[-10], threshold+1, f'{threshold}%', fontsize=8, alpha=0.7)\n    plt.tight_layout()\n    \n    if save_fig:\n        plt.savefig('rank_analysis.png', dpi=300, bbox_inches='tight')\n    \n    plt.show()\n    \n    return fig, axes\n\n\nWe obtain four-panel visualization that gives a clear picture of how well different ranks capture tongue shape patterns. The blue curve shows the classic elbow effect: reconstruction error drops quickly between ranks 2–10, then levels off, and finally approaches zero around ranks 40–45. In other words, using more than ~40 ranks lets us reconstruct the data almost perfectly. The log-scale view zooms in on the details, showing that most of the improvement happens early (ranks 2–15), with only small, gradual gains after that. The green bars highlight the “value added” by each rank. Big improvements show up at low ranks (2–10), with a few extra bumps around ranks 20–30. Occasionally, the bars dip below zero, which just means the algorithm stalled briefly due to some numerical hiccups.\nFrom a practical standpoint, choosing a rank between 7–20 is a sweet spot. It captures the meaningful tongue movement patterns without overfitting. Going beyond 40 may look like perfect reconstruction, but in reality, it’s more likely fitting noise than real speech dynamics.\n\nparafac_mod = parafac_modeling(X_tensor, max_rank=50)\nfig, axes = rank_analysis(model=parafac_mod)\n\n\n\n\n\n\n\n\n\nranks, rel_errors = optimal_rank(model=parafac_mod)\n\n Best overall rank: 50 (error: 0.000%)\n Elbow point: Rank 7 (error: 7.084%)\n\n Recommendations:\n   • For &lt; 5% error: Rank 11 (error: 4.580%)\n   • For &lt; 1% error: Rank 24 (error: 0.973%)\n   • Rank 5: 140 params, 4.6x compression, 8.91% error\n   • Rank 10: 280 params, 2.3x compression, 5.27% error\n   • Rank 15: 420 params, 1.5x compression, 2.91% error\n   • Rank 20: 560 params, 1.2x compression, 1.90% error\n\n\n\n\n\nFor comparison, we also applied Tucker3 decomposition to the tongue shape data. Unlike PARAFAC, where a single rank is chosen for all modes, Tucker3 allows the number of components in each mode to be varied independently. We therefore tested different combinations of model sizes, systematically varying the dimensions of the grid positions, vowels, and speakers modes (e.g., [5, 8, 4] and other configurations).\nAs in the PARAFAC modeling, each Tucker3 model was fit by decomposing the tensor into three factor matrices and a core tensor. The factor matrices represent the main axes of variation within each mode, while the core tensor captures how these axes interact across grid positions, vowels, and speakers. After reconstruction, the error was computed and stored for each configuration, enabling a direct comparison to PARAFAC.\n\ndef tucker_modeling(X_tensor, max_rank=[10, 10, 10]):\n    \"\"\"\n    Tucker3 modeling\n    \n    Parameters:\n    -----------\n    X_tensor : numpy.ndarray\n        Input tensor to decompose\n    max_rank : list, default=[4, 4, 3]\n        Maximum rank for each mode [mode1, mode2, mode3]\n    \n    Returns:\n    --------\n    dict : Results dictionary with Tucker decomposition results\n    \"\"\"\n    ranks_to_test = []\n    for r1 in range(2, max_rank[0] + 1):\n        for r2 in range(2, max_rank[1] + 1):\n            for r3 in range(2, max_rank[2] + 1):\n                ranks_to_test.append([r1, r2, r3])\n    \n    results = {}\n    successful_count = 0\n    \n    for rank_combo in ranks_to_test:\n        try:\n            factors = tucker(\n                X_tensor, \n                rank=rank_combo,\n                init='random', \n                n_iter_max=200, \n                random_state=42\n            )\n            \n            reconstructed = tl.tucker_to_tensor(factors)\n            error = tl.norm(X_tensor - reconstructed)\n            rel_error = error / tl.norm(X_tensor)\n\n            core_params = np.prod(rank_combo) \n            factor_params = sum(X_tensor.shape[i] * rank_combo[i] for i in range(3))\n            total_params = core_params + factor_params\n            \n            key = str(rank_combo)\n            results[key] = {\n                'ranks': rank_combo,\n                'factors': factors,\n                'reconstructed': reconstructed,\n                'error': error,\n                'rel_error': rel_error,\n                'core_params': core_params,\n                'factor_params': factor_params,\n                'total_params': total_params\n            }\n            \n            successful_count += 1\n            \n        except Exception as e:\n            print(f\" Models failed - {str(e)[:30]}...\")\n            results[str(rank_combo)] = None\n            \n    return results\n\n\n\nCode\ndef tucker_results(model):\n    \"\"\"Analyze Tucker decomposition results\"\"\"\n    successful_results = {k: v for k, v in model.items() if v is not None}\n    \n    if not successful_results:\n        print(\" No successful Tucker decompositions to analyze\")\n        return\n    \n    best_key = min(successful_results.keys(), key=lambda k:\n      successful_results[k]['rel_error'])\n    \n    best_result = successful_results[best_key]\n    \n    print(f\" Best configuration: {best_result['ranks']}\")\n    print(f\"   - Relative error: {best_result['rel_error']*100:.3f}%\")\n    print(f\"   - Total parameters: {best_result['total_params']}\")\n    print(f\"   - Core parameters: {best_result['core_params']}\")\n    print(f\"   - Factor parameters: {best_result['factor_params']}\")\n    print(f\"\\n Top 5 configurations:\")\n    sorted_results = sorted(successful_results.items(), key=lambda x: x[1]['rel_error'])\n    \n    for i, (key, result) in enumerate(sorted_results[:5]):\n        compression_ratio = np.prod(tensor_dims) / result['total_params']\n        print(\n          f\"   {i+1}. {result['ranks']}: {result['rel_error']*100:.3f}% error, \"\n          f\"{result['total_params']} params, {compression_ratio:.1f}x compression\"\n          )\n    \n    print(f\"\\n Recommendations:\")\n    for threshold in [0.05, 0.01, 0.005]:\n        good_configs = [(k, v) for k, v in successful_results.items() \n                       if v['rel_error'] &lt; threshold]\n        if good_configs:\n            best_config = min(good_configs, key=lambda x: x[1]['total_params'])\n            result = best_config[1]\n            print(\n              f\"   • For &lt;{threshold*100:.1f}% error: {result['ranks']} \"\n              f\"({result['rel_error']*100:.3f}% error, {result['total_params']} params)\"\n              )\n\n\n\n\nCode\ndef plot_tucker_results(model):\n    \"\"\"Create comprehensive Tucker visualization\"\"\"\n    import matplotlib.pyplot as plt\n  \n    successful_results = {k: v for k, v in model.items() if v is not None}\n    \n    if not successful_results:\n        print(\" No successful decompositions to plot\")\n        return\n    \n    configs = []\n    rel_errors = []\n    total_params = []\n    core_params = []\n    factor_params = []\n    compression_ratios = []\n    \n    tensor_size = tensor_dims[0] * tensor_dims[1] * tensor_dims[2]\n    \n    for key, result in successful_results.items():\n        configs.append(key)\n        rel_errors.append(result['rel_error'] * 100)\n        total_params.append(result['total_params'])\n        core_params.append(result['core_params'])\n        factor_params.append(result['factor_params'])\n        compression_ratios.append(tensor_size / result['total_params'])\n    \n    rel_errors = np.array(rel_errors)\n    total_params = np.array(total_params)\n    compression_ratios = np.array(compression_ratios)\n    \n    fig = plt.figure(figsize=(18, 12))\n    \n    ax1 = plt.subplot(2, 3, 1)\n    sorted_indices = np.argsort(rel_errors)\n    top_20 = sorted_indices[:min(20, len(sorted_indices))]\n    \n    bars = plt.bar(range(len(top_20)), rel_errors[top_20], alpha=0.7, color='skyblue')\n    plt.xlabel('Configuration Rank')\n    plt.ylabel('Relative Error (%)')\n    plt.title('Tucker: Top 20 Configurations by Error')\n    plt.xticks(range(len(top_20)), [configs[i] for i in top_20], rotation=45, ha='right')\n    plt.grid(True, alpha=0.3)\n    \n    if len(top_20) &gt; 0:\n        bars[0].set_color('gold')\n        plt.text(0, rel_errors[top_20[0]] + 0.1, 'Best', ha='center', fontweight='bold')\n    \n    ax2 = plt.subplot(2, 3, 2)\n    scatter = plt.scatter(total_params, rel_errors, c=compression_ratios, \n                         cmap='viridis', alpha=0.7, s=60)\n    plt.xlabel('Total Parameters')\n    plt.ylabel('Relative Error (%)')\n    plt.title('Tucker: Error vs Model Complexity')\n    plt.grid(True, alpha=0.3)\n    \n    cbar = plt.colorbar(scatter)\n    cbar.set_label('Compression Ratio')\n    \n    pareto_indices = _find_pareto_front(total_params, rel_errors)\n    plt.plot(\n      total_params[pareto_indices], \n      rel_errors[pareto_indices], \n      'r-', \n      linewidth=2, \n      alpha=0.8, \n      label='Pareto Front'\n      )\n    plt.legend()\n    \n    ax3 = plt.subplot(2, 3, 3)\n    top_10_by_error = sorted_indices[:10]\n    \n    core_params_top = [core_params[i] for i in top_10_by_error]\n    factor_params_top = [factor_params[i] for i in top_10_by_error]\n    configs_top = [configs[i] for i in top_10_by_error]\n    \n    x_pos = range(len(top_10_by_error))\n    p1 = plt.bar(\n      x_pos, \n      core_params_top, \n      alpha=0.8, \n      color='coral', \n      label='Core Parameters'\n      )\n    p2 = plt.bar(\n      x_pos, \n      factor_params_top, \n      bottom=core_params_top, \n      alpha=0.8, \n      color='lightblue', \n      label='Factor Parameters'\n      )\n    \n    plt.xlabel('Configuration')\n    plt.ylabel('Number of Parameters')\n    plt.title('Tucker: Parameter Breakdown (Top 10)')\n    plt.xticks(x_pos, configs_top, rotation=45, ha='right')\n    plt.legend()\n    plt.grid(True, alpha=0.3)\n    \n    ax4 = plt.subplot(2, 3, 4)\n    plt.scatter(compression_ratios, rel_errors, alpha=0.7, color='green', s=60)\n    plt.xlabel('Compression Ratio')\n    plt.ylabel('Relative Error (%)')\n    plt.title('Tucker: Compression vs Accuracy Trade-off')\n    plt.grid(True, alpha=0.3)\n    \n    high_compression = compression_ratios &gt; 1.0\n    if np.any(high_compression):\n        best_compression_idx = np.argmax(compression_ratios[high_compression])\n        best_idx = np.where(high_compression)[0][best_compression_idx]\n        plt.annotate(\n          f'{configs[best_idx]}\\n{compression_ratios[best_idx]:.1f}x',\n          xy=(compression_ratios[best_idx], rel_errors[best_idx]),\n          xytext=(compression_ratios[best_idx]+0.1, rel_errors[best_idx]+1),\n          arrowprops=dict(arrowstyle='-&gt;', color='red'),\n          fontsize=9, \n          color='red'\n          )\n    \n    ax5 = plt.subplot(2, 3, 5)\n    plt.hist(rel_errors, bins=20, alpha=0.7, color='purple', edgecolor='black')\n    plt.xlabel('Relative Error (%)')\n    plt.ylabel('Frequency')\n    plt.title('Tucker: Error Distribution')\n    plt.axvline(\n      np.mean(rel_errors), \n      color='red', \n      linestyle='--', \n      label=f'Mean: {np.mean(rel_errors):.2f}%'\n      )\n    plt.axvline(\n      np.median(rel_errors), \n      color='orange', \n      linestyle='--', \n      label=f'Median: {np.median(rel_errors):.2f}%'\n      )\n    plt.legend()\n    plt.grid(True, alpha=0.3)\n    ax6 = plt.subplot(2, 3, 6)\n\n    if len(successful_results) &gt; 10:\n        rank_data = []\n        for key, result in successful_results.items():\n            r1, r2, r3 = result['ranks']\n            rank_data.append([r1, r2, r3, result['rel_error']])\n        \n        rank_data = np.array(rank_data)\n        unique_r1 = sorted(set(rank_data[:, 0]))\n        unique_r2 = sorted(set(rank_data[:, 1]))\n        \n        if len(unique_r1) &gt; 1 and len(unique_r2) &gt; 1:\n            heatmap_data = np.full((len(unique_r2), len(unique_r1)), np.nan)\n            \n            for i, r2 in enumerate(unique_r2):\n                for j, r1 in enumerate(unique_r1):\n                    mask = (rank_data[:, 0] == r1) & (rank_data[:, 1] == r2)\n                    if np.any(mask):\n                        heatmap_data[i, j] = np.min(rank_data[mask, 3]) * 100\n            \n            im = plt.imshow(heatmap_data, cmap='RdYlBu_r', aspect='auto')\n            plt.colorbar(im, label='Relative Error (%)')\n            plt.xlabel('Mode 1 Rank')\n            plt.ylabel('Mode 2 Rank')\n            plt.title('Tucker: Error Heatmap (Mode 1 vs Mode 2)')\n            plt.xticks(range(len(unique_r1)), unique_r1)\n            plt.yticks(range(len(unique_r2)), unique_r2)\n        else:\n            plt.text(\n              0.5, \n              0.5, \n              'Insufficient data\\nfor heatmap', \n               ha='center', \n               va='center', \n               transform=ax6.transAxes, fontsize=12\n               )\n    else:\n        plt.text(\n          0.5, \n          0.5, \n          'Too few configurations\\nfor heatmap', \n          ha='center', \n          va='center', \n          transform=ax6.transAxes, \n          fontsize=12\n          )\n    \n    plt.tight_layout()\n    plt.suptitle('Tucker Decomposition Analysis', fontsize=16, y=0.98)\n    plt.show()\n\ndef _find_pareto_front(params, errors):\n    \"\"\"Find Pareto front for parameter-error trade-off\"\"\"\n    sorted_indices = np.argsort(params)\n    pareto_indices = []\n    min_error_so_far = float('inf')\n    for idx in sorted_indices:\n        if errors[idx] &lt; min_error_so_far:\n            pareto_indices.append(idx)\n            min_error_so_far = errors[idx]\n    \n    return np.array(pareto_indices)\n    \"\"\"Analyze Tucker decomposition results\"\"\"\n    successful_results = {k: v for k, v in results.items() if v is not None}\n    \n    if not successful_results:\n        print(\" No successful Tucker decompositions to analyze\")\n        return\n    \n    best_key = min(successful_results.keys(), key=lambda k:\n      successful_results[k]['rel_error'])\n    best_result = successful_results[best_key]\n    \n    print(f\" Best configuration: {best_result['ranks']}\")\n    print(f\"   - Relative error: {best_result['rel_error']*100:.3f}%\")\n    print(f\"   - Total parameters: {best_result['total_params']}\")\n    print(f\"   - Core parameters: {best_result['core_params']}\")\n    print(f\"   - Factor parameters: {best_result['factor_params']}\")\n    \n    print(f\"\\n Top 5 configurations:\")\n    sorted_results = sorted(successful_results.items(), key=lambda x:\n      x[1]['rel_error'])\n    \n    for i, (key, result) in enumerate(sorted_results[:5]):\n        compression_ratio = np.prod(tensor_dims) / result['total_params']\n        print(\n          f\"   {i+1}. {result['ranks']}: {result['rel_error']*100:.3f}% error, \"\n          f\"{result['total_params']} params, {compression_ratio:.1f}x compression\"\n          )\n    \n    print(f\"\\n Recommendations:\")\n    for threshold in [0.05, 0.01, 0.005]:\n        good_configs = [(k, v) for k, v in successful_results.items() \n                       if v['rel_error'] &lt; threshold]\n        if good_configs:\n            best_config = min(good_configs, key=lambda x: x[1]['total_params'])\n            result = best_config[1]\n            print(\n              f\"   • For &lt;{threshold*100:.1f}% error: {result['ranks']} \"\n              f\"({result['rel_error']*100:.3f}% error, {result['total_params']} params)\"\n              )\n\n\nLikewise, the six-panel visualization below provides insights into Tucker decomposition performance across different core tensor configurations.\nThe top-left panel showcases the twenty best-performing configurations, with reconstruction errors clustered impressively between 0.5-2.0%. This immediately tells us that Tucker decomposition can achieve excellent accuracy for our tongue shape data when properly configured. The scatter plot of error versus model complexity illustrates the trade-off between accuracy and efficiency. The red Pareto Front marks the “optimal” configurations, where we cannot improve both accuracy and efficiency at the same time. In other words, if we want a lower error, we must accept higher complexity, and vice versa.\nThe stacked bars break down model complexity into two parts: parameters from the core tensor (orange) and those from the factor matrices (blue). This shows where the computational cost comes from in each model. The green dots compare compression ratio against reconstruction error. Configurations above the diagonal line achieve good compression while still keeping the error low, which makes them especially useful in practice.\nFinally, the histogram shows the overall distribution of errors. Most Tucker configurations fall between 8–10% error, with a median of 8.7% and a mean of 9.2%. A few standout models perform much better, achieving far lower error rates. The color-coded grid provides another view, showing how reconstruction error changes across rank combinations for the first two modes. This makes it easier to detect “sweet spots” where the model balances accuracy and efficiency most effectively.\n\ntucker_mod = tucker_modeling(X_tensor)\nplot_tucker_results(model = tucker_mod)\n\n\n\n\n\n\n\n\n\ntucker_results(model = tucker_mod)\n\n Best configuration: [10, 10, 10]\n   - Relative error: 1.348%\n   - Total parameters: 1280\n   - Core parameters: 1000\n   - Factor parameters: 280\n\n Top 5 configurations:\n   1. [10, 10, 10]: 1.348% error, 1280 params, 0.5x compression\n   2. [10, 10, 9]: 1.348% error, 1175 params, 0.6x compression\n   3. [10, 10, 8]: 1.348% error, 1070 params, 0.6x compression\n   4. [10, 10, 5]: 1.348% error, 755 params, 0.9x compression\n   5. [10, 10, 6]: 1.348% error, 860 params, 0.8x compression\n\n Recommendations:\n   • For &lt;5.0% error: [5, 9, 5] (4.703% error, 405 params)\n\n\n\n\n\nAfter optimizing both decomposition methods, we can now directly compare their performance on our tongue shape data. The comparison reveals distinct characteristics and trade-offs between the two approaches.\nThe original tensor slices in the top row provide our baseline reference, showing the complex spatial patterns of tongue positions across different speakers and vowels. Both PARAFAC and Tucker successfully capture the general structure of these patterns, but with notable differences in their approach and results.\nPARAFAC, configured with rank 7, demonstrates its signature strength in providing interpretable factor loadings across all three modes. The factor plots show clear, distinct patterns for each component, with the grid position factors revealing systematic spatial relationships and the vowel factors capturing acoustic-articulatory connections. The reconstruction achieves a 7.08% relative error with just 196 parameters, making it remarkably parameter-efficient. The error map shows relatively uniform reconstruction quality across the tensor space. Meanwhile, Tucker decomposition, using core dimensions [5, 9, 5], takes a fundamentally different approach with its more flexible structure. The factor matrices show more complex patterns, reflecting Tucker’s ability to capture asymmetric relationships between modes. With 405 parameters, Tucker achieves a superior 4.70% relative error, demonstrating the power of its additional flexibility. The core tensor visualization shows the internal structure that Tucker uses to combine these factors, something absent in PARAFAC’s simpler multiplicative model.\nThe direct comparison panels at the bottom quantify these trade-offs clearly. PARAFAC wins on parameter efficiency with 196 versus 405 parameters, translating to better compression ratios. However, Tucker delivers superior reconstruction accuracy with nearly 30% lower relative error. The reconstruction difference heatmap highlights where these methods disagree most strongly, typically in regions with complex multimodal interactions.\n\n\nCode\ndef compare_optimized_models(X_tensor, parafac_rank, tucker_ranks):\n    \"\"\"\n    Compare optimized PARAFAC and Tucker models with comprehensive analysis\n    \n    Parameters:\n    -----------\n    X_tensor : numpy.ndarray\n        Input tensor to decompose\n    parafac_rank : int\n        Optimal rank for PARAFAC\n    tucker_ranks : list\n        Optimal ranks for Tucker [mode1, mode2, mode3]\n    \"\"\"\n    results = {}\n    \n    # PARAFAC Decomposition\n    try:\n        parafac_factors = parafac(\n            X_tensor, \n            rank=parafac_rank, \n            init='random', \n            n_iter_max=200,\n            random_state=42\n        )\n        \n        X_parafac_reconstructed = tl.cp_to_tensor(parafac_factors)\n        parafac_error = tl.norm(X_tensor - X_parafac_reconstructed)\n        parafac_rel_error = parafac_error / tl.norm(X_tensor)\n        parafac_params = parafac_rank * sum(X_tensor.shape)\n        \n        results['PARAFAC'] = {\n            'success': True,\n            'factors': parafac_factors,\n            'reconstructed': X_parafac_reconstructed,\n            'error': parafac_error,\n            'rel_error': parafac_rel_error,\n            'rank': parafac_rank,\n            'params': parafac_params,\n            'method': 'PARAFAC'\n        }\n        \n    except Exception as e:\n        results['PARAFAC'] = {'success': False, 'error': str(e)}\n    \n    # Tucker Decomposition\n    try:\n        tucker_factors = tucker(\n            X_tensor, \n            rank=tucker_ranks, \n            init='random',\n            n_iter_max=200, \n            random_state=42\n        )\n        \n        X_tucker_reconstructed = tl.tucker_to_tensor(tucker_factors)\n        tucker_error = tl.norm(X_tensor - X_tucker_reconstructed)\n        tucker_rel_error = tucker_error / tl.norm(X_tensor)\n        \n        core_params = np.prod(tucker_ranks)\n        factor_params = sum(X_tensor.shape[i] * tucker_ranks[i] for i in range(3))\n        tucker_params = core_params + factor_params\n        \n        results['Tucker'] = {\n            'success': True,\n            'factors': tucker_factors,\n            'reconstructed': X_tucker_reconstructed,\n            'error': tucker_error,\n            'rel_error': tucker_rel_error,\n            'ranks': tucker_ranks,\n            'params': tucker_params,\n            'core_params': core_params,\n            'factor_params': factor_params,\n            'method': 'Tucker'\n        }\n        \n    except Exception as e:\n        results['Tucker'] = {'success': False, 'error': str(e)}\n    \n    _visualization(results, X_tensor)\n    _summary(results)\n    \n    return results\n\ndef _visualization(results, X_tensor):\n    \"\"\"Create comprehensive comparison visualization\"\"\"\n    successful_methods = [method for method, result in results.items() if result['success']]\n    \n    if not successful_methods:\n        return\n    \n    fig = plt.figure(figsize=(20, 14))\n    \n    # Original tensor slices\n    for i in range(min(5, X_tensor.shape[2])):\n        ax = plt.subplot(4, 6, i+1)\n        im = plt.imshow(X_tensor[:, :, i], cmap='viridis', aspect='auto')\n        plt.title(f'Original Tensor\\nSlice {i+1}', fontsize=10)\n        plt.colorbar(im, shrink=0.6)\n        if i == 0:\n            plt.ylabel('Mode 1', fontsize=9)\n        plt.xlabel('Mode 2', fontsize=9)\n    \n    # Tensor statistics\n    ax = plt.subplot(4, 6, 6)\n    ax.text(0.1, 0.8, f'Shape: {X_tensor.shape}', fontsize=12, transform=ax.transAxes)\n    ax.text(0.1, 0.6, f'Elements: {np.prod(X_tensor.shape)}', fontsize=12, transform=ax.transAxes)\n    ax.text(0.1, 0.4, f'Norm: {tl.norm(X_tensor):.3f}', fontsize=12, transform=ax.transAxes)\n    ax.text(0.1, 0.2, f'Range: [{X_tensor.min():.3f}, {X_tensor.max():.3f}]', fontsize=12, transform=ax.transAxes)\n    ax.set_title('Tensor Stats', fontsize=10)\n    ax.axis('off')\n    \n    # PARAFAC Analysis\n    if 'PARAFAC' in successful_methods:\n        result = results['PARAFAC']\n        factors = result['factors'][1]\n        colors = ['blue', 'red', 'green', 'orange']\n        \n        # PARAFAC factor matrices\n        for mode in range(3):\n            ax = plt.subplot(4, 6, 7 + mode)\n            n_comps_show = min(4, factors[mode].shape[1])\n            for comp in range(n_comps_show):\n                plt.plot(factors[mode][:, comp], color=colors[comp], \n                        label=f'Comp {comp+1}', marker='o', markersize=3, linewidth=1.5)\n            plt.title(f'PARAFAC Mode {mode+1}', fontsize=10)\n            plt.xlabel('Index', fontsize=9)\n            plt.ylabel('Factor Value', fontsize=9)\n            if mode == 0:\n                plt.legend(fontsize=7)\n            plt.grid(True, alpha=0.3)\n        \n        # PARAFAC reconstruction\n        ax = plt.subplot(4, 6, 10)\n        recon_slice = result['reconstructed'][:, :, 0]\n        im = plt.imshow(recon_slice, cmap='viridis', aspect='auto')\n        plt.title(f'PARAFAC Recon', fontsize=10)\n        plt.colorbar(im, shrink=0.6)\n        \n        # PARAFAC error\n        ax = plt.subplot(4, 6, 11)\n        error_slice = np.abs(X_tensor[:, :, 0] - recon_slice)\n        im = plt.imshow(error_slice, cmap='Reds', aspect='auto')\n        plt.title(f'PARAFAC Error', fontsize=10)\n        plt.colorbar(im, shrink=0.6)\n        \n        # PARAFAC stats\n        ax = plt.subplot(4, 6, 12)\n        ax.text(0.1, 0.8, f\"Rank: {result['rank']}\", fontsize=12, transform=ax.transAxes)\n        ax.text(0.1, 0.6, f\"Error: {result['rel_error']*100:.3f}%\", fontsize=12, transform=ax.transAxes)\n        ax.text(0.1, 0.4, f\"Params: {result['params']}\", fontsize=12, transform=ax.transAxes)\n        compression = np.prod(X_tensor.shape) / result['params']\n        ax.text(0.1, 0.2, f\"Compression: {compression:.2f}x\", fontsize=12, transform=ax.transAxes)\n        ax.set_title('PARAFAC Stats', fontsize=10)\n        ax.axis('off')\n    \n    # Tucker Analysis\n    if 'Tucker' in successful_methods:\n        result = results['Tucker']\n        factors = result['factors'][1]\n        \n        # Tucker factor matrices\n        for mode in range(3):\n            ax = plt.subplot(4, 6, 13 + mode)\n            n_comps_show = min(4, factors[mode].shape[1])\n            for comp in range(n_comps_show):\n                plt.plot(factors[mode][:, comp], color=colors[comp], \n                        label=f'Comp {comp+1}', marker='s', markersize=3, linewidth=1.5)\n            plt.title(f'Tucker Mode {mode+1}', fontsize=10)\n            plt.xlabel('Index', fontsize=9)\n            plt.ylabel('Factor Value', fontsize=9)\n            if mode == 0:\n                plt.legend(fontsize=7)\n            plt.grid(True, alpha=0.3)\n        \n        # Tucker core tensor\n        ax = plt.subplot(4, 6, 16)\n        core = result['factors'][0]\n        core_slice = core[:, :, 0] if core.ndim == 3 else core\n        im = plt.imshow(core_slice, cmap='RdBu_r', aspect='auto')\n        plt.title('Tucker Core', fontsize=10)\n        plt.colorbar(im, shrink=0.6)\n        \n        # Tucker reconstruction\n        ax = plt.subplot(4, 6, 17)\n        recon_slice = result['reconstructed'][:, :, 0]\n        im = plt.imshow(recon_slice, cmap='viridis', aspect='auto')\n        plt.title(f'Tucker Recon', fontsize=10)\n        plt.colorbar(im, shrink=0.6)\n        \n        # Tucker stats\n        ax = plt.subplot(4, 6, 18)\n        ax.text(0.1, 0.8, f\"Ranks: {result['ranks']}\", fontsize=12, transform=ax.transAxes)\n        ax.text(0.1, 0.6, f\"Error: {result['rel_error']*100:.3f}%\", fontsize=12, transform=ax.transAxes)\n        ax.text(0.1, 0.4, f\"Params: {result['params']}\", fontsize=12, transform=ax.transAxes)\n        compression = np.prod(X_tensor.shape) / result['params']\n        ax.text(0.1, 0.2, f\"Compression: {compression:.2f}x\", fontsize=12, transform=ax.transAxes)\n        ax.set_title('Tucker Stats', fontsize=10)\n        ax.axis('off')\n    \n    # Direct Comparison\n    if len(successful_methods) &gt;= 2:\n        # Error comparison\n        ax = plt.subplot(4, 6, 19)\n        methods = [result['method'] for result in results.values() if result['success']]\n        errors = [result['rel_error'] for result in results.values() if result['success']]\n        colors_comp = ['skyblue', 'lightcoral'][:len(methods)]\n        \n        bars = plt.bar(methods, errors, color=colors_comp, alpha=0.7)\n        plt.title('Error Comparison', fontsize=10)\n        plt.ylabel('Relative Error', fontsize=9)\n        plt.xticks(rotation=45)\n        \n        for bar, error in zip(bars, errors):\n            plt.text(\n                bar.get_x() + bar.get_width()/2, \n                bar.get_height() + 0.002,\n                f'{error:.4f}', \n                ha='center', \n                va='bottom', \n                fontsize=8\n            )\n        \n        # Parameter comparison\n        ax = plt.subplot(4, 6, 20)\n        params = [result['params'] for result in results.values() if result['success']]\n        bars = plt.bar(methods, params, color=colors_comp, alpha=0.7)\n        plt.title('Parameters', fontsize=10)\n        plt.ylabel('# Parameters', fontsize=9)\n        plt.xticks(rotation=45)\n        \n        for bar, param in zip(bars, params):\n            plt.text(\n                bar.get_x() + bar.get_width()/2, \n                bar.get_height() + max(params)*0.02,\n                f'{param}', \n                ha='center', \n                va='bottom', \n                fontsize=8\n            )\n        \n        # Reconstruction difference\n        ax = plt.subplot(4, 6, 21)\n        if len(successful_methods) == 2:\n            parafac_recon = results['PARAFAC']['reconstructed'][:, :, 0] if 'PARAFAC' in results and results['PARAFAC']['success'] else None\n            tucker_recon = results['Tucker']['reconstructed'][:, :, 0] if 'Tucker' in results and results['Tucker']['success'] else None\n            \n            if parafac_recon is not None and tucker_recon is not None:\n                diff = np.abs(parafac_recon - tucker_recon)\n                im = plt.imshow(diff, cmap='plasma', aspect='auto')\n                plt.title('Reconstruction\\nDifference', fontsize=10)\n                plt.colorbar(im, shrink=0.6)\n            else:\n                plt.text(0.5, 0.5, 'Cannot compute\\ndifference', ha='center', va='center', \n                        transform=ax.transAxes, fontsize=10)\n        else:\n            plt.text(0.5, 0.5, 'Need both methods\\nfor comparison', ha='center', va='center', \n                    transform=ax.transAxes, fontsize=10)\n        \n        # Compression comparison\n        ax = plt.subplot(4, 6, 22)\n        compressions = [np.prod(X_tensor.shape) / result['params'] for result in results.values() if result['success']]\n        bars = plt.bar(methods, compressions, color=colors_comp, alpha=0.7)\n        plt.title('Compression Ratio', fontsize=10)\n        plt.ylabel('Compression', fontsize=9)\n        plt.xticks(rotation=45)\n        \n        for bar, comp in zip(bars, compressions):\n            plt.text(\n                bar.get_x() + bar.get_width()/2, \n                bar.get_height() + max(compressions)*0.02,\n                f'{comp:.2f}x', \n                ha='center', \n                va='bottom', \n                fontsize=8\n            )\n        \n        # Explained variance comparison\n        ax = plt.subplot(4, 6, 23)\n        explained_vars = [(1 - result['rel_error']**2) * 100 for result in results.values() if result['success']]\n        bars = plt.bar(methods, explained_vars, color=colors_comp, alpha=0.7)\n        plt.title('Explained Variance', fontsize=10)\n        plt.ylabel('Variance (%)', fontsize=9)\n        plt.xticks(rotation=45)\n        plt.ylim([0, 100])\n        \n        for bar, var in zip(bars, explained_vars):\n            plt.text(\n                bar.get_x() + bar.get_width()/2, \n                bar.get_height() + 2,\n                f'{var:.1f}%', \n                ha='center', \n                va='bottom', \n                fontsize=8\n            )\n    \n    plt.tight_layout()\n    plt.suptitle('PARAFAC vs Tucker: Comparison Results', fontsize=16, y=0.98)\n    plt.show()\n    plt.close()\n\ndef _summary(results):\n    \"\"\"Print minimal final summary\"\"\"\n    successful_methods = [method for method, result in results.items() if result['success']]\n    \n    if successful_methods:\n        best_method = min(successful_methods, key=lambda m: results[m]['rel_error'])\n        best_result = results[best_method]\n        \n        print(f\"Best performing method: {best_method}\")\n        print(f\"  • Relative error: {best_result['rel_error']*100:.3f}%\")\n        print(f\"  • Model parameters: {best_result['params']}\")\n\n\n\nresults = compare_optimized_models(\n    X_tensor, \n    parafac_rank=7, \n    tucker_ranks=[5, 9, 5]\n    )\n\nBest performing method: Tucker\n  • Relative error: 4.703%\n  • Model parameters: 405"
  },
  {
    "objectID": "blog/posts/post10/index.html#conclusions",
    "href": "blog/posts/post10/index.html#conclusions",
    "title": "Tensor Decomposition: PARAFAC, PARAFC2 and Tucker3",
    "section": "Conclusions",
    "text": "Conclusions\nAlthrough, both methods achieve high explained variance (over 95%), but Tucker’s additional parameters allow it to capture subtle patterns that PARAFAC’s constrained structure cannot represent. In other word, PARAFAC offers cleaner, more interpretable results, while Tucker provides superior reconstruction quality for applications where accuracy trumps interpretability.\nThe comparison suggests that for our tongue shape data, both methods successfully identify meaningful patterns, but serve different analytical purposes depending on whether the priority is understanding the underlying structure or achieving the most accurate data representation."
  },
  {
    "objectID": "blog/posts/post10/index.html#conclusion",
    "href": "blog/posts/post10/index.html#conclusion",
    "title": "Tensor Decomposition: PARAFAC and Tucker",
    "section": "",
    "text": "Althrough, both methods achieve high explained variance (over 95%), but Tucker’s additional parameters allow it to capture subtle patterns that PARAFAC’s constrained structure cannot represent. In other word, PARAFAC offers cleaner, more interpretable results, while Tucker provides superior reconstruction quality for applications where accuracy trumps interpretability.\nThe comparison suggests that for our tongue shape data, both methods successfully identify meaningful patterns, but serve different analytical purposes depending on whether the priority is understanding the underlying structure or achieving the most accurate data representation."
  },
  {
    "objectID": "blog/posts/post0/index.html",
    "href": "blog/posts/post0/index.html",
    "title": "Should We Use Standard Error or Standard Deviation?",
    "section": "",
    "text": "Photo by Cody Board.\n\n\n\n\nStandard deviation (SD) tells us about the variability within our sample. It describes how spread out the individual data points are around the sample mean. Think of it as answering: “How much do individual observations typically differ from the average?”. SD is given by:\n\\[\n\\text{SD} = \\frac{\\sum^n_{i=1}(x_i-\\bar{x})^2}{n-1}\n\\]\nWhere, \\(x_i\\) ​is the individual data points, \\(\\bar{x}\\) the sample mean, and \\(n\\) the sample size.\nNotice the \\(n−1\\) in the denominator. This is called Bessel’s correction, and it adjusts for the fact that we’re estimating the population variance from a sample. Using \\(n−1\\) instead of \\(n\\) prevents underestimating variability. In essence:\n\nBessel’s correction compensates for the fact that a sample is typically less variable than the entire population. By using the sample mean, we’ve already “used up” one degree of freedom. It’s as if our effective sample size is reduced from \\(n\\) to \\(n−1\\). That’s why we often say that Bessel’s correction “adjusts the sample size.”\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\nnp.random.seed(42)\npopulation = np.random.normal(loc=0, scale=1, size=1_000_000)\ntrue_sd = np.std(population, ddof=0)\nsample_sizes = np.logspace(1, 4, num=50, dtype=int)  # from 10 to 10,000\n\nsd_n = []\nsd_n1 = []\n\nfor n in sample_sizes:\n    sample = np.random.choice(population, size=n, replace=False)\n    sd_biased = np.sqrt(np.sum((sample - sample.mean())**2) / n)\n    sd_unbiased = np.std(sample, ddof=1)\n    sd_n.append(sd_biased)\n    sd_n1.append(sd_unbiased)\n\nplt.figure(figsize=(8,5))\nplt.axhline(y=true_sd, color=\"black\", linestyle=\"--\", label=\"True population SD\")\nplt.plot(sample_sizes, sd_n, \"r-\", label=\"SD (divide by n)\", markersize=5)\nplt.plot(sample_sizes, sd_n1, \"b-\", label=\"SD (divide by n-1, Bessel)\", markersize=5)\nplt.xscale(\"log\")\nplt.xlabel(\"Sample size (n)\")\nplt.ylabel(\"Estimated SD\")\nplt.title(\"Effect of Bessel's Correction on SD Estimates\")\nplt.legend()\nplt.grid(True, which=\"both\", linestyle=\"--\", alpha=0.6)\nplt.show()\n\n\n\n\n\n\n\n\nThe figure above compares two ways of calculating the sample standard deviation across different sample sizes (\\(n\\) =10 to \\(n\\) =10,000). For small samples, the difference between dividing by \\(n\\) or \\(n−1\\) is large. Without Bessel’s correction, the population variability is underestimated. As the sample size increases, the bias shrinks and the two estimates converge.\nIn practice, we rarely have very large sample size. Using \\(n−1\\) ensures our standard deviation is an unbiased estimator of the population variability.\n\n\n\nStandard error tells us about the precision of our sample mean as an estimate of the population mean. It describes how much our sample mean would vary if we repeated the study many times. It answers:\n\n“If we repeated this study many times, how much would the sample mean vary?”\n\nStandard error is directly related to standard deviation:\n\\[\n\\text{SE} = \\frac{\\text{SD}}{\\sqrt{n}}\n\\]\nWhere \\(n\\) is the sample size. This relationship reveals something important: as your sample size increases, the standard error decreases (your estimate becomes more precise), but the standard deviation might stay roughly the same (the underlying variability in the population doesn’t change).\n\nnp.random.seed(123)\npopulation = np.random.normal(loc=50, scale=10, size=1_000_000)\nsample_sizes = [10, 20, 30, 40, 50, 60, 70, 80, 90, 100]\n\nall_sample_means = []\nall_sample_sizes = []\n\nfor n in sample_sizes:\n    sample_means = [np.mean(np.random.choice(population, size=n, replace=False)) \n                    for _ in range(100)]\n    all_sample_means.extend(sample_means)\n    all_sample_sizes.extend([n] * 100)\n\nall_sample_means = np.array(all_sample_means)\nall_sample_sizes = np.array(all_sample_sizes)\n\nplt.figure(figsize=(10, 8))\n\njitter = np.random.normal(0, 0.3, len(all_sample_sizes))\nx_positions = all_sample_sizes + jitter\nplt.scatter(x_positions, all_sample_means, alpha=0.6, s=20, color='black')\n\ny_min, y_max = plt.ylim()\nfor y in range(int(y_min), int(y_max) + 1, 5):\n    plt.axhline(y=y, color='gray', linestyle='-', alpha=0.3, linewidth=0.5)\n\npopulation_mean = np.mean(population)\nplt.axhline(y=population_mean, color='red', linestyle='--', alpha=0.7, linewidth=2)\n\nplt.xlabel('Sample Size (n)', fontsize=15)\nplt.ylabel('Sample Mean', fontsize=15)\nplt.title('Distribution of Sample Means for Different Sample Sizes', fontsize=18)\nplt.grid(True, alpha=0.3)\nplt.xlim(5, 105);\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nFrom the figure above, we observe several fundamental statistical principles in action. First, all distributions are precisely centered around the true population mean (indicated by the red dashed line), regardless of sample size. This demonstrates that sample means serve as unbiased estimators of the population parameter, a critical property ensuring our estimates are systematically accurate rather than consistently over- or under-estimating the true value.\nMore striking is the dramatic reduction in variability as we move from left to right across increasing sample sizes. The vertical spread of sample means becomes progressively tighter, providing compelling visual evidence that standard error decreases as sample size increases. This relationship follows the mathematical principle SE = σ/√n, meaning that precision improves predictably but at a diminishing rate, doubling the sample size reduces uncertainty by only √2, not by half. Perhaps most remarkably, each vertical column of points approximates a normal distribution, regardless of the original population’s shape. This illustrates the Central Limit Theorem in action: the sampling distribution of means becomes increasingly normal as sample size grows, even when sampling from non-normal populations. This powerful theorem provides the theoretical foundation for much of inferential statistics.\nWe use standard error when we want to communicate uncertainty about population parameters:\n\nConfidence intervals: SE helps calculate how precisely we’ve estimated the population mean\nHypothesis testing: SE is crucial for determining statistical significance\nComparing groups: When we want to know if observed differences likely reflect real population differences\nMeta-analyses: Combining results from multiple studies\n\nStandard error is essential when making inferences beyond our sample to the broader population.\n\n\n\n\nDon’t use SE to make data look less variable: Some researchers inappropriately use standard error instead of standard deviation because SE is always smaller, making their data appear more consistent. This is misleading.\nDon’t use SD for inferential statistics: If you’re testing hypotheses or building confidence intervals, you need standard error, not standard deviation.\nDon’t confuse error bars: When creating graphs, be explicit about whether your error bars represent SD or SE. They tell completely different stories."
  },
  {
    "objectID": "blog/posts/post0/index.html#standard-deviation-how-spread-out-are-our-data",
    "href": "blog/posts/post0/index.html#standard-deviation-how-spread-out-are-our-data",
    "title": "Should We Use Standard Error or Standard Deviation?",
    "section": "",
    "text": "Standard deviation tells us about the variability within our sample. It describes how spread out the individual data points are around the sample mean. Think of it as answering: “How much do individual observations typically differ from the average?”\n\\[\n\\text{SD} = \\frac{\\sum^n_{i=1}(x_i-\\bar{x})^2}{n-1}\n\\]\nWhere, \\(x_i\\) ​is the individual data points, \\(\\bar{x}\\) the sample mean, and n the sample size.\nNotice the \\(n−1\\) in the denominator. This is called Bessel’s correction, and it adjusts for the fact that we’re estimating the population variance from a sample. Using \\(n−1\\) instead of \\(n\\) prevents underestimating variability. In essence:\n\nBessel’s correction compensates for the fact that a sample is typically less variable than the entire population.\n\n\nAlso, by using the sample mean, we’ve already “used up” one degree of freedom. It’s as if our effective sample size is reduced from \\(n\\) to \\(n−1\\). That’s why we often say that Bessel’s correction “adjusts the sample size.”\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\nnp.random.seed(42)\npopulation = np.random.normal(loc=0, scale=1, size=1_000_000)\ntrue_sd = np.std(population, ddof=0)\nsample_sizes = np.logspace(1, 4, num=50, dtype=int)  # from 10 to 10,000\n\nsd_n = []\nsd_n1 = []\n\nfor n in sample_sizes:\n    sample = np.random.choice(population, size=n, replace=False)\n    sd_biased = np.sqrt(np.sum((sample - sample.mean())**2) / n)\n    sd_unbiased = np.std(sample, ddof=1)\n    sd_n.append(sd_biased)\n    sd_n1.append(sd_unbiased)\n\nplt.figure(figsize=(8,5))\nplt.axhline(y=true_sd, color=\"black\", linestyle=\"--\", label=\"True population SD\")\nplt.plot(sample_sizes, sd_n, \"r-\", label=\"SD (divide by n)\", markersize=5)\nplt.plot(sample_sizes, sd_n1, \"b-\", label=\"SD (divide by n-1, Bessel)\", markersize=5)\nplt.xscale(\"log\")\nplt.xlabel(\"Sample size (n)\")\nplt.ylabel(\"Estimated SD\")\nplt.title(\"Effect of Bessel's Correction on SD Estimates\")\nplt.legend()\nplt.grid(True, which=\"both\", linestyle=\"--\", alpha=0.6)\nplt.show()\n\n\n\n\n\n\n\nplt.close()\n\nThe figure above compares two ways of calculating the sample standard deviation across different sample sizes (\\(n\\) =10 to \\(n\\) =10,000). For small samples, the difference between dividing by \\(n\\) or \\(n−1\\) is large. Without Bessel’s correction, the population variability is underestimated. As the sample size increases, the bias shrinks and the two estimates converge.\nIn practice, we rarely have very large sample size. Using \\(n−1\\) ensures our standard deviation is an unbiased estimator of the population variability."
  },
  {
    "objectID": "blog/posts/post0/index.html#standard-error-how-precise-is-our-mean",
    "href": "blog/posts/post0/index.html#standard-error-how-precise-is-our-mean",
    "title": "Should We Use Standard Error or Standard Deviation?",
    "section": "",
    "text": "Standard error tells us about the precision of our sample mean as an estimate of the population mean. It describes how much our sample mean would vary if we repeated the study many times. It answers:\n\n“If we repeated this study many times, how much would the sample mean vary?”\n\nStandard error is directly related to standard deviation:\n\\[\n\\text{SE} = \\frac{\\text{SD}}{\\sqrt{n}}\n\\]\nWhere \\(n\\) is the sample size. This relationship reveals something important: as your sample size increases, the standard error decreases (your estimate becomes more precise), but the standard deviation might stay roughly the same (the underlying variability in the population doesn’t change).\n\nnp.random.seed(123)\npopulation = np.random.normal(loc=50, scale=10, size=1_000_000)\nsample_sizes = [10, 20, 30, 40, 50, 60, 70, 80, 90, 100]\n\nall_sample_means = []\nall_sample_sizes = []\n\nfor n in sample_sizes:\n    sample_means = [np.mean(np.random.choice(population, size=n, replace=False)) \n                    for _ in range(100)]\n    \n    all_sample_means.extend(sample_means)\n    all_sample_sizes.extend([n] * 100)\n\nall_sample_means = np.array(all_sample_means)\nall_sample_sizes = np.array(all_sample_sizes)\n\nplt.figure(figsize=(10, 8))\n\njitter = np.random.normal(0, 0.3, len(all_sample_sizes))\nx_positions = all_sample_sizes + jitter\nplt.scatter(x_positions, all_sample_means, alpha=0.6, s=20, color='black')\n\ny_min, y_max = plt.ylim()\nfor y in range(int(y_min), int(y_max) + 1, 5):\n    plt.axhline(y=y, color='gray', linestyle='-', alpha=0.3, linewidth=0.5)\n\npopulation_mean = np.mean(population)\nplt.axhline(y=population_mean, color='red', linestyle='--', alpha=0.7, linewidth=2)\n\nplt.xlabel('Sample Size (n)')\nplt.ylabel('Sample Mean')\nplt.title('Distribution of Sample Means for Different Sample Sizes')\nplt.grid(True, alpha=0.3)\nplt.xlim(5, 105)\n\n(5.0, 105.0)\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\nplt.close()\n\nFrom the figure above, we observe several fundamental statistical principles in action. First, all distributions are precisely centered around the true population mean (indicated by the red dashed line), regardless of sample size. This demonstrates that sample means serve as unbiased estimators of the population parameter, a critical property ensuring our estimates are systematically accurate rather than consistently over- or under-estimating the true value.\nMore striking is the dramatic reduction in variability as we move from left to right across increasing sample sizes. The vertical spread of sample means becomes progressively tighter, providing compelling visual evidence that standard error decreases as sample size increases. This relationship follows the mathematical principle SE = σ/√n, meaning that precision improves predictably but at a diminishing rate, doubling the sample size reduces uncertainty by only √2, not by half. Perhaps most remarkably, each vertical column of points approximates a normal distribution, regardless of the original population’s shape. This illustrates the Central Limit Theorem in action: the sampling distribution of means becomes increasingly normal as sample size grows, even when sampling from non-normal populations. This powerful theorem provides the theoretical foundation for much of inferential statistics.\nWe use standard error when we want to communicate uncertainty about population parameters:\n\nConfidence intervals: SE helps calculate how precisely we’ve estimated the population mean\nHypothesis testing: SE is crucial for determining statistical significance\nComparing groups: When we want to know if observed differences likely reflect real population differences\nMeta-analyses: Combining results from multiple studies\n\nStandard error is essential when making inferences beyond our sample to the broader population."
  },
  {
    "objectID": "blog/posts/post0/index.html#summary",
    "href": "blog/posts/post0/index.html#summary",
    "title": "Should We Use Standard Error or Standard Deviation?",
    "section": "",
    "text": "Don’t use SE to make data look less variable: Some researchers inappropriately use standard error instead of standard deviation because SE is always smaller, making their data appear more consistent. This is misleading.\nDon’t use SD for inferential statistics: If you’re testing hypotheses or building confidence intervals, you need standard error, not standard deviation.\nDon’t confuse error bars: When creating graphs, be explicit about whether your error bars represent SD or SE. They tell completely different stories."
  },
  {
    "objectID": "blog/posts/post0/index.html#standard-deviation-how-spread-out-is-your-data",
    "href": "blog/posts/post0/index.html#standard-deviation-how-spread-out-is-your-data",
    "title": "Should We Use Standard Error or Standard Deviation?",
    "section": "",
    "text": "Standard deviation (SD) tells us about the variability within our sample. It describes how spread out the individual data points are around the sample mean. Think of it as answering: “How much do individual observations typically differ from the average?”. SD is given by:\n\\[\n\\text{SD} = \\frac{\\sum^n_{i=1}(x_i-\\bar{x})^2}{n-1}\n\\]\nWhere, \\(x_i\\) ​is the individual data points, \\(\\bar{x}\\) the sample mean, and \\(n\\) the sample size.\nNotice the \\(n−1\\) in the denominator. This is called Bessel’s correction, and it adjusts for the fact that we’re estimating the population variance from a sample. Using \\(n−1\\) instead of \\(n\\) prevents underestimating variability. In essence:\n\nBessel’s correction compensates for the fact that a sample is typically less variable than the entire population. By using the sample mean, we’ve already “used up” one degree of freedom. It’s as if our effective sample size is reduced from \\(n\\) to \\(n−1\\). That’s why we often say that Bessel’s correction “adjusts the sample size.”\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\nnp.random.seed(42)\npopulation = np.random.normal(loc=0, scale=1, size=1_000_000)\ntrue_sd = np.std(population, ddof=0)\nsample_sizes = np.logspace(1, 4, num=50, dtype=int)  # from 10 to 10,000\n\nsd_n = []\nsd_n1 = []\n\nfor n in sample_sizes:\n    sample = np.random.choice(population, size=n, replace=False)\n    sd_biased = np.sqrt(np.sum((sample - sample.mean())**2) / n)\n    sd_unbiased = np.std(sample, ddof=1)\n    sd_n.append(sd_biased)\n    sd_n1.append(sd_unbiased)\n\nplt.figure(figsize=(8,5))\nplt.axhline(y=true_sd, color=\"black\", linestyle=\"--\", label=\"True population SD\")\nplt.plot(sample_sizes, sd_n, \"r-\", label=\"SD (divide by n)\", markersize=5)\nplt.plot(sample_sizes, sd_n1, \"b-\", label=\"SD (divide by n-1, Bessel)\", markersize=5)\nplt.xscale(\"log\")\nplt.xlabel(\"Sample size (n)\")\nplt.ylabel(\"Estimated SD\")\nplt.title(\"Effect of Bessel's Correction on SD Estimates\")\nplt.legend()\nplt.grid(True, which=\"both\", linestyle=\"--\", alpha=0.6)\nplt.show()\n\n\n\n\n\n\n\n\nThe figure above compares two ways of calculating the sample standard deviation across different sample sizes (\\(n\\) =10 to \\(n\\) =10,000). For small samples, the difference between dividing by \\(n\\) or \\(n−1\\) is large. Without Bessel’s correction, the population variability is underestimated. As the sample size increases, the bias shrinks and the two estimates converge.\nIn practice, we rarely have very large sample size. Using \\(n−1\\) ensures our standard deviation is an unbiased estimator of the population variability."
  },
  {
    "objectID": "blog/posts/post0/index.html#standard-error-how-precise-is-your-mean",
    "href": "blog/posts/post0/index.html#standard-error-how-precise-is-your-mean",
    "title": "Should We Use Standard Error or Standard Deviation?",
    "section": "",
    "text": "Standard error tells us about the precision of our sample mean as an estimate of the population mean. It describes how much our sample mean would vary if we repeated the study many times. It answers:\n\n“If we repeated this study many times, how much would the sample mean vary?”\n\nStandard error is directly related to standard deviation:\n\\[\n\\text{SE} = \\frac{\\text{SD}}{\\sqrt{n}}\n\\]\nWhere \\(n\\) is the sample size. This relationship reveals something important: as your sample size increases, the standard error decreases (your estimate becomes more precise), but the standard deviation might stay roughly the same (the underlying variability in the population doesn’t change).\n\nnp.random.seed(123)\npopulation = np.random.normal(loc=50, scale=10, size=1_000_000)\nsample_sizes = [10, 20, 30, 40, 50, 60, 70, 80, 90, 100]\n\nall_sample_means = []\nall_sample_sizes = []\n\nfor n in sample_sizes:\n    sample_means = [np.mean(np.random.choice(population, size=n, replace=False)) \n                    for _ in range(100)]\n    all_sample_means.extend(sample_means)\n    all_sample_sizes.extend([n] * 100)\n\nall_sample_means = np.array(all_sample_means)\nall_sample_sizes = np.array(all_sample_sizes)\n\nplt.figure(figsize=(10, 8))\n\njitter = np.random.normal(0, 0.3, len(all_sample_sizes))\nx_positions = all_sample_sizes + jitter\nplt.scatter(x_positions, all_sample_means, alpha=0.6, s=20, color='black')\n\ny_min, y_max = plt.ylim()\nfor y in range(int(y_min), int(y_max) + 1, 5):\n    plt.axhline(y=y, color='gray', linestyle='-', alpha=0.3, linewidth=0.5)\n\npopulation_mean = np.mean(population)\nplt.axhline(y=population_mean, color='red', linestyle='--', alpha=0.7, linewidth=2)\n\nplt.xlabel('Sample Size (n)', fontsize=15)\nplt.ylabel('Sample Mean', fontsize=15)\nplt.title('Distribution of Sample Means for Different Sample Sizes', fontsize=18)\nplt.grid(True, alpha=0.3)\nplt.xlim(5, 105);\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nFrom the figure above, we observe several fundamental statistical principles in action. First, all distributions are precisely centered around the true population mean (indicated by the red dashed line), regardless of sample size. This demonstrates that sample means serve as unbiased estimators of the population parameter, a critical property ensuring our estimates are systematically accurate rather than consistently over- or under-estimating the true value.\nMore striking is the dramatic reduction in variability as we move from left to right across increasing sample sizes. The vertical spread of sample means becomes progressively tighter, providing compelling visual evidence that standard error decreases as sample size increases. This relationship follows the mathematical principle SE = σ/√n, meaning that precision improves predictably but at a diminishing rate, doubling the sample size reduces uncertainty by only √2, not by half. Perhaps most remarkably, each vertical column of points approximates a normal distribution, regardless of the original population’s shape. This illustrates the Central Limit Theorem in action: the sampling distribution of means becomes increasingly normal as sample size grows, even when sampling from non-normal populations. This powerful theorem provides the theoretical foundation for much of inferential statistics.\nWe use standard error when we want to communicate uncertainty about population parameters:\n\nConfidence intervals: SE helps calculate how precisely we’ve estimated the population mean\nHypothesis testing: SE is crucial for determining statistical significance\nComparing groups: When we want to know if observed differences likely reflect real population differences\nMeta-analyses: Combining results from multiple studies\n\nStandard error is essential when making inferences beyond our sample to the broader population."
  },
  {
    "objectID": "blog/posts/post10/index.html#implementation",
    "href": "blog/posts/post10/index.html#implementation",
    "title": "Tensor Decomposition: PARAFAC and Tucker",
    "section": "",
    "text": "In the examples below, we demonstrate a practical implementation of tensor decomposition methods using the Python library TensorLy. In the first example, we built and used a synthetic dataset composed of three Gaussian distribution patterns, illustrating how PARAFAC decomposition can effectively capture and uncovored underlying structures in a noisy multidimensional data. In the second example, we use a published dataset that captures how our tongues move when we speak. Using this dataset, we will compare PARAFAC and Tucker decomposition, highlighting the differences in how each method captures the underlying structures of a complex multidimensional data.\n\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import pearsonr\nfrom mpl_toolkits.mplot3d import Axes3D\nfrom itertools import starmap\nimport math\nimport warnings\nwarnings.filterwarnings('ignore')\n\nFor our tensor decomposition analysis, we rely on TensorLy, a powerful Python library specifically designed for multilinear algebra and tensor operations. TensorLy provides a unified, user-friendly interface for tensor computations while maintaining computational efficiency through optimized backends.\n\nimport tensorly as tl\nfrom tensorly.decomposition import parafac, tucker\nfrom tensorly.cp_tensor import cp_to_tensor\nfrom tensorly.tucker_tensor import tucker_to_tensor\n\nWe also specify the computational backend with tl.set_backend('numpy'), which determines how TensorLy performs its underlying mathematical operations. TensorLy supports multiple backends including NumPy, PyTorch, TensorFlow, and JAX, each offering different advantages depending on the application. For more details see here.\n\nnp.random.seed(123)\ntl.set_backend('numpy')\n\n\n\n\nThe bivariate Gaussian distribution with means \\(\\mu_x,\\mu_y\\), standards deviations \\(\\sigma_x, \\sigma_y\\), and correlation coefficient \\(\\rho\\), is given by:\n\\[\nf(x, y) = \\frac{1}{2\\pi \\sigma_x \\sigma_y \\sqrt{1 - \\rho^2}} \\exp\\left( -\\frac{1}{2(1 - \\rho^2)} \\left[ \\left( \\frac{x - \\mu_x}{\\sigma_x} \\right)^2 - 2\\rho \\left( \\frac{x - \\mu_x}{\\sigma_x} \\right) \\left( \\frac{y - \\mu_y}{\\sigma_y} \\right) + \\left( \\frac{y - \\mu_y}{\\sigma_y} \\right)^2 \\right] \\right)\n\\]\nWe consider the simplified 2D Gaussian distribution when \\(\\rho = 0\\) (i.e., no correlation between \\(x\\) and \\(y\\)):\n\\[\nf(x, y) = \\frac{1}{2\\pi \\sigma_x \\sigma_y} \\exp\\left( - \\frac{1}{2} \\left[ \\left( \\frac{x - \\mu_x}{\\sigma_x} \\right)^2 + \\left( \\frac{y - \\mu_y}{\\sigma_y} \\right)^2 \\right] \\right)\n\\]\n\ndef gaussian_distribution(x, y, mu=(0,0), sigma=(1,1), amplitude=1):\n  alpha = 1 / (2 * math.pi * mu[0] * mu[1])\n  beta_x = ((x - mu[0]) / sigma[0])**2\n  beta_y = ((y - mu[1]) / sigma[1])**2\n  beta = beta_x + beta_y\n  return amplitude * alpha * np.exp(-(1 / 2) * beta)\n\nNow, let’s built three different Gaussian distributions with specific characteristics.\n\nx = np.linspace(-5, 5, 100)\ny = np.linspace(-5, 5, 100)\nX, Y = np.meshgrid(x, y)\n\n# Broad distribution\nbroad = gaussian_distribution(X, Y, (-2, -2), (2, 2), 1)\n\n# Two peaks\npeak1 = gaussian_distribution(X, -Y, (1.5, 1.5), (0.8, 0.8), 1)\npeak2 = gaussian_distribution(-X, Y, (2, 2), (0.8, 0.8), 2)\ntwopeaks = peak1 + peak2 \n\n# Sharp distribution\nsharp = gaussian_distribution(X, Y, (1, 1), (0.2, 0.2), 1.2)\n\nNext, we stack the three distributions into a tensor of dimension \\(100 \\times 100 \\times 3\\).\n\nT = np.stack([broad, twopeaks, sharp], axis=-1)\n\nWe can now visualize the three distributions to verify their shapes and characteristics.\n\nfig = plt.figure(figsize=(15, 5))\n\nfor i in range(3):\n    ax = fig.add_subplot(1, 3, i+1, projection='3d')\n    surf = ax.plot_surface(X, Y, T[:, :, i], cmap='viridis', linewidth=0, antialiased=True)\n    ax.set_xlabel('X-Position')\n    ax.set_ylabel('Y-Position')\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\nplt.close()\n\nFor the purpose of this post, we add some noise into the data at three distinct levels: 0.001, 0.01, and 0.05.\n\nnoisy1_T = T + 0.001 * np.random.randn(*T.shape)\nnoisy2_T = T + 0.01 * np.random.randn(*T.shape)\nnoisy3_T = T + 0.05 * np.random.randn(*T.shape)\n\nThen, we create a list of tensors to decompose, e.g., [noisy1_T, noisy2_T, noisy3_T].\n\ndatasets = [noisy1_T, noisy2_T, noisy3_T]\nnoise_levels = [0.001, 0.01, 0.05]\ntitles = ['Broad', 'Two-peak', 'Sharp']\n\n\ndef plot_tensor(data, noise):\n    fig = plt.figure(figsize=(15, 5))\n    fig.suptitle(f\"Noise level = {noise}\", fontsize=16)\n    \n    for i in range(3):\n        ax = fig.add_subplot(1, 3, i+1, projection='3d')\n        surf = ax.plot_surface(X, Y, data[:, :, i], cmap='viridis', linewidth=0, antialiased=True)\n        ax.set_xlabel('X-Position')\n        ax.set_ylabel('Y-Position')\n\n    plt.tight_layout()\n    plt.show()\n    plt.close()\n\n\nfor _ in map(plot_tensor, datasets, noise_levels):\n    pass\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPARAFAC decomposition is applied iteratively to the list of tensors. For each tensor in datasets, the method extracts three components, producing a set of weights and factor matrices that capture the underlying patterns along each mode:\n\nMode 0 factors: describe the pattern along the rows (X-axis) of the tensor.\nMode 1 factors: describe the pattern along the columns (Y-axis) of the tensor.\nMode 2 factors: correspond to the the different Gaussian distributions.\n\nAll of these results are collected and stored in a list called results, for further analysis and visualization.\n\nresults = list(map(lambda data: parafac(data, 3, init='random', random_state=42), datasets))\n\nWe then reconstruct the tensor from the factorization results and evaluate the quality of the decomposition by calculating the relative reconstruction error. The reconstructed tensor is obtained using the factor matrices and weights returned by the PARAFAC model. The error is computed as the ratio between the Frobenius norm, \\(\\lVert{\\cdot}\\rVert_F\\), of the difference (original tensor minus reconstructed tensor) and the Frobenius norm of the original tensor. This provides a normalized measure of how well the decomposition approximates the data, with lower values indicating a more accurate reconstruction.\n\\[\n\\text{Relative Reconstruction Error} = \\frac{\\lVert\\mathcal{T} - \\hat{\\mathcal{T}}\\rVert_F}{\\lVert \\mathcal{T}\\rVert_F}\n\\]\nWhere \\(\\mathcal{T}\\) is the original tensor and \\(\\lVert \\mathcal{\\hat{T}} \\rVert_F\\) is the reconstructed tensor.\n\ndef compute_error(result, data):\n  weights, factors = result\n  reconstructed_T = cp_to_tensor((weights, factors))\n  return np.linalg.norm(data - reconstructed_T) / np.linalg.norm(data)\n\n\nerrors = list(map(compute_error, results, datasets))\n\n\nfor noise, err in zip(noise_levels, errors):\n    print(f\"Noise level = {noise}, Relative reconstruction error = {err:.4f}\")\n\nNoise level = 0.001, Relative reconstruction error = 0.3160\nNoise level = 0.01, Relative reconstruction error = 0.6662\nNoise level = 0.05, Relative reconstruction error = 0.9623\n\n\nNext, we visualize the decomposed modes obtained from the tensor factorization. Each of the three component represents a distinct pattern captured along a specific mode of the data, and plotting them allows us to interpret the underlying structures identified by the decomposition.\n\ntitles = ['Component 1', 'Component 2', 'Component 3']\n\n\ndef plot_components(result, noise):\n    weights, factors = result\n    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n    fig.suptitle(f\"Noise level = {noise}\", fontsize=16)\n\n    for i, ax in enumerate(axes):\n        component = np.outer(factors[0][:, i], factors[1][:, i])\n        component /= np.max(np.abs(component))\n        line_y = component.sum(axis=0)\n        line_x = component.sum(axis=1)\n        ax.plot(line_y, label='Mode 1 (Y-position)')\n        ax.plot(line_x, label='Mode 0 (X-position)')\n        ax.set_title(titles[i])\n        ax.set_xlabel('Index')\n        ax.set_ylabel('Amplitude (normalized)')\n        ax.legend()\n\n    plt.tight_layout()\n    plt.show()\n    plt.close()\n\n\nfor _ in map(plot_components, results, noise_levels):\n    pass\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntitles = ['Broad', 'Two-peak', 'Sharp']\n\n\ndef plot_reconstruction_error(noisy, reconstructed, noise_level):\n    error_tensor = noisy - reconstructed\n    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n    fig.suptitle(f'Reconstruction Error - Noise level = {noise_level}', fontsize=16)\n    for i, ax in enumerate(axes):\n        im = ax.imshow(error_tensor[:, :, i], cmap='RdBu', aspect='auto')\n        ax.set_title(titles[i])\n        fig.colorbar(im, ax=ax, label='Error')\n\n    plt.tight_layout()\n    plt.show()\n    plt.close()\n\n\nreconstructed_tensors = [cp_to_tensor(res) for res in results]\nfor _ in map(plot_reconstruction_error, datasets, reconstructed_tensors, noise_levels):\n    pass\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIn this example, we use a fascinating dataset from pioneering linguistic research by Ladefoged et al. (1971), that captures how our tongues move when we speak. The study used X-ray imaging to study tongue shapes as five different speakers pronounced various English vowels. By mapping tongue positions onto a defined grid, they created a unique 3D dataset that reveals the biomechanics of human speech.\nThe tensor has dimensions \\(5 \\times 10 \\times 13\\), representing:\n\n5 speakers (different individuals)\n10 vowels (various English vowel sounds)\n13 grid positions (spatial measurements in centimeters)\n\nThis creates a rich, multi-dimensional dataset perfect for demonstrating tensor decomposition techniques. The original study aimed to identify the fundamental patterns underlying tongue movement during speech. The data was preprocessed by centering across vowels, and the researchers found evidence for two definitive components in the speech patterns, with a possible third component that couldn’t be reliably established. This real-world complexity makes it an excellent example for comparing different tensor decomposition methods like PARAFAC and Tucker, as it contains the kind of structural challenges often found in genuine scientific data. For complete experimental details, see the original research paper.\n\n\nCode\nX = np.array([\n    [2.45, 2.40, 2.40, 2.50, 2.45, 2.05, 1.65, 1.00, 0.45, 0.40, 0.55, 0.55, 0.95],\n    [2.55, 2.05, 1.95, 1.90, 1.80, 1.60, 1.30, 0.95, 0.55, 0.65, 0.90, 0.90, 1.05],\n    [2.35, 1.90, 1.80, 1.80, 1.80, 1.55, 1.30, 0.95, 0.65, 0.70, 0.90, 0.85, 1.05],\n    [2.50, 2.05, 1.65, 1.65, 1.55, 1.45, 1.25, 1.05, 0.70, 1.05, 1.20, 1.15, 1.10],\n    [2.05, 1.50, 1.35, 1.50, 1.55, 1.45, 1.30, 1.15, 0.95, 1.40, 1.55, 1.40, 1.25],\n    [1.55, 1.00, 0.85, 1.00, 1.25, 1.35, 1.50, 2.00, 2.10, 2.55, 2.65, 2.35, 2.00],\n    [1.65, 0.90, 0.65, 0.65, 0.75, 0.95, 1.40, 1.90, 2.15, 2.60, 2.70, 2.60, 2.40],\n    [1.90, 1.30, 0.95, 0.85, 0.75, 0.75, 0.95, 1.30, 1.65, 2.15, 2.30, 2.25, 2.30],\n    [2.40, 1.60, 1.45, 1.25, 1.00, 0.95, 0.80, 0.85, 1.10, 1.50, 2.10, 2.00, 1.65],\n    [2.70, 1.95, 1.50, 1.30, 0.90, 0.70, 0.55, 0.55, 0.95, 1.45, 1.80, 1.90, 2.00],\n    [2.95, 2.70, 2.75, 2.75, 2.70, 2.60, 2.25, 1.00, 0.35, 0.15, 0.30, 0.60, 1.15],\n    [2.40, 2.20, 2.25, 2.20, 2.25, 2.15, 1.85, 1.25, 0.75, 0.75, 0.90, 1.05, 1.10],\n    [2.25, 2.45, 2.65, 2.65, 2.40, 2.20, 2.05, 1.55, 0.95, 0.85, 1.10, 1.40, 1.65],\n    [2.00, 1.75, 1.90, 2.30, 2.40, 2.20, 2.00, 1.45, 1.00, 1.05, 1.40, 1.75, 1.80],\n    [1.25, 1.15, 1.30, 1.65, 1.95, 1.90, 1.80, 1.65, 1.40, 1.70, 2.15, 2.45, 2.60],\n    [0.45, 0.25, 0.30, 0.40, 1.15, 1.70, 1.95, 2.30, 2.60, 2.95, 3.30, 3.15, 2.60],\n    [0.40, 0.20, 0.20, 0.30, 0.60, 1.05, 1.35, 1.65, 2.60, 3.05, 3.45, 3.60, 3.40],\n    [1.00, 0.55, 0.55, 0.45, 0.65, 0.80, 1.15, 1.55, 2.25, 2.75, 3.20, 3.35, 3.25],\n    [1.30, 0.70, 0.65, 0.45, 0.65, 0.90, 1.20, 1.45, 1.90, 2.40, 2.85, 2.80, 2.45],\n    [2.15, 1.80, 1.50, 1.05, 0.65, 0.55, 0.65, 0.80, 0.95, 1.55, 2.10, 2.35, 2.60],\n    [2.10, 2.00, 2.15, 2.05, 1.95, 1.80, 1.45, 1.10, 0.75, 0.65, 0.75, 0.80, 0.90],\n    [2.00, 1.70, 1.90, 1.95, 1.90, 1.75, 1.35, 1.15, 0.95, 1.00, 1.10, 0.90, 0.65],\n    [1.95, 1.80, 1.80, 1.95, 1.95, 1.95, 1.65, 1.25, 0.90, 0.85, 1.05, 0.95, 0.90],\n    [1.55, 1.40, 1.50, 1.70, 1.85, 1.80, 1.90, 1.80, 1.75, 1.70, 1.70, 1.40, 1.10],\n    [1.65, 1.25, 1.40, 1.70, 1.90, 1.95, 2.05, 2.10, 1.95, 1.95, 2.15, 2.10, 1.70],\n    [0.95, 0.55, 0.70, 1.15, 1.65, 2.20, 2.65, 2.95, 3.05, 3.20, 3.35, 2.95, 1.90],\n    [1.20, 0.65, 0.45, 0.65, 0.75, 1.00, 1.45, 2.10, 2.40, 2.65, 2.80, 2.55, 1.95],\n    [1.55, 1.45, 1.05, 1.15, 1.05, 1.00, 1.15, 1.45, 1.90, 2.40, 2.70, 2.65, 1.85],\n    [1.80, 1.05, 1.05, 1.05, 1.00, 1.00, 1.15, 1.40, 1.65, 1.95, 2.15, 1.85, 1.50],\n    [2.00, 1.70, 1.40, 1.20, 1.00, 0.85, 0.95, 1.00, 1.10, 1.55, 1.80, 1.70, 1.25],\n    [2.70, 2.60, 2.55, 2.50, 2.45, 2.40, 1.80, 1.35, 0.70, 0.55, 0.75, 0.85, 1.85],\n    [2.25, 1.90, 1.85, 1.90, 2.15, 2.05, 1.85, 1.65, 1.35, 1.40, 1.50, 1.90, 1.80],\n    [2.25, 2.20, 2.30, 2.25, 2.30, 2.20, 1.70, 1.45, 0.90, 0.90, 1.10, 1.25, 1.85],\n    [1.90, 1.50, 1.40, 1.40, 1.65, 1.75, 1.75, 1.85, 1.60, 1.80, 1.90, 1.65, 1.50],\n    [1.70, 1.20, 1.05, 1.05, 1.55, 1.70, 1.80, 1.90, 1.85, 2.10, 2.35, 2.40, 2.25],\n    [1.05, 0.90, 0.45, 0.60, 1.45, 2.05, 2.90, 2.90, 3.00, 3.20, 3.35, 2.95, 2.15],\n    [0.90, 0.40, 0.45, 0.55, 1.30, 1.80, 2.30, 2.80, 3.10, 3.40, 3.45, 3.00, 2.40],\n    [2.00, 1.30, 1.05, 0.90, 0.95, 0.90, 1.25, 1.65, 1.80, 2.30, 2.60, 2.60, 1.90],\n    [2.15, 1.70, 1.45, 1.30, 1.30, 1.25, 1.20, 1.35, 1.45, 1.95, 2.20, 2.25, 1.95],\n    [2.95, 2.30, 2.05, 1.80, 1.70, 1.45, 1.00, 0.80, 0.80, 1.15, 1.55, 1.90, 1.40],\n    [3.00, 2.45, 2.30, 2.20, 2.10, 1.45, 1.15, 0.80, 0.40, 0.60, 0.45, 0.40, 0.85],\n    [2.40, 2.10, 1.95, 1.90, 1.80, 1.45, 1.10, 0.90, 0.70, 0.95, 0.95, 0.75, 1.10],\n    [2.50, 2.40, 2.20, 2.05, 2.05, 1.70, 1.30, 0.95, 0.65, 0.95, 1.00, 0.85, 1.20],\n    [2.25, 2.10, 1.95, 1.90, 1.90, 1.55, 1.15, 1.00, 0.90, 1.10, 1.05, 0.90, 1.25],\n    [1.70, 1.95, 2.05, 2.10, 1.95, 1.50, 1.15, 1.15, 1.10, 1.30, 1.30, 1.20, 1.45],\n    [1.40, 0.85, 1.05, 1.30, 1.55, 1.55, 1.65, 2.00, 2.40, 2.75, 2.80, 2.60, 2.35],\n    [1.10, 0.70, 0.70, 0.90, 1.15, 1.00, 1.20, 1.80, 2.40, 2.75, 2.80, 2.35, 2.05],\n    [1.80, 1.05, 0.75, 0.70, 0.70, 0.55, 0.60, 1.20, 1.85, 2.40, 2.45, 2.25, 2.40],\n    [1.90, 1.25, 1.05, 0.90, 0.95, 0.65, 0.65, 1.25, 1.85, 2.35, 2.35, 2.05, 2.30],\n    [2.70, 2.05, 1.65, 1.40, 1.15, 0.60, 0.40, 0.50, 0.60, 1.15, 1.40, 1.60, 1.65]\n])\n\n\nLet’s walk through the essential steps to convert our raw data into a proper 3D tensor for analysis. First, we transpose our original data matrix. This rearranges our data so that the spatial measurements (13 grid positions) become the first dimension, which will be important for our tensor structure.\n\nX = X.T\n\nNext, we’re specifying how to organize our data into a 3D structure.\n\n13 = Grid positions (spatial measurements along the tongue)\n10 = Vowel sounds (different English vowels)\n5 = Speakers (different individuals)\n\nFinally, the .reshape() function reorganizes the 637 total elements \\((13 \\times 49)\\) into our desired 3D structure while preserving the original data values. We now have a clean 3D tensor where X_tensor[i, j, k] represents the tongue measurement at grid position i, for vowel j, spoken by speaker k.\n\ntensor_dims = [13, 10, 5]\nX_tensor = X.reshape(tensor_dims)\n\n\n\nIn the following, we first proceed with one important step. We systematically test different ranks (complexity levels) to find the best PARAFAC decomposition for our tongue shape data. We test multiple ranks from 2 to 50, asking: “How many components do we need to best represent our data?”\nIn doing so, we follow a systematic decomposition loop for each rank. We decompose the tensor into factor matrices using PARAFAC, then reconstruct the original tensor from these factors. We measure the error between the original and reconstructed data, and store all results for comparison. PARAFAC breaks down our 3D tensor into three factor matrices: a grid positions factor (13 × rank), a vowels factor (10 × rank), and a speakers factor (5 × rank). Think of this as finding the fundamental “building blocks” that, when combined, recreate the original tongue movement patterns.\n\ndef parafac_modeling(X_tensor, max_rank=50):\n    \"\"\"\n    PARAFAC modeling\n    \n    Parameters:\n    -----------\n    X_tensor : numpy.ndarray\n        Input tensor to decompose\n    max_rank : intenger, default=50\n        Maximum rank\n    \n    Returns:\n    --------\n    dict : Results dictionary with PARAFAC decomposition results\n    \"\"\"\n    ranks_to_test = list(range(2, max_rank + 1))\n    results = {}\n    \n    for rank in ranks_to_test:\n        try:\n            factors = parafac(\n              X_tensor, \n              rank=rank, \n              init='random', \n              n_iter_max=200, \n              random_state=42\n              )\n              \n            reconstructed = tl.cp_to_tensor(factors)\n            error = tl.norm(X_tensor - reconstructed)\n            rel_error = error / tl.norm(X_tensor)\n            \n            results[rank] = {\n                'factors': factors,\n                'reconstructed': reconstructed,\n                'error': error,\n                'rel_error': rel_error\n            }\n            \n        except Exception as e:\n            print(f\" Models failed...\")\n            results[rank] = None\n            \n    return results\n\n\n\nCode\ndef optimal_rank(model, criteria='elbow'):\n    \"\"\"\n    Analyze and suggest optimal rank based on different criteria\n    \"\"\"\n    ranks = []\n    rel_errors = []\n    \n    for rank, result in model.items():\n        if result is not None:\n            ranks.append(rank)\n            rel_errors.append(result['rel_error'])\n    \n    ranks = np.array(ranks)\n    rel_errors = np.array(rel_errors)\n    \n    best_rank = ranks[np.argmin(rel_errors)]\n    best_error = np.min(rel_errors)\n    print(f\" Best overall rank: {best_rank} (error: {best_error*100:.3f}%)\")\n    \n    if len(ranks) &gt; 3:\n        first_diff = np.diff(rel_errors)\n        second_diff = np.diff(first_diff)\n        if len(second_diff) &gt; 0:\n            second_diff_ranks = ranks[2:len(second_diff)+2]\n            small_second_diff_mask = np.abs(second_diff) &lt; 0.001\n            if np.any(small_second_diff_mask):\n                elbow_rank = second_diff_ranks[small_second_diff_mask][0]\n                elbow_error = rel_errors[ranks == elbow_rank][0]\n                print(f\" Elbow point: Rank {elbow_rank} (error: {elbow_error*100:.3f}%)\")\n            else:\n                max_reduction_idx = np.argmax(np.abs(first_diff))\n                if max_reduction_idx &lt; len(ranks) - 1:\n                    elbow_rank = ranks[max_reduction_idx + 1]\n                    elbow_error = rel_errors[max_reduction_idx + 1]\n                    print(f\" Largest improvement at: Rank {elbow_rank} (error: {elbow_error*100:.3f}%)\")\n    \n    print(\"\\n Recommendations:\")\n    \n    good_ranks = ranks[rel_errors &lt; 0.05]\n    if len(good_ranks) &gt; 0:\n        practical_rank = good_ranks[0]  # First rank with &lt; 5% error\n        practical_error = rel_errors[ranks == practical_rank][0]\n        print(f\"   • For &lt; 5% error: Rank {practical_rank} (error: {practical_error*100:.3f}%)\")\n    \n    excellent_ranks = ranks[rel_errors &lt; 0.01]\n    if len(excellent_ranks) &gt; 0:\n        excellent_rank = excellent_ranks[0]\n        excellent_error = rel_errors[ranks == excellent_rank][0]\n        print(f\"   • For &lt; 1% error: Rank {excellent_rank} (error: {excellent_error*100:.3f}%)\")\n    \n    tensor_size = np.prod([13, 10, 5])\n    for rank in [5, 10, 15, 20]:\n        if rank in ranks:\n            params = rank * (13 + 10 + 5)\n            compression_ratio = tensor_size / params\n            error = rel_errors[ranks == rank][0]\n            print(f\"   • Rank {rank}: {params} params, {compression_ratio:.1f}x compression, {error*100:.2f}% error\")\n    \n    return ranks, rel_errors\n\n\n\n\nCode\ndef rank_analysis(model, save_fig=False):\n    \"\"\"\n    Rank analysis showing reconstruction error vs rank\n    \"\"\"\n    ranks = []\n    rel_errors = []\n    abs_errors = []\n    \n    for rank, result in model.items():\n        if result is not None:\n            ranks.append(rank)\n            rel_errors.append(result['rel_error'])\n            abs_errors.append(result['error'])\n    \n    ranks = np.array(ranks)\n    rel_errors = np.array(rel_errors)\n    abs_errors = np.array(abs_errors)\n    \n    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n    \n    axes[0, 0].plot(ranks, rel_errors * 100, 'bo-', linewidth=2, markersize=4)\n    axes[0, 0].set_xlabel('Rank')\n    axes[0, 0].set_ylabel('Relative Error (%)')\n    axes[0, 0].set_title('Reconstruction Error vs Rank')\n    axes[0, 0].grid(True, alpha=0.3)\n    axes[0, 0].set_ylim(bottom=0)\n    \n    min_error_idx = np.argmin(rel_errors)\n    axes[0, 0].annotate(\n      f'Min: Rank {ranks[min_error_idx]}\\nError: {rel_errors[min_error_idx]*100:.2f}%',\n      xy=(ranks[min_error_idx], rel_errors[min_error_idx]*100),\n      xytext=(ranks[min_error_idx]+5, rel_errors[min_error_idx]*100+2),\n      arrowprops=dict(arrowstyle='-&gt;', color='red'),\n      fontsize=9, color='red'\n      )\n    \n    axes[0, 1].semilogy(ranks, rel_errors * 100, 'ro-', linewidth=2, markersize=4)\n    axes[0, 1].set_xlabel('Rank')\n    axes[0, 1].set_ylabel('Relative Error (%) [Log Scale]')\n    axes[0, 1].set_title('Error vs Rank (Log Scale)')\n    axes[0, 1].grid(True, alpha=0.3)\n    \n    error_reduction = np.diff(rel_errors) / rel_errors[:-1] * -100\n    axes[1, 0].bar(ranks[1:], error_reduction, alpha=0.7, color='green')\n    axes[1, 0].set_xlabel('Rank')\n    axes[1, 0].set_ylabel('Error Reduction Rate (%)')\n    axes[1, 0].set_title('Error Improvement Rate (Rank i vs Rank i-1)')\n    axes[1, 0].grid(True, alpha=0.3)\n    axes[1, 0].axhline(y=0, color='black', linestyle='-', alpha=0.5)\n    \n    explained_variance = (1 - rel_errors**2) * 100\n    axes[1, 1].plot(ranks, explained_variance, 'mo-', linewidth=2, markersize=4)\n    axes[1, 1].set_xlabel('Rank')\n    axes[1, 1].set_ylabel('Explained Variance (%)')\n    axes[1, 1].set_title('Explained Variance vs Rank')\n    axes[1, 1].grid(True, alpha=0.3)\n    axes[1, 1].set_ylim([0, 100])\n    \n    for threshold in [90, 95, 99]:\n        axes[1, 1].axhline(y=threshold, color='gray', linestyle='--', alpha=0.5)\n        axes[1, 1].text(ranks[-10], threshold+1, f'{threshold}%', fontsize=8, alpha=0.7)\n    plt.tight_layout()\n    \n    if save_fig:\n        plt.savefig('rank_analysis.png', dpi=300, bbox_inches='tight')\n    \n    plt.show()\n    \n    return fig, axes\n\n\nWe obtain four-panel visualization that gives a clear picture of how well different ranks capture tongue shape patterns. The blue curve shows the classic elbow effect: reconstruction error drops quickly between ranks 2–10, then levels off, and finally approaches zero around ranks 40–45. In other words, using more than ~40 ranks lets us reconstruct the data almost perfectly. The log-scale view zooms in on the details, showing that most of the improvement happens early (ranks 2–15), with only small, gradual gains after that. The green bars highlight the “value added” by each rank. Big improvements show up at low ranks (2–10), with a few extra bumps around ranks 20–30. Occasionally, the bars dip below zero, which just means the algorithm stalled briefly due to some numerical hiccups.\nFrom a practical standpoint, choosing a rank between 7–20 is a sweet spot. It captures the meaningful tongue movement patterns without overfitting. Going beyond 40 may look like perfect reconstruction, but in reality, it’s more likely fitting noise than real speech dynamics.\n\nparafac_mod = parafac_modeling(X_tensor, max_rank=50)\nfig, axes = rank_analysis(model=parafac_mod)\n\n\n\n\n\n\n\n\n\nranks, rel_errors = optimal_rank(model=parafac_mod)\n\n Best overall rank: 50 (error: 0.000%)\n Elbow point: Rank 7 (error: 7.084%)\n\n Recommendations:\n   • For &lt; 5% error: Rank 11 (error: 4.580%)\n   • For &lt; 1% error: Rank 24 (error: 0.973%)\n   • Rank 5: 140 params, 4.6x compression, 8.91% error\n   • Rank 10: 280 params, 2.3x compression, 5.27% error\n   • Rank 15: 420 params, 1.5x compression, 2.91% error\n   • Rank 20: 560 params, 1.2x compression, 1.90% error\n\n\n\n\n\nFor comparison, we also applied Tucker3 decomposition to the tongue shape data. Unlike PARAFAC, where a single rank is chosen for all modes, Tucker3 allows the number of components in each mode to be varied independently. We therefore tested different combinations of model sizes, systematically varying the dimensions of the grid positions, vowels, and speakers modes (e.g., [5, 8, 4] and other configurations).\nAs in the PARAFAC modeling, each Tucker3 model was fit by decomposing the tensor into three factor matrices and a core tensor. The factor matrices represent the main axes of variation within each mode, while the core tensor captures how these axes interact across grid positions, vowels, and speakers. After reconstruction, the error was computed and stored for each configuration, enabling a direct comparison to PARAFAC.\n\ndef tucker_modeling(X_tensor, max_rank=[10, 10, 10]):\n    \"\"\"\n    Tucker3 modeling\n    \n    Parameters:\n    -----------\n    X_tensor : numpy.ndarray\n        Input tensor to decompose\n    max_rank : list, default=[4, 4, 3]\n        Maximum rank for each mode [mode1, mode2, mode3]\n    \n    Returns:\n    --------\n    dict : Results dictionary with Tucker decomposition results\n    \"\"\"\n    ranks_to_test = []\n    for r1 in range(2, max_rank[0] + 1):\n        for r2 in range(2, max_rank[1] + 1):\n            for r3 in range(2, max_rank[2] + 1):\n                ranks_to_test.append([r1, r2, r3])\n    \n    results = {}\n    successful_count = 0\n    \n    for rank_combo in ranks_to_test:\n        try:\n            factors = tucker(\n                X_tensor, \n                rank=rank_combo,\n                init='random', \n                n_iter_max=200, \n                random_state=42\n            )\n            \n            reconstructed = tl.tucker_to_tensor(factors)\n            error = tl.norm(X_tensor - reconstructed)\n            rel_error = error / tl.norm(X_tensor)\n\n            core_params = np.prod(rank_combo) \n            factor_params = sum(X_tensor.shape[i] * rank_combo[i] for i in range(3))\n            total_params = core_params + factor_params\n            \n            key = str(rank_combo)\n            results[key] = {\n                'ranks': rank_combo,\n                'factors': factors,\n                'reconstructed': reconstructed,\n                'error': error,\n                'rel_error': rel_error,\n                'core_params': core_params,\n                'factor_params': factor_params,\n                'total_params': total_params\n            }\n            \n            successful_count += 1\n            \n        except Exception as e:\n            print(f\" Models failed - {str(e)[:30]}...\")\n            results[str(rank_combo)] = None\n            \n    return results\n\n\n\nCode\ndef tucker_results(model):\n    \"\"\"Analyze Tucker decomposition results\"\"\"\n    successful_results = {k: v for k, v in model.items() if v is not None}\n    \n    if not successful_results:\n        print(\" No successful Tucker decompositions to analyze\")\n        return\n    \n    best_key = min(successful_results.keys(), key=lambda k:\n      successful_results[k]['rel_error'])\n    \n    best_result = successful_results[best_key]\n    \n    print(f\" Best configuration: {best_result['ranks']}\")\n    print(f\"   - Relative error: {best_result['rel_error']*100:.3f}%\")\n    print(f\"   - Total parameters: {best_result['total_params']}\")\n    print(f\"   - Core parameters: {best_result['core_params']}\")\n    print(f\"   - Factor parameters: {best_result['factor_params']}\")\n    print(f\"\\n Top 5 configurations:\")\n    sorted_results = sorted(successful_results.items(), key=lambda x: x[1]['rel_error'])\n    \n    for i, (key, result) in enumerate(sorted_results[:5]):\n        compression_ratio = np.prod(tensor_dims) / result['total_params']\n        print(\n          f\"   {i+1}. {result['ranks']}: {result['rel_error']*100:.3f}% error, \"\n          f\"{result['total_params']} params, {compression_ratio:.1f}x compression\"\n          )\n    \n    print(f\"\\n Recommendations:\")\n    for threshold in [0.05, 0.01, 0.005]:\n        good_configs = [(k, v) for k, v in successful_results.items() \n                       if v['rel_error'] &lt; threshold]\n        if good_configs:\n            best_config = min(good_configs, key=lambda x: x[1]['total_params'])\n            result = best_config[1]\n            print(\n              f\"   • For &lt;{threshold*100:.1f}% error: {result['ranks']} \"\n              f\"({result['rel_error']*100:.3f}% error, {result['total_params']} params)\"\n              )\n\n\n\n\nCode\ndef plot_tucker_results(model):\n    \"\"\"Create comprehensive Tucker visualization\"\"\"\n    import matplotlib.pyplot as plt\n  \n    successful_results = {k: v for k, v in model.items() if v is not None}\n    \n    if not successful_results:\n        print(\" No successful decompositions to plot\")\n        return\n    \n    configs = []\n    rel_errors = []\n    total_params = []\n    core_params = []\n    factor_params = []\n    compression_ratios = []\n    \n    tensor_size = tensor_dims[0] * tensor_dims[1] * tensor_dims[2]\n    \n    for key, result in successful_results.items():\n        configs.append(key)\n        rel_errors.append(result['rel_error'] * 100)\n        total_params.append(result['total_params'])\n        core_params.append(result['core_params'])\n        factor_params.append(result['factor_params'])\n        compression_ratios.append(tensor_size / result['total_params'])\n    \n    rel_errors = np.array(rel_errors)\n    total_params = np.array(total_params)\n    compression_ratios = np.array(compression_ratios)\n    \n    fig = plt.figure(figsize=(18, 12))\n    \n    ax1 = plt.subplot(2, 3, 1)\n    sorted_indices = np.argsort(rel_errors)\n    top_20 = sorted_indices[:min(20, len(sorted_indices))]\n    \n    bars = plt.bar(range(len(top_20)), rel_errors[top_20], alpha=0.7, color='skyblue')\n    plt.xlabel('Configuration Rank')\n    plt.ylabel('Relative Error (%)')\n    plt.title('Tucker: Top 20 Configurations by Error')\n    plt.xticks(range(len(top_20)), [configs[i] for i in top_20], rotation=45, ha='right')\n    plt.grid(True, alpha=0.3)\n    \n    if len(top_20) &gt; 0:\n        bars[0].set_color('gold')\n        plt.text(0, rel_errors[top_20[0]] + 0.1, 'Best', ha='center', fontweight='bold')\n    \n    ax2 = plt.subplot(2, 3, 2)\n    scatter = plt.scatter(total_params, rel_errors, c=compression_ratios, \n                         cmap='viridis', alpha=0.7, s=60)\n    plt.xlabel('Total Parameters')\n    plt.ylabel('Relative Error (%)')\n    plt.title('Tucker: Error vs Model Complexity')\n    plt.grid(True, alpha=0.3)\n    \n    cbar = plt.colorbar(scatter)\n    cbar.set_label('Compression Ratio')\n    \n    pareto_indices = _find_pareto_front(total_params, rel_errors)\n    plt.plot(\n      total_params[pareto_indices], \n      rel_errors[pareto_indices], \n      'r-', \n      linewidth=2, \n      alpha=0.8, \n      label='Pareto Front'\n      )\n    plt.legend()\n    \n    ax3 = plt.subplot(2, 3, 3)\n    top_10_by_error = sorted_indices[:10]\n    \n    core_params_top = [core_params[i] for i in top_10_by_error]\n    factor_params_top = [factor_params[i] for i in top_10_by_error]\n    configs_top = [configs[i] for i in top_10_by_error]\n    \n    x_pos = range(len(top_10_by_error))\n    p1 = plt.bar(\n      x_pos, \n      core_params_top, \n      alpha=0.8, \n      color='coral', \n      label='Core Parameters'\n      )\n    p2 = plt.bar(\n      x_pos, \n      factor_params_top, \n      bottom=core_params_top, \n      alpha=0.8, \n      color='lightblue', \n      label='Factor Parameters'\n      )\n    \n    plt.xlabel('Configuration')\n    plt.ylabel('Number of Parameters')\n    plt.title('Tucker: Parameter Breakdown (Top 10)')\n    plt.xticks(x_pos, configs_top, rotation=45, ha='right')\n    plt.legend()\n    plt.grid(True, alpha=0.3)\n    \n    ax4 = plt.subplot(2, 3, 4)\n    plt.scatter(compression_ratios, rel_errors, alpha=0.7, color='green', s=60)\n    plt.xlabel('Compression Ratio')\n    plt.ylabel('Relative Error (%)')\n    plt.title('Tucker: Compression vs Accuracy Trade-off')\n    plt.grid(True, alpha=0.3)\n    \n    high_compression = compression_ratios &gt; 1.0\n    if np.any(high_compression):\n        best_compression_idx = np.argmax(compression_ratios[high_compression])\n        best_idx = np.where(high_compression)[0][best_compression_idx]\n        plt.annotate(\n          f'{configs[best_idx]}\\n{compression_ratios[best_idx]:.1f}x',\n          xy=(compression_ratios[best_idx], rel_errors[best_idx]),\n          xytext=(compression_ratios[best_idx]+0.1, rel_errors[best_idx]+1),\n          arrowprops=dict(arrowstyle='-&gt;', color='red'),\n          fontsize=9, \n          color='red'\n          )\n    \n    ax5 = plt.subplot(2, 3, 5)\n    plt.hist(rel_errors, bins=20, alpha=0.7, color='purple', edgecolor='black')\n    plt.xlabel('Relative Error (%)')\n    plt.ylabel('Frequency')\n    plt.title('Tucker: Error Distribution')\n    plt.axvline(\n      np.mean(rel_errors), \n      color='red', \n      linestyle='--', \n      label=f'Mean: {np.mean(rel_errors):.2f}%'\n      )\n    plt.axvline(\n      np.median(rel_errors), \n      color='orange', \n      linestyle='--', \n      label=f'Median: {np.median(rel_errors):.2f}%'\n      )\n    plt.legend()\n    plt.grid(True, alpha=0.3)\n    ax6 = plt.subplot(2, 3, 6)\n\n    if len(successful_results) &gt; 10:\n        rank_data = []\n        for key, result in successful_results.items():\n            r1, r2, r3 = result['ranks']\n            rank_data.append([r1, r2, r3, result['rel_error']])\n        \n        rank_data = np.array(rank_data)\n        unique_r1 = sorted(set(rank_data[:, 0]))\n        unique_r2 = sorted(set(rank_data[:, 1]))\n        \n        if len(unique_r1) &gt; 1 and len(unique_r2) &gt; 1:\n            heatmap_data = np.full((len(unique_r2), len(unique_r1)), np.nan)\n            \n            for i, r2 in enumerate(unique_r2):\n                for j, r1 in enumerate(unique_r1):\n                    mask = (rank_data[:, 0] == r1) & (rank_data[:, 1] == r2)\n                    if np.any(mask):\n                        heatmap_data[i, j] = np.min(rank_data[mask, 3]) * 100\n            \n            im = plt.imshow(heatmap_data, cmap='RdYlBu_r', aspect='auto')\n            plt.colorbar(im, label='Relative Error (%)')\n            plt.xlabel('Mode 1 Rank')\n            plt.ylabel('Mode 2 Rank')\n            plt.title('Tucker: Error Heatmap (Mode 1 vs Mode 2)')\n            plt.xticks(range(len(unique_r1)), unique_r1)\n            plt.yticks(range(len(unique_r2)), unique_r2)\n        else:\n            plt.text(\n              0.5, \n              0.5, \n              'Insufficient data\\nfor heatmap', \n               ha='center', \n               va='center', \n               transform=ax6.transAxes, fontsize=12\n               )\n    else:\n        plt.text(\n          0.5, \n          0.5, \n          'Too few configurations\\nfor heatmap', \n          ha='center', \n          va='center', \n          transform=ax6.transAxes, \n          fontsize=12\n          )\n    \n    plt.tight_layout()\n    plt.suptitle('Tucker Decomposition Analysis', fontsize=16, y=0.98)\n    plt.show()\n\ndef _find_pareto_front(params, errors):\n    \"\"\"Find Pareto front for parameter-error trade-off\"\"\"\n    sorted_indices = np.argsort(params)\n    pareto_indices = []\n    min_error_so_far = float('inf')\n    for idx in sorted_indices:\n        if errors[idx] &lt; min_error_so_far:\n            pareto_indices.append(idx)\n            min_error_so_far = errors[idx]\n    \n    return np.array(pareto_indices)\n    \"\"\"Analyze Tucker decomposition results\"\"\"\n    successful_results = {k: v for k, v in results.items() if v is not None}\n    \n    if not successful_results:\n        print(\" No successful Tucker decompositions to analyze\")\n        return\n    \n    best_key = min(successful_results.keys(), key=lambda k:\n      successful_results[k]['rel_error'])\n    best_result = successful_results[best_key]\n    \n    print(f\" Best configuration: {best_result['ranks']}\")\n    print(f\"   - Relative error: {best_result['rel_error']*100:.3f}%\")\n    print(f\"   - Total parameters: {best_result['total_params']}\")\n    print(f\"   - Core parameters: {best_result['core_params']}\")\n    print(f\"   - Factor parameters: {best_result['factor_params']}\")\n    \n    print(f\"\\n Top 5 configurations:\")\n    sorted_results = sorted(successful_results.items(), key=lambda x:\n      x[1]['rel_error'])\n    \n    for i, (key, result) in enumerate(sorted_results[:5]):\n        compression_ratio = np.prod(tensor_dims) / result['total_params']\n        print(\n          f\"   {i+1}. {result['ranks']}: {result['rel_error']*100:.3f}% error, \"\n          f\"{result['total_params']} params, {compression_ratio:.1f}x compression\"\n          )\n    \n    print(f\"\\n Recommendations:\")\n    for threshold in [0.05, 0.01, 0.005]:\n        good_configs = [(k, v) for k, v in successful_results.items() \n                       if v['rel_error'] &lt; threshold]\n        if good_configs:\n            best_config = min(good_configs, key=lambda x: x[1]['total_params'])\n            result = best_config[1]\n            print(\n              f\"   • For &lt;{threshold*100:.1f}% error: {result['ranks']} \"\n              f\"({result['rel_error']*100:.3f}% error, {result['total_params']} params)\"\n              )\n\n\nLikewise, the six-panel visualization below provides insights into Tucker decomposition performance across different core tensor configurations.\nThe top-left panel showcases the twenty best-performing configurations, with reconstruction errors clustered impressively between 0.5-2.0%. This immediately tells us that Tucker decomposition can achieve excellent accuracy for our tongue shape data when properly configured. The scatter plot of error versus model complexity illustrates the trade-off between accuracy and efficiency. The red Pareto Front marks the “optimal” configurations, where we cannot improve both accuracy and efficiency at the same time. In other words, if we want a lower error, we must accept higher complexity, and vice versa.\nThe stacked bars break down model complexity into two parts: parameters from the core tensor (orange) and those from the factor matrices (blue). This shows where the computational cost comes from in each model. The green dots compare compression ratio against reconstruction error. Configurations above the diagonal line achieve good compression while still keeping the error low, which makes them especially useful in practice.\nFinally, the histogram shows the overall distribution of errors. Most Tucker configurations fall between 8–10% error, with a median of 8.7% and a mean of 9.2%. A few standout models perform much better, achieving far lower error rates. The color-coded grid provides another view, showing how reconstruction error changes across rank combinations for the first two modes. This makes it easier to detect “sweet spots” where the model balances accuracy and efficiency most effectively.\n\ntucker_mod = tucker_modeling(X_tensor)\nplot_tucker_results(model = tucker_mod)\n\n\n\n\n\n\n\n\n\ntucker_results(model = tucker_mod)\n\n Best configuration: [10, 10, 10]\n   - Relative error: 1.348%\n   - Total parameters: 1280\n   - Core parameters: 1000\n   - Factor parameters: 280\n\n Top 5 configurations:\n   1. [10, 10, 10]: 1.348% error, 1280 params, 0.5x compression\n   2. [10, 10, 9]: 1.348% error, 1175 params, 0.6x compression\n   3. [10, 10, 8]: 1.348% error, 1070 params, 0.6x compression\n   4. [10, 10, 5]: 1.348% error, 755 params, 0.9x compression\n   5. [10, 10, 6]: 1.348% error, 860 params, 0.8x compression\n\n Recommendations:\n   • For &lt;5.0% error: [5, 9, 5] (4.703% error, 405 params)\n\n\n\n\n\nAfter optimizing both decomposition methods, we can now directly compare their performance on our tongue shape data. The comparison reveals distinct characteristics and trade-offs between the two approaches.\nThe original tensor slices in the top row provide our baseline reference, showing the complex spatial patterns of tongue positions across different speakers and vowels. Both PARAFAC and Tucker successfully capture the general structure of these patterns, but with notable differences in their approach and results.\nPARAFAC, configured with rank 7, demonstrates its signature strength in providing interpretable factor loadings across all three modes. The factor plots show clear, distinct patterns for each component, with the grid position factors revealing systematic spatial relationships and the vowel factors capturing acoustic-articulatory connections. The reconstruction achieves a 7.08% relative error with just 196 parameters, making it remarkably parameter-efficient. The error map shows relatively uniform reconstruction quality across the tensor space. Meanwhile, Tucker decomposition, using core dimensions [5, 9, 5], takes a fundamentally different approach with its more flexible structure. The factor matrices show more complex patterns, reflecting Tucker’s ability to capture asymmetric relationships between modes. With 405 parameters, Tucker achieves a superior 4.70% relative error, demonstrating the power of its additional flexibility. The core tensor visualization shows the internal structure that Tucker uses to combine these factors, something absent in PARAFAC’s simpler multiplicative model.\nThe direct comparison panels at the bottom quantify these trade-offs clearly. PARAFAC wins on parameter efficiency with 196 versus 405 parameters, translating to better compression ratios. However, Tucker delivers superior reconstruction accuracy with nearly 30% lower relative error. The reconstruction difference heatmap highlights where these methods disagree most strongly, typically in regions with complex multimodal interactions.\n\n\nCode\ndef compare_optimized_models(X_tensor, parafac_rank, tucker_ranks):\n    \"\"\"\n    Compare optimized PARAFAC and Tucker models with comprehensive analysis\n    \n    Parameters:\n    -----------\n    X_tensor : numpy.ndarray\n        Input tensor to decompose\n    parafac_rank : int\n        Optimal rank for PARAFAC\n    tucker_ranks : list\n        Optimal ranks for Tucker [mode1, mode2, mode3]\n    \"\"\"\n    results = {}\n    \n    # PARAFAC Decomposition\n    try:\n        parafac_factors = parafac(\n            X_tensor, \n            rank=parafac_rank, \n            init='random', \n            n_iter_max=200,\n            random_state=42\n        )\n        \n        X_parafac_reconstructed = tl.cp_to_tensor(parafac_factors)\n        parafac_error = tl.norm(X_tensor - X_parafac_reconstructed)\n        parafac_rel_error = parafac_error / tl.norm(X_tensor)\n        parafac_params = parafac_rank * sum(X_tensor.shape)\n        \n        results['PARAFAC'] = {\n            'success': True,\n            'factors': parafac_factors,\n            'reconstructed': X_parafac_reconstructed,\n            'error': parafac_error,\n            'rel_error': parafac_rel_error,\n            'rank': parafac_rank,\n            'params': parafac_params,\n            'method': 'PARAFAC'\n        }\n        \n    except Exception as e:\n        results['PARAFAC'] = {'success': False, 'error': str(e)}\n    \n    # Tucker Decomposition\n    try:\n        tucker_factors = tucker(\n            X_tensor, \n            rank=tucker_ranks, \n            init='random',\n            n_iter_max=200, \n            random_state=42\n        )\n        \n        X_tucker_reconstructed = tl.tucker_to_tensor(tucker_factors)\n        tucker_error = tl.norm(X_tensor - X_tucker_reconstructed)\n        tucker_rel_error = tucker_error / tl.norm(X_tensor)\n        \n        core_params = np.prod(tucker_ranks)\n        factor_params = sum(X_tensor.shape[i] * tucker_ranks[i] for i in range(3))\n        tucker_params = core_params + factor_params\n        \n        results['Tucker'] = {\n            'success': True,\n            'factors': tucker_factors,\n            'reconstructed': X_tucker_reconstructed,\n            'error': tucker_error,\n            'rel_error': tucker_rel_error,\n            'ranks': tucker_ranks,\n            'params': tucker_params,\n            'core_params': core_params,\n            'factor_params': factor_params,\n            'method': 'Tucker'\n        }\n        \n    except Exception as e:\n        results['Tucker'] = {'success': False, 'error': str(e)}\n    \n    _visualization(results, X_tensor)\n    _summary(results)\n    \n    return results\n\ndef _visualization(results, X_tensor):\n    \"\"\"Create comprehensive comparison visualization\"\"\"\n    successful_methods = [method for method, result in results.items() if result['success']]\n    \n    if not successful_methods:\n        return\n    \n    fig = plt.figure(figsize=(20, 14))\n    \n    # Original tensor slices\n    for i in range(min(5, X_tensor.shape[2])):\n        ax = plt.subplot(4, 6, i+1)\n        im = plt.imshow(X_tensor[:, :, i], cmap='viridis', aspect='auto')\n        plt.title(f'Original Tensor\\nSlice {i+1}', fontsize=10)\n        plt.colorbar(im, shrink=0.6)\n        if i == 0:\n            plt.ylabel('Mode 1', fontsize=9)\n        plt.xlabel('Mode 2', fontsize=9)\n    \n    # Tensor statistics\n    ax = plt.subplot(4, 6, 6)\n    ax.text(0.1, 0.8, f'Shape: {X_tensor.shape}', fontsize=12, transform=ax.transAxes)\n    ax.text(0.1, 0.6, f'Elements: {np.prod(X_tensor.shape)}', fontsize=12, transform=ax.transAxes)\n    ax.text(0.1, 0.4, f'Norm: {tl.norm(X_tensor):.3f}', fontsize=12, transform=ax.transAxes)\n    ax.text(0.1, 0.2, f'Range: [{X_tensor.min():.3f}, {X_tensor.max():.3f}]', fontsize=12, transform=ax.transAxes)\n    ax.set_title('Tensor Stats', fontsize=10)\n    ax.axis('off')\n    \n    # PARAFAC Analysis\n    if 'PARAFAC' in successful_methods:\n        result = results['PARAFAC']\n        factors = result['factors'][1]\n        colors = ['blue', 'red', 'green', 'orange']\n        \n        # PARAFAC factor matrices\n        for mode in range(3):\n            ax = plt.subplot(4, 6, 7 + mode)\n            n_comps_show = min(4, factors[mode].shape[1])\n            for comp in range(n_comps_show):\n                plt.plot(factors[mode][:, comp], color=colors[comp], \n                        label=f'Comp {comp+1}', marker='o', markersize=3, linewidth=1.5)\n            plt.title(f'PARAFAC Mode {mode+1}', fontsize=10)\n            plt.xlabel('Index', fontsize=9)\n            plt.ylabel('Factor Value', fontsize=9)\n            if mode == 0:\n                plt.legend(fontsize=7)\n            plt.grid(True, alpha=0.3)\n        \n        # PARAFAC reconstruction\n        ax = plt.subplot(4, 6, 10)\n        recon_slice = result['reconstructed'][:, :, 0]\n        im = plt.imshow(recon_slice, cmap='viridis', aspect='auto')\n        plt.title(f'PARAFAC Recon', fontsize=10)\n        plt.colorbar(im, shrink=0.6)\n        \n        # PARAFAC error\n        ax = plt.subplot(4, 6, 11)\n        error_slice = np.abs(X_tensor[:, :, 0] - recon_slice)\n        im = plt.imshow(error_slice, cmap='Reds', aspect='auto')\n        plt.title(f'PARAFAC Error', fontsize=10)\n        plt.colorbar(im, shrink=0.6)\n        \n        # PARAFAC stats\n        ax = plt.subplot(4, 6, 12)\n        ax.text(0.1, 0.8, f\"Rank: {result['rank']}\", fontsize=12, transform=ax.transAxes)\n        ax.text(0.1, 0.6, f\"Error: {result['rel_error']*100:.3f}%\", fontsize=12, transform=ax.transAxes)\n        ax.text(0.1, 0.4, f\"Params: {result['params']}\", fontsize=12, transform=ax.transAxes)\n        compression = np.prod(X_tensor.shape) / result['params']\n        ax.text(0.1, 0.2, f\"Compression: {compression:.2f}x\", fontsize=12, transform=ax.transAxes)\n        ax.set_title('PARAFAC Stats', fontsize=10)\n        ax.axis('off')\n    \n    # Tucker Analysis\n    if 'Tucker' in successful_methods:\n        result = results['Tucker']\n        factors = result['factors'][1]\n        \n        # Tucker factor matrices\n        for mode in range(3):\n            ax = plt.subplot(4, 6, 13 + mode)\n            n_comps_show = min(4, factors[mode].shape[1])\n            for comp in range(n_comps_show):\n                plt.plot(factors[mode][:, comp], color=colors[comp], \n                        label=f'Comp {comp+1}', marker='s', markersize=3, linewidth=1.5)\n            plt.title(f'Tucker Mode {mode+1}', fontsize=10)\n            plt.xlabel('Index', fontsize=9)\n            plt.ylabel('Factor Value', fontsize=9)\n            if mode == 0:\n                plt.legend(fontsize=7)\n            plt.grid(True, alpha=0.3)\n        \n        # Tucker core tensor\n        ax = plt.subplot(4, 6, 16)\n        core = result['factors'][0]\n        core_slice = core[:, :, 0] if core.ndim == 3 else core\n        im = plt.imshow(core_slice, cmap='RdBu_r', aspect='auto')\n        plt.title('Tucker Core', fontsize=10)\n        plt.colorbar(im, shrink=0.6)\n        \n        # Tucker reconstruction\n        ax = plt.subplot(4, 6, 17)\n        recon_slice = result['reconstructed'][:, :, 0]\n        im = plt.imshow(recon_slice, cmap='viridis', aspect='auto')\n        plt.title(f'Tucker Recon', fontsize=10)\n        plt.colorbar(im, shrink=0.6)\n        \n        # Tucker stats\n        ax = plt.subplot(4, 6, 18)\n        ax.text(0.1, 0.8, f\"Ranks: {result['ranks']}\", fontsize=12, transform=ax.transAxes)\n        ax.text(0.1, 0.6, f\"Error: {result['rel_error']*100:.3f}%\", fontsize=12, transform=ax.transAxes)\n        ax.text(0.1, 0.4, f\"Params: {result['params']}\", fontsize=12, transform=ax.transAxes)\n        compression = np.prod(X_tensor.shape) / result['params']\n        ax.text(0.1, 0.2, f\"Compression: {compression:.2f}x\", fontsize=12, transform=ax.transAxes)\n        ax.set_title('Tucker Stats', fontsize=10)\n        ax.axis('off')\n    \n    # Direct Comparison\n    if len(successful_methods) &gt;= 2:\n        # Error comparison\n        ax = plt.subplot(4, 6, 19)\n        methods = [result['method'] for result in results.values() if result['success']]\n        errors = [result['rel_error'] for result in results.values() if result['success']]\n        colors_comp = ['skyblue', 'lightcoral'][:len(methods)]\n        \n        bars = plt.bar(methods, errors, color=colors_comp, alpha=0.7)\n        plt.title('Error Comparison', fontsize=10)\n        plt.ylabel('Relative Error', fontsize=9)\n        plt.xticks(rotation=45)\n        \n        for bar, error in zip(bars, errors):\n            plt.text(\n                bar.get_x() + bar.get_width()/2, \n                bar.get_height() + 0.002,\n                f'{error:.4f}', \n                ha='center', \n                va='bottom', \n                fontsize=8\n            )\n        \n        # Parameter comparison\n        ax = plt.subplot(4, 6, 20)\n        params = [result['params'] for result in results.values() if result['success']]\n        bars = plt.bar(methods, params, color=colors_comp, alpha=0.7)\n        plt.title('Parameters', fontsize=10)\n        plt.ylabel('# Parameters', fontsize=9)\n        plt.xticks(rotation=45)\n        \n        for bar, param in zip(bars, params):\n            plt.text(\n                bar.get_x() + bar.get_width()/2, \n                bar.get_height() + max(params)*0.02,\n                f'{param}', \n                ha='center', \n                va='bottom', \n                fontsize=8\n            )\n        \n        # Reconstruction difference\n        ax = plt.subplot(4, 6, 21)\n        if len(successful_methods) == 2:\n            parafac_recon = results['PARAFAC']['reconstructed'][:, :, 0] if 'PARAFAC' in results and results['PARAFAC']['success'] else None\n            tucker_recon = results['Tucker']['reconstructed'][:, :, 0] if 'Tucker' in results and results['Tucker']['success'] else None\n            \n            if parafac_recon is not None and tucker_recon is not None:\n                diff = np.abs(parafac_recon - tucker_recon)\n                im = plt.imshow(diff, cmap='plasma', aspect='auto')\n                plt.title('Reconstruction\\nDifference', fontsize=10)\n                plt.colorbar(im, shrink=0.6)\n            else:\n                plt.text(0.5, 0.5, 'Cannot compute\\ndifference', ha='center', va='center', \n                        transform=ax.transAxes, fontsize=10)\n        else:\n            plt.text(0.5, 0.5, 'Need both methods\\nfor comparison', ha='center', va='center', \n                    transform=ax.transAxes, fontsize=10)\n        \n        # Compression comparison\n        ax = plt.subplot(4, 6, 22)\n        compressions = [np.prod(X_tensor.shape) / result['params'] for result in results.values() if result['success']]\n        bars = plt.bar(methods, compressions, color=colors_comp, alpha=0.7)\n        plt.title('Compression Ratio', fontsize=10)\n        plt.ylabel('Compression', fontsize=9)\n        plt.xticks(rotation=45)\n        \n        for bar, comp in zip(bars, compressions):\n            plt.text(\n                bar.get_x() + bar.get_width()/2, \n                bar.get_height() + max(compressions)*0.02,\n                f'{comp:.2f}x', \n                ha='center', \n                va='bottom', \n                fontsize=8\n            )\n        \n        # Explained variance comparison\n        ax = plt.subplot(4, 6, 23)\n        explained_vars = [(1 - result['rel_error']**2) * 100 for result in results.values() if result['success']]\n        bars = plt.bar(methods, explained_vars, color=colors_comp, alpha=0.7)\n        plt.title('Explained Variance', fontsize=10)\n        plt.ylabel('Variance (%)', fontsize=9)\n        plt.xticks(rotation=45)\n        plt.ylim([0, 100])\n        \n        for bar, var in zip(bars, explained_vars):\n            plt.text(\n                bar.get_x() + bar.get_width()/2, \n                bar.get_height() + 2,\n                f'{var:.1f}%', \n                ha='center', \n                va='bottom', \n                fontsize=8\n            )\n    \n    plt.tight_layout()\n    plt.suptitle('PARAFAC vs Tucker: Comparison Results', fontsize=16, y=0.98)\n    plt.show()\n    plt.close()\n\ndef _summary(results):\n    \"\"\"Print minimal final summary\"\"\"\n    successful_methods = [method for method, result in results.items() if result['success']]\n    \n    if successful_methods:\n        best_method = min(successful_methods, key=lambda m: results[m]['rel_error'])\n        best_result = results[best_method]\n        \n        print(f\"Best performing method: {best_method}\")\n        print(f\"  • Relative error: {best_result['rel_error']*100:.3f}%\")\n        print(f\"  • Model parameters: {best_result['params']}\")\n\n\n\nresults = compare_optimized_models(\n    X_tensor, \n    parafac_rank=7, \n    tucker_ranks=[5, 9, 5]\n    )\n\nBest performing method: Tucker\n  • Relative error: 4.703%\n  • Model parameters: 405"
  }
]