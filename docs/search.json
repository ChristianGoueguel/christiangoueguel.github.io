[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Christian L. Goueguel",
    "section": "",
    "text": "Dr. Christian L. Goueguel is a research scientist specializing in laser spectroscopy and chemometrics. His work spans industries such as environmental monitoring, agriculture, medical devices, and dairy production. He has authored 20+ peer-reviewed articles and presented his research at numerous international conferences.\nThroughout his career, Christian has consistently pushed the boundaries of analytical chemistry, delivering innovative, high-impact solutions that bridge research and practical applications.\nHe actively contributes to the academic and research community by serving as a referee for several scientific journals including J. Anal. At. Spectrom., ChemComm, Appl. Opt., Photonics Research, Opt. Lett., and Opt. Express. In this role, he evaluates and reviews submitted manuscripts, providing constructive feedback to ensure the quality, rigor, and integrity of the published research."
  },
  {
    "objectID": "bio.html",
    "href": "bio.html",
    "title": "Christian L. Goueguel",
    "section": "",
    "text": "Photo by Christian L. Goueguel.\n\n\n\n\nChristian earned his PhD in 2012 from the Institut National de la Recherche Scientifique (INRS)-University of Quebec, in Varennes, Canada under the supervision of Mohamed Chaker and François Vidal. Before pursuing his PhD in Canada, he completed both his Bachelor’s and Master’s degrees in Physics at the Grenoble Institute of Technology, France. His doctoral research, conducted at the National Research Council (NRC) in Boucherville, Canada, focused on improving the analytical performance of Laser-Induced Breakdown Spectroscopy (LIBS) for trace element analysis in metallic alloys.\nFollowing his PhD, Christian undertook a NETL-RAU postdoctoral role at Carnegie Mellon University (CMU) in Pittsburgh, Pennsylvania, and an ORISE postdoctoral fellowship at the U.S. Department of Energy’s National Energy Technology Laboratory (NETL). He conducted research on underwater LIBS for real-time environmental monitoring in carbon capture and storage (CCS). His efforts centered on developing innovative approaches to address analytical challenges associated with monitoring trace elements under extreme subsurface conditions.\nChristian has also applied his expertise in agricultural innovation as a research scientist in an agronomic services company. He developed LIBS-based systems for rapid, cost-effective soil and plant tissue analysis, optimizing measurement procedures, designing multivariate calibration models, and integrating machine learning algorithms to predict soil textural properties and nutrient content with high accuracy. Christian also worked as a senior optical engineer for a California-based tech company headquartered in Fremont, while contributing to operations at their Scottsdale office in Arizona. Additionally, he has worked as a chemometrics consultant, where he specialized in developing machine learning models for milk quality assessment using near-infrared spectroscopy (NIRS) data.\nChristian values quality time with his friends and family, finding joy in outdoor activities. A passionate amateur photographer, he also enjoys capturing the beauty of landscapes and cities through his lens, blending his love for optics with his artistic passion. Driven by his belief in learning through teaching, Christian launched his blog to share insights and ideas. Having written a few posts already, he looks forward to publishing many more in the future."
  },
  {
    "objectID": "bio.html#about",
    "href": "bio.html#about",
    "title": "Christian L. Goueguel",
    "section": "",
    "text": "Christian earned his PhD in 2012 from the Institut National de la Recherche Scientifique (INRS)-University of Quebec, in Varennes, Canada under the supervision of Mohamed Chaker and François Vidal. Before pursuing his PhD in Canada, he completed both his Bachelor’s and Master’s degrees in Physics at the Grenoble Institute of Technology, France. His doctoral research, conducted at the National Research Council (NRC) in Boucherville, Canada, focused on improving the analytical performance of Laser-Induced Breakdown Spectroscopy (LIBS) for trace element analysis in metallic alloys.\nFollowing his PhD, Christian undertook a NETL-RAU postdoctoral role at Carnegie Mellon University (CMU) in Pittsburgh, Pennsylvania, and an ORISE postdoctoral fellowship at the U.S. Department of Energy’s National Energy Technology Laboratory (NETL). He conducted research on underwater LIBS for real-time environmental monitoring in carbon capture and storage (CCS). His efforts centered on developing innovative approaches to address analytical challenges associated with monitoring trace elements under extreme subsurface conditions.\nChristian has also applied his expertise in agricultural innovation as a research scientist in an agronomic services company. He developed LIBS-based systems for rapid, cost-effective soil and plant tissue analysis, optimizing measurement procedures, designing multivariate calibration models, and integrating machine learning algorithms to predict soil textural properties and nutrient content with high accuracy. Christian also worked as a senior optical engineer for a California-based tech company headquartered in Fremont, while contributing to operations at their Scottsdale office in Arizona. Additionally, he has worked as a chemometrics consultant, where he specialized in developing machine learning models for milk quality assessment using near-infrared spectroscopy (NIRS) data.\nChristian values quality time with his friends and family, finding joy in outdoor activities. A passionate amateur photographer, he also enjoys capturing the beauty of landscapes and cities through his lens, blending his love for optics with his artistic passion. Driven by his belief in learning through teaching, Christian launched his blog to share insights and ideas. Having written a few posts already, he looks forward to publishing many more in the future."
  },
  {
    "objectID": "publications.html",
    "href": "publications.html",
    "title": "Christian L. Goueguel",
    "section": "",
    "text": "Photo by Sylvia Yang."
  },
  {
    "objectID": "publications.html#book-chapters",
    "href": "publications.html#book-chapters",
    "title": "Christian L. Goueguel",
    "section": "Book Chapters",
    "text": "Book Chapters\n\nC.R. Bhatt, C.L. Goueguel, J.C. Jain, D.L. McIntyre, J.P. Singh, LIBS application to liquid samples In: Laser-Induced Breakdown Spectroscopy (2nd Edition), Elsevier Science, 2020\nD.L. McIntyre, J.C. Jain, C.L. Goueguel, J.P. Singh, Application of Laser-Induced Breakdown Spectroscopy (LIBS) to Carbon Sequestration Research and Development In: Spectroscopic Techniques for Security, Forensic and Environmental Applications, Nova Science Publishers, 2014"
  },
  {
    "objectID": "publications.html#articles",
    "href": "publications.html#articles",
    "title": "Christian L. Goueguel",
    "section": "Articles",
    "text": "Articles\n\nC.L. Goueguel, A. Soumare, C. Nault, J. Nault, Direct determination of soil texture using laser-induced breakdown spectroscopy and multivariate linear regressions, Journal of Analytical Atomic Spectrometry 34, 2019\nC.L. Goueguel, C.R. Bhatt, J.C. Jain, C.L. Lopano, D.L. McIntyre, Quantification of dissolved metals in high-pressure CO2-water solutions by underwater laser-induced breakdown spectroscopy, Optics & Laser Technology 108, 2018\nC.R. Bhatt, J.C. Jain, C.L. Goueguel, D.L. McIntyre, J.P. Singh, Determination of rare earth elements in geological samples using laser-induced breakdown spectroscopy (LIBS), Applied spectroscopy 72, 2018\nC.R. Bhatt, J.C. Jain, C.L. Goueguel, D.L. McIntyre, J.P. Singh, Measurement of Eu and Yb in aqueous solutions by underwater laser-induced breakdown spectroscopy, Spectrochemical Acta Part B: Atomic Spectroscopy 137, 2017\nC.R. Bhatt, C.L. Goueguel, J.C. Jain, H.M. Edenborn, D.L. McIntyre, Analysis of charcoal blast furnace slags by laser-induced breakdown spectroscopy, Applied Optics 56, 2017\nJ.C. Jain, C.L. Goueguel, C.R. Bhatt, D.L. McIntyre, LIBS Sensor for Sub-surface CO2 Leak Detection in Carbon Sequestration, Sensors & Transducers Journal 214, 2017\nJ.C. Jain, D.L. McIntyre, C.L. Goueguel, Harsh environment low- cost LIBS sensor for sub-surface CO2 leak detection in carbon sequestration, Materials for Energy, Efficiency, and Sustainability: TechConnect Briefs 2017\nC.L. Goueguel, J.C. Jain, D.L. McIntyre, C.G. Carson, H.M. Edenborn, In situ measurements of calcium carbonate dissolution under rising CO2 pressure using underwater laser-induced breakdown spectroscopy, Journal of Analytical Atomic Spectrometry 31, 2016\nC.L. Goueguel, D.L. McIntyre, J.C. Jain, Matrix effect of sodium compounds on the determination of metal ions in aqueous solutions by underwater laser-induced breakdown spectroscopy, Optics letters 41, 2016\nC.G. Carson, C.L. Goueguel, H. Sanghapi, J.C. Jain, D.L. McIntyre, Evaluation of a commercially available passively Q-switched Nd: YAG laser with LiF: F2- saturable absorber for laser-induced breakdown spectroscopy, Optics & Laser Technology 79, 2016\nC.L. Goueguel, D.L. McIntyre, J.C. Jain, A.K. Karamalidis, C.G. Carson, Matrix effect of sodium compounds on the determination of metal ions in aqueous solutions by underwater laser-induced breakdown spectroscopy, Applied optics 54, 2015\nC.L. Goueguel, D.L. McIntyre, J.C. Jain, A.K. Karamalidis, Laser-Induced Breakdown Spectroscopy (LIBS) of a High-Pressure CO2–Water Mixture: Application to Carbon Sequestration, Applied spectroscopy 68, 2014\nJ.C. Jain, D.L. McIntyre, K. Ayyalasomayajula, V. Dikshit, C.L. Goueguel, F. Yu-Yueh, J.P. Singh, Application of laser-induced breakdown spectroscopy in carbon sequestration research and development, Pramana 83, 2014\nC.L. Goueguel, J.P. Singh, D.L. McIntyre, J.C. Jain, A.K. Karamalidis, Effect of sodium chloride concentration on elemental analysis of brines by laser-induced breakdown spectroscopy (LIBS), Applied spectroscopy 68, 2014\nC.L. Goueguel, S. Laville, F. Vidal, M. Chaker, M. Sabsabi, Resonant laser-induced breakdown spectroscopy for analysis of lead traces in copper alloys, Journal of Analytical Atomic Spectrometry 26, 2011\nF. Vidal, S. Laville, C.L. Goueguel, H. Loudyi, K. Rifai, M. Chaker, M. Sabsabi, A simple model of laser-induced fluorescence under arbitrary optical thickness conditions at the excitation wavelength, Journal of Quantitative Spectroscopy and Radiative Transfer 111, 2010\nC.L. Goueguel, S. Laville, F. Vidal, M. Sabsabi, M. Chaker, Investigation of resonance-enhanced laser-induced breakdown spectroscopy for analysis of aluminum alloys, Journal of Analytical Atomic Spectrometry 25, 2010\nS. Laville, C.L. Goueguel, H. Loudyi, F. Vidal, M. Chaker, M. Sabsabi, Laser-induced fluorescence detection of lead atoms in a laser-induced plasma: An experimental analytical optimization study, Spectrochimica Acta Part B: Atomic Spectroscopy 64, 2009"
  },
  {
    "objectID": "publications.html#conference-proceedings",
    "href": "publications.html#conference-proceedings",
    "title": "Christian L. Goueguel",
    "section": "Conference Proceedings",
    "text": "Conference Proceedings\n\nJ.C. Jain, H.M. Edenborn, C.L. Goueguel, C.R. Bhatt, D. Hartzler, D.L. McIntyre, A Novel Approach for Tracking CO2 Leakage Into Groundwater Using Carbonate Mineral Dissolution, Geological Society of America, 50, 2018\nJ.C. Jain, C.L. Goueguel, D.L. McIntyre, Development of LIBS Sensor for Sub-Surface CO2 Leak Detection in Carbon Sequestration, 2017 AIChE Annual Meeting\nJ.C. Jain, H.M. Edenborn, C.L. Goueguel, C.R. Bhatt, D.L. McIntyre, A Rapid Method for the Chemical Analysis of Charcoal Iron Furnace Slags, Geological Society of America, 49, 2017\nC.G. Carson, C.L. Goueguel, J.C. Jain, D.L. McIntyre, Development of laser-induced breakdown spectroscopy sensor to assess groundwater quality impacts resulting from geologic carbon sequestration, Micro-and Nanotechnology Sensors, Systems, and Applications VII 9467, 2015\nF. Vidal, M. Chaker, C.L. Goueguel, S. Laville, H. Loudyi, K. Rifai, M. Sabsabi, Enhanced Laser‐Induced Breakdown Spectroscopy by Second‐Pulse Selective Wavelength Excitation, AIP Conference Proceedings 1047, 2008\nC.L. Goueguel, S. Laville, H. Loudyi, M. Chaker, M. Sabsabi, F. Vidal, Detection of lead in brass by laser-induced breakdown spectroscopy combined with laser-induced fluorescence, Photonics North 2008, 709927, 2008"
  },
  {
    "objectID": "blog/posts/post3/index.html",
    "href": "blog/posts/post3/index.html",
    "title": "Chemometric Modeling with Tidymodels: A Tutorial for Spectroscopic Data",
    "section": "",
    "text": "Photo by Robert Lukeman.\n\n\n\n\nFor this tutorial, we use the beer dataset, publicly available and commonly used for spectroscopy-based regression problems. This dataset contains near-infrared spectroscopy (NIRS) data of beer samples alongside measurements of the original gravity (alcoholic beverage), which serves as the target variable. Original gravity (OG) is one of the primary metrics used by brewers to estimate the potential alcohol content of the beer, as it reflects the fermentable sugar content available for yeast metabolism. By analyzing OG alongside the NIRS spectra, we can explore how the spectral data correlates with this fundamental brewing property, offering insights into the chemical composition and quality of the beer samples.\n\nSetup\nBelow, we use suppressPackageStartupMessages to suppress startup messages for clarity and load essential packages.\n\nssh = suppressPackageStartupMessages\n\n\ntidyverse for data manipulation and visualization.\ntidymodels for modeling workflows and machine learning.\ntidymodels_prefer() ensures consistency across conflicting tidymodels functions.\n\n\nssh(library(tidyverse))\nssh(library(tidymodels))\ntidymodels_prefer()\n\nAdditional libraries include:\n\nkknn: Implements k-nearest neighbors (KNN).\nglmnet: Used for elastic net regression.\nranger: Used for random forest.\nplsmod: Supports partial least squares (PLS) regression.\nmagrittr: Provides pipe operators (%&gt;%, %&lt;&gt;%).\npatchwork: Simplifies combining ggplot2 plots.\n\n\nlibrary(kknn)\nlibrary(plsmod)\nssh(library(glmnet))\nssh(library(ranger))\n\n\nssh(library(magrittr))\nlibrary(patchwork)\n\nWe set a custom theme with a clean white background and adjusted sizes for all ggplot2 plots.\n\nbase_size = 15 \nggplot2::theme_bw(\n  base_size = base_size,\n  base_line_size = base_size / 22,\n  base_rect_size = base_size / 15\n  ) %&gt;% \n  theme_set()\n\n\n\nDataset Overview\nWe begin by loading the beer dataset and identifying the spectral predictor columns, which correspond to the NIRS wavelength variables. Usually, I prefer storing spectral wavelengths as character strings in a variable named wavelength because it makes data manipulation easier. This approach enhances flexibility when selecting, filtering, or grouping columns, simplifies integration with tidyverse functions, and ensures compatibility with tidymodels preprocessing workflows.\n\nbeer_data &lt;- read_csv(\"beer.csv\", show_col_types = FALSE)\nwavelength &lt;- beer_data %&gt;% select(starts_with(\"xtrain\")) %&gt;% names()\n\nPreviewing the first rows of the dataset helps us ensure its integrity and understand its structure.\n\nbeer_data %&gt;% head()\n\n# A tibble: 6 × 577\n  originalGravity xtrain.1 xtrain.2 xtrain.3 xtrain.4 xtrain.5 xtrain.6 xtrain.7\n            &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n1            4.23    0.245    0.252    0.258    0.265    0.272    0.280    0.288\n2            6.02    0.243    0.249    0.256    0.262    0.269    0.277    0.285\n3            6.49    0.242    0.248    0.254    0.261    0.268    0.276    0.283\n4            8.92    0.240    0.246    0.252    0.259    0.266    0.273    0.281\n5            8.98    0.241    0.247    0.254    0.260    0.267    0.275    0.282\n6           10.2     0.240    0.246    0.253    0.259    0.266    0.274    0.281\n# ℹ 569 more variables: xtrain.8 &lt;dbl&gt;, xtrain.9 &lt;dbl&gt;, xtrain.10 &lt;dbl&gt;,\n#   xtrain.11 &lt;dbl&gt;, xtrain.12 &lt;dbl&gt;, xtrain.13 &lt;dbl&gt;, xtrain.14 &lt;dbl&gt;,\n#   xtrain.15 &lt;dbl&gt;, xtrain.16 &lt;dbl&gt;, xtrain.17 &lt;dbl&gt;, xtrain.18 &lt;dbl&gt;,\n#   xtrain.19 &lt;dbl&gt;, xtrain.20 &lt;dbl&gt;, xtrain.21 &lt;dbl&gt;, xtrain.22 &lt;dbl&gt;,\n#   xtrain.23 &lt;dbl&gt;, xtrain.24 &lt;dbl&gt;, xtrain.25 &lt;dbl&gt;, xtrain.26 &lt;dbl&gt;,\n#   xtrain.27 &lt;dbl&gt;, xtrain.28 &lt;dbl&gt;, xtrain.29 &lt;dbl&gt;, xtrain.30 &lt;dbl&gt;,\n#   xtrain.31 &lt;dbl&gt;, xtrain.32 &lt;dbl&gt;, xtrain.33 &lt;dbl&gt;, xtrain.34 &lt;dbl&gt;, …\n\n\n\n\nShow the code\np &lt;- beer_data %&gt;% mutate(spectra_id = paste0(\"s\", 1:80)) %&gt;%\n  pivot_longer(\n  cols = -c(originalGravity, spectra_id),\n  names_to = \"wavelength\",\n  values_to = \"intensity\"\n  ) %&gt;%\n  mutate(wavelength = rep(seq(1100, 2250, 2), times = 80)) %&gt;%\n  ggplot() +\n  aes(x = wavelength, y = intensity, colour = originalGravity, group = spectra_id) +\n  geom_line() +\n  scale_color_viridis_c(option = \"inferno\", direction = 1) +\n  labs(\n    x = \"Wavelength [nm]\", \n    y = \"Absorbance [arb. units]\", \n    title = \"NIRS Spectra of Beer Samples\", \n    subtitle = \"Contains 80 samples, measured from 1100 to 2250 nm\", \n    color = \"Original Gravity\") +\n  theme_minimal()\n\nplotly::ggplotly(p)\n\n\n\n\n\n\n\n\nSupervised Learning Techniques\nFor this analysis, we’ll evaluate and compare the performance of four supervised learning algorithms, categorized by their linearity or modeling approach (parametric vs. non-parametric):\n\n\n\n\n\n\n\n\nAlgorithm\nAcronym\nApproach\n\n\n\n\nsparse Partial Least Squares\nsPLS\nLinear\n\n\nElastic Net\nENet\nLinear\n\n\nk-Nearest Neighbors\nKNN\nNon-linear\n\n\nRandom Forests\nRF\nNon-linear\n\n\n\n\n\nStep 1: Data Splitting\nTo ensure unbiased model evaluation, we partition the data into training (80%) and testing (20%) sets, employing stratified sampling based on the target variable’s distribution.\n\nset.seed(123)\nsplit_data &lt;- initial_split(beer_data, prop = 0.8, strata = originalGravity)\ntrain_data &lt;- training(split_data)\ntest_data &lt;- testing(split_data)\n\n\n\nStep 2: Cross-Validation\nWe use a 5-fold repeated cross-validation strategy for hyperparameters tuning and performance evaluation, minimizing the risk of overfitting.\n\ncv_folds &lt;- vfold_cv(train_data, v = 5, repeats = 3)\n\nInterestingly, the vfold_cv function provides a powerful way to visualize the distribution of data across folds, allowing us to confirm that the stratification and splits are evenly balanced. This ensures that each fold accurately represents the overall dataset, enhancing the reliability of cross-validation results.\n\n\nShow the code\ncv_folds %&gt;%\n  tidy() %&gt;%\n  ggplot(aes(x = Fold, y = Row, fill = Data)) +\n  geom_tile() + \n  facet_wrap(~Repeat) + \n  scale_fill_brewer(palette = \"Paired\")\n\n\n\n\n\n\n\n\n\n\n\nStep 3: Preprocessing\nPreprocessing spectral data is a vast and intricate topic, deserving its own dedicated discussion, which we will explore in a future post. For this tutorial, we remove zero-variance predictors, and center the spectra intensity to ensure the data is well-suited for modeling.\ntidymodels provides a wide array of preprocessing steps through its versatile step_* functions. These functions allow for comprehensive data transformations, including centering, scaling, feature selection, and more, to be seamlessly integrated into the modeling workflow. Additionally, tidymodels offers the flexibility to create custom recipe steps, enabling you to design and implement tailored data transformations that meet your specific needs.\n\nbase_recipe &lt;- recipe(originalGravity ~ ., data = train_data) %&gt;%\n  update_role(originalGravity, new_role = \"outcome\") %&gt;%\n  update_role(all_of(wavelength), new_role = \"predictor\") %&gt;%\n  step_zv(all_predictors()) %&gt;%\n  step_center(all_predictors())\n\nFor comparison with the base preprocessing step, we introduce an additional step that applies Principal Component Analysis (PCA) to the predictor variables. This transformation reduces the dimensionality of the data while retaining the most significant variance.\n\npca_recipe &lt;- base_recipe %&gt;%\n  step_pca(all_predictors())\n\n\n\nStep 4: Model Specifications\nWe now define model specifications for each algorithm, incorporating hyperparameter tuning within the tidymodels framework. Notably, the tune function is used to specify hyperparameters that require optimization during the tuning process. For parameters with predefined values, these can be directly assigned within their allowable range.\n\n# k-Nearest Neighbors (KNN)\nknn_spec &lt;- nearest_neighbor() %&gt;%\n  set_args(neighbors = tune(), weight_func = tune(), dist_power = tune()) %&gt;%\n  set_engine('kknn') %&gt;%\n  set_mode('regression')\n\n# Partial Least Squares Regression (PLSR)\nspls_spec &lt;- pls() %&gt;%\n  set_args(predictor_prop = tune(), num_comp = tune()) %&gt;%\n  set_engine('mixOmics') %&gt;%\n  set_mode('regression')\n\n# Elastic Net (ENet)\nenet_spec &lt;-linear_reg() %&gt;%\n  set_args(penalty = tune(), mixture = tune()) %&gt;%\n  set_engine(\"glmnet\") %&gt;%\n  set_mode(\"regression\")\n\n# Random Forest (RF)\nrf_spec &lt;- rand_forest() %&gt;%\n  set_args(mtry = tune(), trees = tune(), min_n = tune()) %&gt;%\n  set_engine(\"ranger\") %&gt;% \n  set_mode(\"regression\")\n\n\n\nStep 5: Model Training and Tuning\nUsing the workflow_set function, we define a workflow for each model and tune hyperparameters using a grid search, and enable parallel processing using 4 CPU cores, accelerating computation during tuning.\n\nwflowSet &lt;- workflow_set(\n  preproc = list(base = base_recipe, pca = pca_recipe), \n  models = list(\n    knn = knn_spec, \n    spls = spls_spec, \n    enet = enet_spec, \n    rf = rf_spec), \n  cross = TRUE\n  )\n\nAs mentionned earlier, we opted to train each model using both the base preprocessing approach and the PCA-transformed data. This strategy results in a total of eight unique training models as follows:\n\nwflowSet \n\n# A workflow set/tibble: 8 × 4\n  wflow_id  info             option    result    \n  &lt;chr&gt;     &lt;list&gt;           &lt;list&gt;    &lt;list&gt;    \n1 base_knn  &lt;tibble [1 × 4]&gt; &lt;opts[0]&gt; &lt;list [0]&gt;\n2 base_spls &lt;tibble [1 × 4]&gt; &lt;opts[0]&gt; &lt;list [0]&gt;\n3 base_enet &lt;tibble [1 × 4]&gt; &lt;opts[0]&gt; &lt;list [0]&gt;\n4 base_rf   &lt;tibble [1 × 4]&gt; &lt;opts[0]&gt; &lt;list [0]&gt;\n5 pca_knn   &lt;tibble [1 × 4]&gt; &lt;opts[0]&gt; &lt;list [0]&gt;\n6 pca_spls  &lt;tibble [1 × 4]&gt; &lt;opts[0]&gt; &lt;list [0]&gt;\n7 pca_enet  &lt;tibble [1 × 4]&gt; &lt;opts[0]&gt; &lt;list [0]&gt;\n8 pca_rf    &lt;tibble [1 × 4]&gt; &lt;opts[0]&gt; &lt;list [0]&gt;\n\n\n\nctrl_grid &lt;- control_grid(\n  verbose = TRUE,\n  allow_par = TRUE,\n  extract = NULL,\n  save_pred = TRUE,\n  pkgs = c(\"doParallel\", \"doFuture\"),\n  save_workflow = FALSE,\n  event_level = \"first\",\n  parallel_over = \"resamples\"\n  )\n\n\ncl &lt;- parallel::makePSOCKcluster(4)\ndoParallel::registerDoParallel(cl)\n\n\nwflowSet %&lt;&gt;%\n  workflow_map(\n    fn = \"tune_grid\",\n    resamples = cv_folds,\n    grid = 10,\n    metrics = metric_set(rmse, mae, rsq),\n    control = ctrl_grid,\n    seed = 3L,\n    verbose = TRUE\n  )\n\n\nparallel::stopCluster(cl)\n\nNext, we utilize the rank_results function to systematically rank the training models based on key performance metrics such as the root mean squared error (RMSE), the mean absolute error (MAE), and the coefficient of determination \\((R^2)\\). Following this, we visualize the model performance with error bars, providing a clear and insightful comparison of their predictive capabilities.\n\nwflowSet %&gt;%\n  rank_results(rank_metric = \"rmse\") %&gt;%\n  relocate(c(rank, mean, std_err, model), .before = wflow_id)\n\n# A tibble: 240 × 9\n    rank  mean std_err model      wflow_id  .config   .metric     n preprocessor\n   &lt;int&gt; &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;     &lt;chr&gt;     &lt;chr&gt;   &lt;int&gt; &lt;chr&gt;       \n 1     1 0.146 0.00634 pls        base_spls Preproce… mae        15 recipe      \n 2     1 0.186 0.0106  pls        base_spls Preproce… rmse       15 recipe      \n 3     1 0.994 0.00102 pls        base_spls Preproce… rsq        15 recipe      \n 4     2 0.160 0.0139  linear_reg base_enet Preproce… mae        15 recipe      \n 5     2 0.226 0.0250  linear_reg base_enet Preproce… rmse       15 recipe      \n 6     2 0.990 0.00307 linear_reg base_enet Preproce… rsq        15 recipe      \n 7     3 0.160 0.0139  linear_reg base_enet Preproce… mae        15 recipe      \n 8     3 0.226 0.0249  linear_reg base_enet Preproce… rmse       15 recipe      \n 9     3 0.990 0.00305 linear_reg base_enet Preproce… rsq        15 recipe      \n10     4 0.160 0.0140  linear_reg base_enet Preproce… mae        15 recipe      \n# ℹ 230 more rows\n\n\n\nwflowSet %&gt;% autoplot(std_errs = qnorm(0.95), type = \"wflow_id\") +\n  ggsci::scale_color_lancet() +\n  theme(legend.position = \"bottom\") +\n  ylab(\"\")\n\n\n\n\n\n\n\n\n\n\nStep 6: Model Selection\nBy configuring the autoplot function with the argument select_best = TRUE, we rank the training models while visually emphasizing the best-performing model, making it easy to identify the optimal choice for further evaluation.\n\nwflowSet %&gt;% autoplot(select_best = TRUE, std_errs = qnorm(0.95), type = \"wflow_id\") +\n  geom_point(size = 3) +\n  ggsci::scale_color_lancet() +\n  theme(legend.position = \"bottom\") +\n  ylab(\"\")\n\n\n\n\n\n\n\n\nThe extract_workflow_set_result function retrieves the optimized hyperparameter values for the best-performing model, as determined by the lowest RMSE. In this analysis, it determines the optimal settings for base_spls, specifically identifying the optimized number of latent variables (num_comp) and the proportion of predictors (predictor_prop) allowed to have non-zero coefficients.\n\nbest_model &lt;- wflowSet %&gt;% \n  extract_workflow_set_result(\"base_spls\") %&gt;% \n  select_best(metric = \"rmse\") %&gt;%\n  print()\n\n# A tibble: 1 × 3\n  predictor_prop num_comp .config              \n           &lt;dbl&gt;    &lt;int&gt; &lt;chr&gt;                \n1         0.0633        3 Preprocessor1_Model01\n\n\nWe use the collect_predictions function to gather the best-performing model’s training data and visualize the relationship between the actual and predicted values. This allows us to assess the model’s predictive accuracy. Additionally, we perform a residual analysis using standardized residuals to further evaluate the model’s performance and identify any potential areas for improvement.\n\ntrain_results &lt;- wflowSet %&gt;% \n  collect_predictions() %&gt;% \n  filter(wflow_id == \"base_spls\" & .config == best_model %&gt;% pull(.config)) %&gt;%\n  select(-.row) %&gt;%\n  print()\n\n# A tibble: 62 × 6\n   wflow_id  .config               preproc model .pred originalGravity\n   &lt;chr&gt;     &lt;chr&gt;                 &lt;chr&gt;   &lt;chr&gt; &lt;dbl&gt;           &lt;dbl&gt;\n 1 base_spls Preprocessor1_Model01 recipe  pls    3.74            4.23\n 2 base_spls Preprocessor1_Model01 recipe  pls    6.39            6.02\n 3 base_spls Preprocessor1_Model01 recipe  pls    6.83            6.49\n 4 base_spls Preprocessor1_Model01 recipe  pls    9.11            8.92\n 5 base_spls Preprocessor1_Model01 recipe  pls    9.29            8.98\n 6 base_spls Preprocessor1_Model01 recipe  pls   10.3            10.2 \n 7 base_spls Preprocessor1_Model01 recipe  pls   10.3            10.4 \n 8 base_spls Preprocessor1_Model01 recipe  pls   10.6            10.4 \n 9 base_spls Preprocessor1_Model01 recipe  pls   10.4            10.5 \n10 base_spls Preprocessor1_Model01 recipe  pls   10.2            10.5 \n# ℹ 52 more rows\n\n\n\n\nShow the code\np1 &lt;- train_results %&gt;% \n  ggplot() +\n  aes(x = originalGravity, y = .pred) +\n  geom_point(alpha = .5) +\n  geom_abline(color = \"gray50\", lty = 2) +\n  coord_obs_pred() +\n  labs(x = \"Actual Original Gravity\", y = \"Predicted Original Gravity\")\n\n\n\n\nShow the code\np2 &lt;- train_results %&gt;% \n  ggplot() +\n  aes(x = .pred, y = (originalGravity - .pred)/sd((originalGravity - .pred))) +\n  geom_hline(yintercept = 0, color = \"gray50\", lty = 2) + \n  geom_point(alpha = .5) +\n  geom_smooth(method = \"loess\", se = FALSE, color = \"red\", lty = 1, linewidth = 0.5) +\n  labs(x = \"Predicted Original Gravity\", y = \"Standardized Residuals\")\n\n\n\n\nShow the code\np1 | p2\n\n\n\n\n\n\n\n\n\n\n\nStep 7: Model Testing\nFinally, we finalize the best-performing model (base_spls) by utilizing the extract_workflow and finalize_workflow functions. We then assess the model’s performance on the test set using last_fit(split = split_data), calculating key metrics to evaluate its accuracy. These metrics are retrieved using the collect_metrics() function.\nAs done previously, we visualize the results through actual vs. predicted plots, complemented by residual diagnostics, to provide a comprehensive evaluation of the model’s performance.\n\ntest_results &lt;- wflowSet %&gt;% \n  extract_workflow(\"base_spls\") %&gt;% \n  finalize_workflow(best_model) %&gt;% \n  last_fit(split = split_data)\n\n\ntest_results %&gt;% collect_metrics()\n\n# A tibble: 2 × 4\n  .metric .estimator .estimate .config             \n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 rmse    standard       0.102 Preprocessor1_Model1\n2 rsq     standard       0.999 Preprocessor1_Model1\n\n\n\n\nShow the code\np3 &lt;- test_results %&gt;% \n  collect_predictions() %&gt;% \n  ggplot() +\n  aes(x = originalGravity, y = .pred) +\n  geom_abline(color = \"gray50\", lty = 2) + \n  geom_point(alpha = .5) + \n  coord_obs_pred() +\n  labs(x = \"Actual Original Gravity\", y = \"Predicted Original Gravity\")\n\n\n\n\nShow the code\np4 &lt;- test_results %&gt;% \n  collect_predictions() %&gt;% \n  ggplot() +\n  aes(x = .pred, y = (originalGravity - .pred)/sd((originalGravity - .pred))) +\n  geom_hline(yintercept = 0, color = \"gray50\", lty = 2) + \n  geom_point(alpha = .5) +\n  geom_smooth(method = \"loess\", se = FALSE, color = \"red\", lty = 1, linewidth = 0.5) +\n  labs(x = \"Predicted Original Gravity\", y = \"Standardized Residuals\")\n\n\n\n\nShow the code\np3 | p4\n\n\n\n\n\n\n\n\n\n\n\nConclusion\nThis tutorial showcased the versatility of the Tidymodels framework for chemometric applications. By leveraging its modular and tidy design, you can implement robust spectroscopic models tailored to your dataset, ensuring both accuracy and reproducibility."
  },
  {
    "objectID": "blog/posts/post1/index.html",
    "href": "blog/posts/post1/index.html",
    "title": "An Overview of Orthogonal Partial Least Squares",
    "section": "",
    "text": "Photo by Simon Berger.\n\n\n\n\nFirst and foremost, let me briefly recall that Partial Least Squares (PLS) regression is, without doubt,one of the most, or maybe the most, multivariate regression methods commonly used in chemometrics. In fact, PLS was originally developed around 1975 by Herman Wold for use in the field of econometrics and was later embraced in the 1980’s by prominent chemometricians such as Svante Wold — Herman Wold’s son, who in 1971 coined the term chemometrics to describe this emerging field of using advanced statistical and machine learning methods in analytical chemistry applications — , Harald Martens, Bruce Kowalski, Tormod Naes, and Paul Geladi, just to name a few. Over the past few decades, PLS — whether as a regression or classification method — has become very popular among chemometrics practitioners for dealing with multicollinearity in applications such as multivariate calibration and process analytical technology, where there are usually fewer observations (samples) than variables (wavelengths).\nIn 2002, Johan Trygg and Svante Wold introduced a variant of PLS in a paper entitled “Orthogonal projections to latent structures (O‐PLS)”. The new approach is a supervised multivariate data projection method used to relate a set of predictor variables \\((X)\\) to one or more responses \\((Y)\\).Basically, like PLS, O-PLS attempts to extract the maximum information reflecting the variation in the data set, while assuming the existence of a small subset of hidden variables in the \\(X\\)-data to predict the response variables. These subsets are formally called latent variables (or LVs) because they are unmeasurable. Historically, this concept of hidden structure in a data set is a century old and comes from methods such as Principal Component Analysis (PCA). The O-PLS method, as named by the authors, uses orthogonal signal correction previously developed by S. Wold — Chemometrics Intell. Lab. Syst., 1998, 44, 175 — to maximize the explained covariance on the first LV, while the remaining LVs capture variance in the predictors which is orthogonal, i.e. statistically uncorrelated to the response variables. To put it simply, this means that unlike PLS — which handle random noise fairly well — , the new method also known as Orthogonal Partial Least-Squares (OPLS) enables to filter out the structured noise in the data set by modeling separately variations of the \\(X\\)-predictors correlated and uncorrelated to the \\(Y\\)-responses. Ultimately, this reduces the model’s complexity by lowing the number of LVs, in addition to allowing identification, analysis and investigation of the main source of orthogonal variation.\n\n\n\nLet’s take a quick look at the mathematics behind OPLS. Herein we consider \\(X\\) as a matrix of size \\(n \\times p\\) and \\(Y\\) as a matrix of size \\(n\\times m\\), where \\(p\\)isthe number of predictor variables, \\(m\\) isthe number of response variables and, \\(n\\) is the number of observations. Recall that PLS has been developed with the aim of searching the direction of a certain number of LV — with the constraint of being orthogonal to each other — that meet the following criteria: (1) capture maximum variance in the \\(X\\)-space, (2) capture maximum variance in the \\(Y\\)-space and, (3) maximize correlation between the \\(X\\)- and \\(Y\\)-space.\nA PLS model can be written as:\n\\[\n\\begin{align*}\nX =& TP^T + E \\\\\nY =& UC^T + F \\\\\nT =& U + H\n\\end{align*}\n\\]\nwhere \\(T\\) is the scores matrix summarizing variation in the \\(X\\)-predictors — i.e. similarities/differences between samples — , \\(P\\) is the loadings matrix — or often called the \\(X\\)‐loadings to distinguish it from the \\(Y\\)‐loadings — , \\(U\\) is the scores matrix that summarizes variation in the \\(Y\\)-responses, \\(C\\) is the \\(Y\\)‐loadings and, \\(E\\),\\(F\\),\\(H\\)arethe residual matrices. The superscript \\(\"T\"\\) denotes the matrix transpose. Note that \\(P\\) expresses the correlation between \\(X\\) and \\(U\\), whilst \\(C\\) expresses the correlation between \\(Y\\) and \\(T\\).\n\n\n\nFIG. 1: Geometrical illustration of a PLS model. Data (black dots) on the \\(X\\)-space are projected (orthogonal projection) onto the subspace defined by the first two latent variables (LVs).\n\n\nOPLS model operates similarly to PLS model but separates the systematic variation in \\(X\\)into three parts: (1) a predictive part that is correlated to \\(Y\\), (2) an orthogonal part, “orth”, that is uncorrelated to \\(Y\\), (3) a noise part — residual variation.\nThus, an OPLS model can be formulated as:\n\\[\n\\begin{align*}\nX =& TP^T + T_{\\text{orth}}P^T_{\\text{orth}} + E \\\\\nY =& UC^T + F\n\\end{align*}\n\\]\n\n\n\nFIG. 2: Geometrical illustration of the difference between PLS and OPLS models. Orthogonal variation (white dots) are filtered out by the second latent variable.\n\n\nIt should be mentioned that in fact two different OPLS algorithms have been developed. The first algorithm, most frequently referred to as O1-PLS or simply OPLS, as described above is unidirectional \\((X \\implies Y)\\), meaning that only orthogonal variations in the \\(X\\)-space are filtered out. The second algorithm, referred to as O2-PLS, is bi-directional \\((X \\iff Y)\\), meaning that orthogonal variations in both the \\(X\\)- and \\(Y\\)-space are filtered out.\nAn O2-PLS model can be written as:\n\\[\n\\begin{align*}\nX =& TP^T + T_{\\text{Y-orth}}P^T_{\\text{Y-orth}} + E \\\\\nY =& UC^T + U_{\\text{X-orth}}C^T_{\\text{X-orth}} + F\n\\end{align*}\n\\]\nO2-PLS separates \\(X\\) and \\(Y\\) into three parts: (1) a joint part (correlation between \\(X\\) and \\(Y\\)), (2) an orthogonal part — unrelated latent variation in \\(X\\) and \\(Y\\) separately— ,and (3) a noise part.\n\n\n\nFor comparison purposes, I used the LIBS spectra of plant materials to predict the concentration of potassium using the PLS and OPLS approaches. PLS was performed using the R package caret. Since OPLS is not available in caret, I used the package ropls instead.\n\n\n\nFIG. 3: LIBS spectra of five plant samples. A baseline offset has been applied for easier viewing of the spectra.\n\n\nFigures below show the \\(X\\)-scores scatter plots (projection of the \\(X\\)-space) for PLS (left panel) and OPLS (right panel) modeling. In both figures, the \\(x\\)-axis represents the first component (or latent variable), whereas the \\(y\\)-axis represents the second component for PLS and the first orthogonal component for OPLS. The rainbow colors represent the concentration of potassium (K) in each sample, from 1% (dark blue) to 7% (dark red). As we can see, OPLS only requires one component to correlate variation in the samples with K concentration (see the black arrow).\n\n\n\nFIG. 4: Comparison between the \\(X\\)-scores scatter plots for the PLS and OPLS models.\n\n\n\n\n\nSo in summary, the most important thing to remember is that OPLS (or O2-PLS) offers a valuable preprocessing tool, and will help to generate more efficient predictive models, especially in situations where structured noise dominates."
  },
  {
    "objectID": "blog/posts/post1/index.html#what-is-opls",
    "href": "blog/posts/post1/index.html#what-is-opls",
    "title": "An Overview of Orthogonal Partial Least Squares",
    "section": "",
    "text": "First and foremost, let me briefly recall that Partial Least Squares (PLS) regression is, without doubt,one of the most, or maybe the most, multivariate regression methods commonly used in chemometrics. In fact, PLS was originally developed around 1975 by Herman Wold for use in the field of econometrics and was later embraced in the 1980’s by prominent chemometricians such as Svante Wold — Herman Wold’s son, who in 1971 coined the term chemometrics to describe this emerging field of using advanced statistical and machine learning methods in analytical chemistry applications — , Harald Martens, Bruce Kowalski, Tormod Naes, and Paul Geladi, just to name a few. Over the past few decades, PLS — whether as a regression or classification method — has become very popular among chemometrics practitioners for dealing with multicollinearity in applications such as multivariate calibration and process analytical technology, where there are usually fewer observations (samples) than variables (wavelengths).\nIn 2002, Johan Trygg and Svante Wold introduced a variant of PLS in a paper entitled “Orthogonal projections to latent structures (O‐PLS)”. The new approach is a supervised multivariate data projection method used to relate a set of predictor variables \\((X)\\) to one or more responses \\((Y)\\).Basically, like PLS, O-PLS attempts to extract the maximum information reflecting the variation in the data set, while assuming the existence of a small subset of hidden variables in the \\(X\\)-data to predict the response variables. These subsets are formally called latent variables (or LVs) because they are unmeasurable. Historically, this concept of hidden structure in a data set is a century old and comes from methods such as Principal Component Analysis (PCA). The O-PLS method, as named by the authors, uses orthogonal signal correction previously developed by S. Wold — Chemometrics Intell. Lab. Syst., 1998, 44, 175 — to maximize the explained covariance on the first LV, while the remaining LVs capture variance in the predictors which is orthogonal, i.e. statistically uncorrelated to the response variables. To put it simply, this means that unlike PLS — which handle random noise fairly well — , the new method also known as Orthogonal Partial Least-Squares (OPLS) enables to filter out the structured noise in the data set by modeling separately variations of the \\(X\\)-predictors correlated and uncorrelated to the \\(Y\\)-responses. Ultimately, this reduces the model’s complexity by lowing the number of LVs, in addition to allowing identification, analysis and investigation of the main source of orthogonal variation."
  },
  {
    "objectID": "blog/posts/post1/index.html#mathematical-framework",
    "href": "blog/posts/post1/index.html#mathematical-framework",
    "title": "An Overview of Orthogonal Partial Least Squares",
    "section": "",
    "text": "Let’s take a quick look at the mathematics behind OPLS. Herein we consider \\(X\\) as a matrix of size \\(n \\times p\\) and \\(Y\\) as a matrix of size \\(n\\times m\\), where \\(p\\)isthe number of predictor variables, \\(m\\) isthe number of response variables and, \\(n\\) is the number of observations. Recall that PLS has been developed with the aim of searching the direction of a certain number of LV — with the constraint of being orthogonal to each other — that meet the following criteria: (1) capture maximum variance in the \\(X\\)-space, (2) capture maximum variance in the \\(Y\\)-space and, (3) maximize correlation between the \\(X\\)- and \\(Y\\)-space.\nA PLS model can be written as:\n\\[\n\\begin{align*}\nX =& TP^T + E \\\\\nY =& UC^T + F \\\\\nT =& U + H\n\\end{align*}\n\\]\nwhere \\(T\\) is the scores matrix summarizing variation in the \\(X\\)-predictors — i.e. similarities/differences between samples — , \\(P\\) is the loadings matrix — or often called the \\(X\\)‐loadings to distinguish it from the \\(Y\\)‐loadings — , \\(U\\) is the scores matrix that summarizes variation in the \\(Y\\)-responses, \\(C\\) is the \\(Y\\)‐loadings and, \\(E\\),\\(F\\),\\(H\\)arethe residual matrices. The superscript \\(\"T\"\\) denotes the matrix transpose. Note that \\(P\\) expresses the correlation between \\(X\\) and \\(U\\), whilst \\(C\\) expresses the correlation between \\(Y\\) and \\(T\\).\n\n\n\nFIG. 1: Geometrical illustration of a PLS model. Data (black dots) on the \\(X\\)-space are projected (orthogonal projection) onto the subspace defined by the first two latent variables (LVs).\n\n\nOPLS model operates similarly to PLS model but separates the systematic variation in \\(X\\)into three parts: (1) a predictive part that is correlated to \\(Y\\), (2) an orthogonal part, “orth”, that is uncorrelated to \\(Y\\), (3) a noise part — residual variation.\nThus, an OPLS model can be formulated as:\n\\[\n\\begin{align*}\nX =& TP^T + T_{\\text{orth}}P^T_{\\text{orth}} + E \\\\\nY =& UC^T + F\n\\end{align*}\n\\]\n\n\n\nFIG. 2: Geometrical illustration of the difference between PLS and OPLS models. Orthogonal variation (white dots) are filtered out by the second latent variable.\n\n\nIt should be mentioned that in fact two different OPLS algorithms have been developed. The first algorithm, most frequently referred to as O1-PLS or simply OPLS, as described above is unidirectional \\((X \\implies Y)\\), meaning that only orthogonal variations in the \\(X\\)-space are filtered out. The second algorithm, referred to as O2-PLS, is bi-directional \\((X \\iff Y)\\), meaning that orthogonal variations in both the \\(X\\)- and \\(Y\\)-space are filtered out.\nAn O2-PLS model can be written as:\n\\[\n\\begin{align*}\nX =& TP^T + T_{\\text{Y-orth}}P^T_{\\text{Y-orth}} + E \\\\\nY =& UC^T + U_{\\text{X-orth}}C^T_{\\text{X-orth}} + F\n\\end{align*}\n\\]\nO2-PLS separates \\(X\\) and \\(Y\\) into three parts: (1) a joint part (correlation between \\(X\\) and \\(Y\\)), (2) an orthogonal part — unrelated latent variation in \\(X\\) and \\(Y\\) separately— ,and (3) a noise part."
  },
  {
    "objectID": "blog/posts/post1/index.html#example-libs-spectra",
    "href": "blog/posts/post1/index.html#example-libs-spectra",
    "title": "An Overview of Orthogonal Partial Least Squares",
    "section": "",
    "text": "For comparison purposes, I used the LIBS spectra of plant materials to predict the concentration of potassium using the PLS and OPLS approaches. PLS was performed using the R package caret. Since OPLS is not available in caret, I used the package ropls instead.\n\n\n\nFIG. 3: LIBS spectra of five plant samples. A baseline offset has been applied for easier viewing of the spectra.\n\n\nFigures below show the \\(X\\)-scores scatter plots (projection of the \\(X\\)-space) for PLS (left panel) and OPLS (right panel) modeling. In both figures, the \\(x\\)-axis represents the first component (or latent variable), whereas the \\(y\\)-axis represents the second component for PLS and the first orthogonal component for OPLS. The rainbow colors represent the concentration of potassium (K) in each sample, from 1% (dark blue) to 7% (dark red). As we can see, OPLS only requires one component to correlate variation in the samples with K concentration (see the black arrow).\n\n\n\nFIG. 4: Comparison between the \\(X\\)-scores scatter plots for the PLS and OPLS models."
  },
  {
    "objectID": "blog/posts/post1/index.html#summary",
    "href": "blog/posts/post1/index.html#summary",
    "title": "An Overview of Orthogonal Partial Least Squares",
    "section": "",
    "text": "So in summary, the most important thing to remember is that OPLS (or O2-PLS) offers a valuable preprocessing tool, and will help to generate more efficient predictive models, especially in situations where structured noise dominates."
  },
  {
    "objectID": "photography.html",
    "href": "photography.html",
    "title": "Christian L. Goueguel",
    "section": "",
    "text": "“To me, photography is an art of observation. It’s about finding something interesting in an ordinary place… I’ve found it has little to do with the things you see and everything to do with the way you see them.”\n– Elliott Erwitt\n\n\n\n\nLandscape Photography\n\n \n\n\n\n\n\n\nCity Photography"
  },
  {
    "objectID": "research.html",
    "href": "research.html",
    "title": "Christian L. Goueguel",
    "section": "",
    "text": "Welcome to an overview of my research projects, from current initiatives to notable past projects in laser spectroscopy and analytical instrumentation. Feel free to reach out if you’d like to collaborate or learn more about my work.\n\n\n\n\nLeading research to enhance quantitative analysis of agricultural soils, with a focus on mitigating matrix interference through LIBS combined with advanced chemometric modeling. This work aims to support sustainable agricultural practices.\n\n\n\n\n\n\nDeveloped a fiber-optic LIBS system integrated with chemometrics for online and real-time monitoring of electrolyte solutes in clinical and point-of-care (POC) settings.\n\n\n\n\n\n\n\n\n\nDeveloped a LIBS-based benchtop instrument for the quantitative analysis of agricultural soils, leveraging machine learning and chemometrics to interpret complex spectral data. This research was highlighted in the Journal of Analytical Atomic Spectrometry on the back cover of Volume 34, Issue 8 (2019), for its contributions to soil texture prediction.\n\n\n\nAn example of LIBS spectra recorded from three soil types: sandy, silty, and clayey.\n\n\n\n\n\nPrincipal component analysis (PCA) biplot of spectral data. Sandy soils (grey) and clayey soils (blue) clusters.\n\n\n\n\n\n\nDeveloped a compact, robust LIBS sensor for in-situ monitoring of CCS activities in extreme environments (high temperature, pressure, and salinity). One of our research projects earned the prestigious distinction of being featured on the front cover of the Journal of Analytical Atomic Spectrometry (Volume 31, Issue 7, 2016). This study explored the application of underwater LIBS for real-time, in situ monitoring of carbonate mineral dissolution under rising CO2 pressure.\n\n\n\nUnderwater LIBS experimental setup. Measurements were taken in a controlled, high-pressure environment, replicating up to 400 bar (≈ 6,000 psi) conditions in carbonated brine environments. Quantitative analysis of key elements (Mg, Ca, K, Ba, Mn, Sr, etc.) in brine.\n\n\n\n\n\nLIBS for CCS: Signal-to-noise ratio for key emission lines.\n\n\n\n\n\n\nResearch focused on improving the limit of detection (LOD) for trace impurities in metallic alloys using selective wavelength approaches in laser-produced plasma. Explored LA-LIF, resonance-enhance LIBS, and RLA-LIF approaches for enhanced LODs in impurity analysis. Proposed efficient excitation-fluorescence schemes for targeted element detection in RLA-LIF. Our research on the investigation of resonance-enhance LIBS was featured on the inside front cover of the Journal of Analytical Atomic Spectrometry, Volume 25, Issue 5 (2010).\n\n\n\nLA-LIF and resonance-enhance LIBS experimental setup. A first Nd:YAG pulse generates low-density vapor, followed by a second tunable pulse targeting a resonant transition of an analyte (LA-LIF), or of a matrix element (resonance-enhance LIBS).\n\n\n\n\n\n(upper panel) Confocal microscopy images of resonance-enhance LIBS sample damage at three ablation laser fluences. (lower panel) Resonance-enhance LIBS is especially advantageous when minimal surface damage is critical, as it ablates approximately 10 times less material per shot than standard LIBS.\n\n\n\n\n\nPartial Grotrian diagram illustrating the excitation and fluorescence emission processes of lead, as used in LA-LIF and RLA-LIF experiments.\n\n\n\n\n\n(left panel) RLA-LIF improves detection sensitivity by approximately 1 order of magnitude at a significantly lower laser fluence compared to standard LIBS. (right panel) Optical coherence tomography (OCT)-based 3D plots and scanning electron microscope (SEM) images of sample ablation craters in RLA-LIF."
  },
  {
    "objectID": "research.html#current-research",
    "href": "research.html#current-research",
    "title": "Christian L. Goueguel",
    "section": "",
    "text": "Leading research to enhance quantitative analysis of agricultural soils, with a focus on mitigating matrix interference through LIBS combined with advanced chemometric modeling. This work aims to support sustainable agricultural practices."
  },
  {
    "objectID": "research.html#past-research",
    "href": "research.html#past-research",
    "title": "Christian L. Goueguel",
    "section": "",
    "text": "Developed a fiber-optic LIBS system integrated with chemometrics for online and real-time monitoring of electrolyte solutes in clinical and point-of-care (POC) settings.\n\n\n\n\n\n\n\n\n\nDeveloped a LIBS-based benchtop instrument for the quantitative analysis of agricultural soils, leveraging machine learning and chemometrics to interpret complex spectral data. This research was highlighted in the Journal of Analytical Atomic Spectrometry on the back cover of Volume 34, Issue 8 (2019), for its contributions to soil texture prediction.\n\n\n\nAn example of LIBS spectra recorded from three soil types: sandy, silty, and clayey.\n\n\n\n\n\nPrincipal component analysis (PCA) biplot of spectral data. Sandy soils (grey) and clayey soils (blue) clusters.\n\n\n\n\n\n\nDeveloped a compact, robust LIBS sensor for in-situ monitoring of CCS activities in extreme environments (high temperature, pressure, and salinity). One of our research projects earned the prestigious distinction of being featured on the front cover of the Journal of Analytical Atomic Spectrometry (Volume 31, Issue 7, 2016). This study explored the application of underwater LIBS for real-time, in situ monitoring of carbonate mineral dissolution under rising CO2 pressure.\n\n\n\nUnderwater LIBS experimental setup. Measurements were taken in a controlled, high-pressure environment, replicating up to 400 bar (≈ 6,000 psi) conditions in carbonated brine environments. Quantitative analysis of key elements (Mg, Ca, K, Ba, Mn, Sr, etc.) in brine.\n\n\n\n\n\nLIBS for CCS: Signal-to-noise ratio for key emission lines.\n\n\n\n\n\n\nResearch focused on improving the limit of detection (LOD) for trace impurities in metallic alloys using selective wavelength approaches in laser-produced plasma. Explored LA-LIF, resonance-enhance LIBS, and RLA-LIF approaches for enhanced LODs in impurity analysis. Proposed efficient excitation-fluorescence schemes for targeted element detection in RLA-LIF. Our research on the investigation of resonance-enhance LIBS was featured on the inside front cover of the Journal of Analytical Atomic Spectrometry, Volume 25, Issue 5 (2010).\n\n\n\nLA-LIF and resonance-enhance LIBS experimental setup. A first Nd:YAG pulse generates low-density vapor, followed by a second tunable pulse targeting a resonant transition of an analyte (LA-LIF), or of a matrix element (resonance-enhance LIBS).\n\n\n\n\n\n(upper panel) Confocal microscopy images of resonance-enhance LIBS sample damage at three ablation laser fluences. (lower panel) Resonance-enhance LIBS is especially advantageous when minimal surface damage is critical, as it ablates approximately 10 times less material per shot than standard LIBS.\n\n\n\n\n\nPartial Grotrian diagram illustrating the excitation and fluorescence emission processes of lead, as used in LA-LIF and RLA-LIF experiments.\n\n\n\n\n\n(left panel) RLA-LIF improves detection sensitivity by approximately 1 order of magnitude at a significantly lower laser fluence compared to standard LIBS. (right panel) Optical coherence tomography (OCT)-based 3D plots and scanning electron microscope (SEM) images of sample ablation craters in RLA-LIF."
  },
  {
    "objectID": "talks.html",
    "href": "talks.html",
    "title": "Christian L. Goueguel",
    "section": "",
    "text": "Photo by Susan Q Yin."
  },
  {
    "objectID": "talks.html#oralposter-presentations",
    "href": "talks.html#oralposter-presentations",
    "title": "Home",
    "section": "Oral/Poster Presentations",
    "text": "Oral/Poster Presentations\n\nC. L. Goueguel, Direct determination of soil texture using laser-induced breakdown spectroscopy and multivariate linear regressions. Annual meetings of the North American Society for Laser-Induced Breakdown Spectroscopy (NASLIBS) hosted by FACSS SciX Conference 2019 (Palm Springs, CA, USA)\nC. L. Goueguel, Using LIBS analyzer for laser-based soil analysis for precision agriculture. Invited speaker at the Canadian Society for Analytical Sciences and Spectroscopy, Jan. 1, 2018 (Toronto, ON, Canada)\nJ. C. Jain, C. L. Goueguel, D. L. McIntyre, H. M. Edenborn, Dissolution of mineral carbonates with increasing CO2 pressure and the implications for carbon sequestration leak detection. GSA annual meeting, 2016 (Denver, CO, USA)\nC. G. Carson, C. Goueguel, J. Jain, D. McIntyre, Development of laser induced breakdown spectroscopy sensor to assess groundwater quality impacts resulting from geologic carbon sequestration. SPIE Micro- and Nanotechnology Sensors, Systems, and Applications VII, 2015 (Baltimore, MD, SA)\nC. L. Goueguel, J. C. Jain, D. L. McIntyre, A. K. Karamalidis, Application of laser induced breakdown spectroscopy (LIBS) to analyze CO2-bearing solutions in high pressure high temperature environment. ACS 248th - National Meeting, 2014 (San Francisco, CA, USA)\nC. L. Goueguel, J. C. Jain, A. K. Karamalidis, D. L. McIntyre, J. P. Singh, Laser-induced breakdown spectroscopy of high-pressure carbonated brine solutions. 2014 PITTCON Conference & Expo, 2014 (Chicago, IL, USA)\nC. L. Goueguel, D. L. McIntyre, J. C. Jain, J. P. Singh, A. K. Karamalidis, Analysis of calcium in CO2-laden brine (NaCl-CaCl2) by Laser-induced breakdown spectroscopy (LIBS). Annual meetings of the North American Society for Laser-Induced Breakdown Spectroscopy (NASLIBS) hosted by FACSS SciX Conference 2013 (Milwaukee, WI, USA)\nJ. P. Singh, C L. Goueguel, D. L. McIntyre, J. C. Jain, A. K. Karamalidis, Laser-induced breakdown spectroscopy for elemental analysis of brines. 7th International Conference on Laser-Induced Breakdown Spectroscopy, 2012 (Luxor, Egypt)\nC. Goueguel, Laser Induced Breakdown Spectroscopy (LIBS): From Mars exploration to GCS monitoring. Postdoctoral Seminar at the Department of Civil and Environmental Engineering, Carnegie Mellon University, 2012 (Pittsburgh, PA, USA)\nC. Goueguel, S. Laville, F. Vidal, M. Sabsabi, M. Chaker, Investigation of resonance-enhanced laser-induced breakdown spectroscopy (RELIBS) for the analysis of aluminum alloys. 6th International Conference on Laser- Induced Breakdown Spectroscopy, 2010 (Memphis, TN, USA)\nC. Goueguel, S. Laville, F. Vidal, M. Sabsabi, Chaker, Investigation of resonance-enhanced laser-induced breakdown spectroscopy (RELIBS) for the analysis of aluminum alloys, PITTCON Conference & Expo, 2010 (Orlando, FL, USA)\nC. Goueguel, S. Laville, H. Loudyi, F. Vidal, M. Sabsabi, M. Chaker, Investigation of resonance-enhanced laser-induced breakdown spectroscopy for the analysis of aluminum alloys. 2nd North American Symposium on Laser-Induced Breakdown Spectroscopy, 2009 (New Orleans, LA, USA)\nC. Goueguel, H. Loudyi, S. Laville, M. Chaker, M. Sabsabi, F. Vidal, Excitation spectrale sélective d’un plasma d’aluminium par une seconde impulsion laser: Optimisation des paramètres expérimentaux. Colloque Plasma- Québec, 2009 (Montreal, QC, Canada)\nS. Laville, K. Rifai, H. Loudyi, M. Chaker, M. Sabsabi, F. Vidal, C. Goueguel, Analysis of trace elements in liquids using LIBS combined with laser-induced fluorescence. 5th International Conference on Laser-Induced Breakdown Spectroscopy, 2008 (Berlin, Germany)\nF. Vidal, M. Chaker, C. Goueguel, S. Laville, H. Loudyi, K. Rifai, M. Sabsabi, Enhanced laser-induced breakdown spectroscopy by second pulse selective wavelength excitation. Laser and plasma applications in materials science: 1st International Conference on Laser Plasma Applications in Materials Science, LAPAMS, 2008 (Alger, Algeria)\nC. Goueguel, S. Laville, H. Loudyi, M. Chaker, M. Sabsabi, F. Vidal. Detection of lead in brass by Laser-Induced Breakdown Spectroscopy combined with Laser-Induced Fluorescence. Photonics North, 2008 (Montreal, QC, Canada)\nH. Loudyi, C. Goueguel, K. Rifai, S. Laville, M. Sabsabi, M. Chaker, F. Vidal, Enhancing the sensitivity of the laser-Induced Breakdown Spectroscopy technique: spectrally selective excitation of specific elements in laser produced plasma. Canadian Association of Physicists Congress, 2008 (Laval, QC, Canada)\nC. Goueguel, H. Loudyi, S. Laville, M. Chaker, M. Sabsabi, F. Vidal, Analyse des matériaux solides par LIBS: Effet d’une deuxième impulsion laser basée sur une excitation spectrale sélective. Colloque Plasma-Québec, 2008 (Montreal, QC, Canada)\nF. Vidal, S. Laville, C. Goueguel, H. Loudyi, M. Chaker, M. Sabsabi, Investigation of laser-induced breakdown spectroscopy combined with laser- induced fluorescence. 4th Euro-Mediterranean Symposium on Laser-Induced Breakdown Spectroscopy, 2007 (Paris, France)"
  },
  {
    "objectID": "projects/index.html",
    "href": "projects/index.html",
    "title": "R Packages",
    "section": "",
    "text": "Hotelling’s T-Squared Statistic and Ellipse\n\n\n\nR\n\n\nMachine Learning\n\n\nChemometrics\n\n\nExploratory Data Analysis\n\n\n\nThe HotellingEllipse package facilitates the comparison of multivariate datasets based on their PCA or PLS scores by computing the Hotelling’s T-squared statistic. The package also provides the semi-minor and semi-major axes for drawing…\n\n\n\nMay 18, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nComputation of 2D and 3D Elliptical Joint Confidence Regions\n\n\n\nR\n\n\nMachine Learning\n\n\nChemometrics\n\n\nClustering\n\n\n\nThe ConfidenceEllipse package calculates the coordinates of elliptical joint regions at a specified confidence level. It provides the flexibility to estimate classical or robust confidence regions, which can be visualized as 2D or 3D plots.…\n\n\n\nApr 22, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPreprocessing Tools for Spectroscopy Data Analysis\n\n\n\nR\n\n\nVisualization\n\n\nChemometrics\n\n\nPreprocessing\n\n\nExploratory Data Analysis\n\n\n\nThe specProc package performs a wide range of preprocessing tasks essential for spectroscopic data analysis. Spectral preprocessing is essential in ensuring accurate and reliable results by minimizing the impact of various distortions and…\n\n\n\nNov 1, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCRAN Package Usage App\n\n\n\nR\n\n\nVisualization\n\n\nApp\n\n\n\nShiny app that provides an interactive way to explore CRAN package usage. It enables users to display detailed download trends and compare up to 20 packages simultaneously.\n\n\n\nDec 1, 2024\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blog/posts/post2/index.html",
    "href": "blog/posts/post2/index.html",
    "title": "Multivariate Outlier Detection in High-Dimensional Spectral Data",
    "section": "",
    "text": "Photo by Will Myers.\n\n\n\n\nIn the realm of laser spectroscopy, outliers are usually characterized by values that deviate significantly from the values of other observations, abnormally high or low — for example, humidity change, spatial heterogeneity on the samples surface, drift of the instrument parameters, human error, etc. Or by a subtle change in the characteristics of the spectra not taken into account before, since calibration models are usually trained on samples with relatively “similar” chemical and physical properties — what we call matrix effects.\n\n“An outlier is an observation which deviates so much from the other observations as to arouse suspicions that it was generated by a different mechanism.” D. M. Hawkins\n\nSolutions for the former case — referred to as gross outliers or extreme values — are often straightforward: we fix what was wrong and repeat the measurements if necessary. The latter case is usually more complex and requires advanced statistical methods. However, as discussed later, these subtle changes are often very difficult to detect with traditional methods in addition to being ineffective in high dimensions. The challenge is therefore to find reliable methods which are both:\n\nFast\nRobust to outliers or contamination\nApplicable to high-dimensional data\n\nIn high-dimensional space, the data points are very sparse, so that all points are almost equidistant from each other. In other words, the use of Euclidean distances become meaningless. The result is that the degree of outlyingness of data points is indistinguishable from each other. For this reason, outliers are best detected by using a lower-dimensional local subspace in which only a subset of features may be relevant.\n\n“Outliers are often hidden in the unusual local behavior of low-dimensional subspaces, and this deviant behavior is masked by full-dimensional analysis.” C. C. Aggarwal\n\n\n\n\nStandard and widely used distance-based methods consist of computing the Mahalanobis distance. This class of methods only uses distance space to flag outlier observations. The Mahalanobis distance (MD) for the \\(i\\)-th observation is given by:\n\\[\n\\text{MD}_i = \\sqrt{(x_i-\\bar{x})^T C^{-1} (x_i-\\bar{x})} \\quad \\text{with} \\quad C = \\frac{1}{n-1}X^TX\n\\]\n\\(X\\) is the data matrix of size \\(n \\times p\\), where \\(p\\)is the number of variables and n is the number of observations. \\(x_i\\) is an observation (a row of \\(X\\)), \\(\\bar{x}\\) is the mean vector, \\(C\\) is the sample covariance matrix which gives information about the covariance structure of the data — i.e. the shape of the ellipsoid specified by the covariance matrix.\n\n\n\nFIG. 1: Robust Mahalanobis distance versus the sample (observation) number.\n\n\nTo identify outlier candidates, MD² is computed and compared to a cut-off value equal to the 0.975 quantile of the Chi-Square distributionwith m degrees of freedom, m being the number of variables. This comes from the fact that MD² of multivariate normal data follows a Chi-Square distribution. Albeit, Hardin and Rocke (2005) have reported that a better approximation is found using an F-distribution, especially for small sample sizes. Nonetheless, Chi-Square distribution is still reasonably good for approximating the squared Mahalanobis distance. Hence, an observation is considered as an outlier candidate if:\n\\[\n\\text{MD}^2 = \\chi^2_{m,0.975}\n\\]\nHowever, this approach has two major issues: (1) the arithmetic mean and the sample covariance matrix are sensitive to outliers and (2) the covariance matrix \\(X^TX\\) must be invertible — more formally non singular. The former is solved by applying robust statistics, whilst the latter is obviously a severe limitation for high-dimensional data often encountered in chemometrics where \\(p \\ggg n\\). Indeed, the same limitation holds even when one chooses to use the robust version of MD, which is derived from the Minimum Covariance Determinant (MCD) estimator, instead of the classical MD.\n\n\n\nIn chemometrics, Principal Component Analysis (PCA) is widely used for exploratory analysis and for dimensionality reduction and can be used as outlier detection method.\nIndeed, PCA score is often used in conjunction with the Mahalanobis distance (or Hotelling’s \\(T^2\\) at 95% confidence level) to determine how far an observation is from the centroid of the elliptical region that encompasses most of the observations.\n\n\n\nFIG. 2: PCA score for the LIBS spectra of plant samples with the 95% confidence limit for Hotelling’s \\(T^2\\). The transparency level of the data points shows the contribution of each sample to the first component. The darkest samples are those that contribute most.\n\n\nHowever, this approach is not robust since classical PCA relies on the sample covariance matrix C, and therefore suffers from being extremely sensitive to outliers observations, as can be seen in the above figure. The consequences of this are twofold: on one hand, the variance explained by the principal components is inflated towards the outliers direction and therefore masking observations that deviate from the regular observations — the so-called masking effect. On the other hand, regular observations might be falsely flagged as outliers — the so-called swamping effect.\n\n\n\nFIG. 3: Same PCA score but with the addition of two colors: (red) with and (blue) without measurements error. The former (70 samples) are indistinguishable from error-free measurements (369 samples), although they seem to cluster in the upper right corner which may indicate a similar error.\n\n\nHence, to avoid these effects, a number of robustified versions of PCA based upon a robust estimate of the covariance matrix (each characterized by their breakdown point), by means of S-estimator, MM-estimator, (Fast)MCD-estimator or, the re-weighted MCD- (RMCD) estimator have been proposed over the past decades. However, these methods are limited to small or moderate dimensions since the robust estimates are only applicable when the number of observations is at least twice the number of variables \\((n﹥2p)\\).\n\n\n\nFIG. 4: Robust PCA (Fast-MCD) score for the plant samples. Prior to perform robust PCA, feature selection was carried out to reduce the dimensionality. Here, we can better differentiate the measurements with (red) and without (blue) error. It turns out that the former contribute the most (dark red) to the first component.\n\n\n\n\n\nFIG. 5: 3D score plot for the plant samples. Now we can see that the red samples form two separate clusters. The first is clearly separated from error-free measurements, while the second is still difficult to discern but clusters above the regular values.\n\n\nThere is another group of robust PCA methods that have been developed and that are better suited for handling high-dimensional data in the situation where the sample size is lower than the dimension \\((p \\ggg n)\\). These methods are:\n\nRobust PCA by projection-pursuit (PP-PCA)\nSpherical PCA (SPCA)\nRobust PCA (ROBPCA)\nRobust Sparse PCA (ROSPCA)\n\nThe projection-pursuit approach to robust PCA has been initially introduced by Li and Chen (1985) and is based on finding the directions that maximize a projection index. PP-PCA uses the median absolute deviation (MAD) or Qn-estimator as projection index instead of the variance. SPCA was derived by Locantore et al. (1999) and aims to project data onto the surface of a unit sphere to alleviate outliers effect. Like PP-PCA, SPCA uses MAD or Qn-estimator as a robust estimate of spread.\nMore recently, ROBPCA and ROSPCA have been introduced by Hubert et al. 2005 and by Hubert et al. 2015. ROBPCA combines the projection pursuit approach with the robust covariance estimation based on the RMCD in a low dimensional space. Interestingly, ROBPCA is much faster than the aforementioned methods and has the advantage to be applicable to both symmetrically distributed data and skewed data (Hubert et al. 2009). On the other hand, ROSPCA is derived from ROBPCA, but uses Sparse PCA (Zou et al. 2006) instead of PCA.\n\n\n\nAfter having briefly reviewed outlier detection methods based upon robust PCA, it is worth mentioning that outliers can be classified into two categories: Leverage points and Orthogonal outliers. As the figure below illustrates, the leverage points category can be split into Good leverage points and Bad leverage points. As their name suggests, one can have a positive effect while the other has a negative effect.\n\n\n\nFIG. 6: Illustration of the different types of outliers on the 2-dimensional PCA subspace. Image credit: Christian Goueguel.\n\n\nLeverage points and orthogonal outliers are differentiated by their respective scores and orthogonal distances. These distances tell us how far an observation is from the center of the ellipse defined by the regular observations (score = 0). The score distance (SD) is a measure of the distance between an observation belonging to the \\(k\\)-dimensional PCA subspace and the origin of that subspace. The orthogonal distance (OD) measures the deviation — i.e. lack of fit — of an observation from the \\(k\\)-dimensional PCA subspace.\n\n\n\nFIG. 7: Illustration of the score and orthogonal distances. Image credit: Christian Goueguel.\n\n\nThus, leverage points are characterized by a high score distance, while orthogonal outliers are characterized by a high orthogonal distance. Likewise, good and bad leverage points are differentiated by their respective orthogonal distance. Bad leverage points display higher orthogonal distance than good leverage points.\nThe score distance for the \\(i\\)-th observation on the \\(k\\)-dimensional PCA subspace is given by:\n\\[\n\\text{SD}^2_i = \\sum^k_{j=1}{\\frac{\\text{t}^2_{ij}}{l_j}}\n\\]\nwhere \\(l\\) are the eigenvalues of the MCD scatter matrix and t are the robust scores, for each \\(j = 1, \\cdots, k\\). Like the Mahalanobis distance, the cut-off values for outlying the squared score distances are obtained from the 0.975 quantile of the Chi-Square distributionwith \\(k\\)degrees of freedom.\n\\[\n\\text{SD}^2 &gt;  \\chi^2_{k,0.975}\n\\]\nThe corresponding orthogonal distance is given by:\n\\[\n\\text{OD}_i = \\| x_i-\\hat{\\mu}_x - P\\times t_i \\|\n\\]\nwhere \\(P\\) is the loading matrix, ûₓ is the robust estimate of center. The cut-off values for the orthogonal distances are obtained using the Wilson-Hilferty approximation for a Chi-Squared distribution. As a result, the orthogonal distances, raised to the power 2/3, are approximately normally distributed. Thus, the cut-off values for outliers observations are given by:\n\\[\n\\text{OD} &gt; \\left[ \\text{median}\\left(\\text{OD}^{2/3} \\right) + Z_{0.975}\\text{MAD}\\left(\\text{OD}^{2/3} \\right)  \\right]^{3/2}\n\\]\nwhere \\(Z_{0.975}\\) is the 0.975 quantile of the Standard Normal distribution.\n\n\n\nOutlier map provides a powerful graphical tool for visually identifying the different types of outliers.\n\n\n\nFIG. 8: Illustration of outlier map.\n\n\nFor each observation in the dataset, the score distance and the orthogonal distance are plotted on the x- and y-axis. Their respective cut-off values divide the map into four quadrants:\n\nRegular observations are in the bottom left corner of the map,\nOrthogonal outliers are in the upper left corner of the map,\nLeverage points are on the right side, with good leverage points at the bottom and bad leverage points at the top.\n\n\n\n\nFIG. 9: Outlier map obtained from ROBPCA using LIBS spectra of plant samples.\n\n\nThe above figure was obtained with ROBPCA. It took 15.86 seconds of execution time (laptop, 2.80GHz) to process the data matrix of size 439×7153. We can see that most of the measurements are in the quadrant of regular observations while some are flagged as orthogonal outliers and bad leverage points. Some of them are good leverage points. On the other hand, the measurements with error are mostly orthogonal outliers, although some are considered regular observations."
  },
  {
    "objectID": "blog/posts/post2/index.html#introduction",
    "href": "blog/posts/post2/index.html#introduction",
    "title": "Multivariate Outlier Detection in High-Dimensional Spectral Data",
    "section": "",
    "text": "In the realm of laser spectroscopy, outliers are usually characterized by values that deviate significantly from the values of other observations, abnormally high or low — for example, humidity change, spatial heterogeneity on the samples surface, drift of the instrument parameters, human error, etc. Or by a subtle change in the characteristics of the spectra not taken into account before, since calibration models are usually trained on samples with relatively “similar” chemical and physical properties — what we call matrix effects.\n\n“An outlier is an observation which deviates so much from the other observations as to arouse suspicions that it was generated by a different mechanism.” D. M. Hawkins\n\nSolutions for the former case — referred to as gross outliers or extreme values — are often straightforward: we fix what was wrong and repeat the measurements if necessary. The latter case is usually more complex and requires advanced statistical methods. However, as discussed later, these subtle changes are often very difficult to detect with traditional methods in addition to being ineffective in high dimensions. The challenge is therefore to find reliable methods which are both:\n\nFast\nRobust to outliers or contamination\nApplicable to high-dimensional data\n\nIn high-dimensional space, the data points are very sparse, so that all points are almost equidistant from each other. In other words, the use of Euclidean distances become meaningless. The result is that the degree of outlyingness of data points is indistinguishable from each other. For this reason, outliers are best detected by using a lower-dimensional local subspace in which only a subset of features may be relevant.\n\n“Outliers are often hidden in the unusual local behavior of low-dimensional subspaces, and this deviant behavior is masked by full-dimensional analysis.” C. C. Aggarwal"
  },
  {
    "objectID": "blog/posts/post2/index.html#mahalanobis-distance",
    "href": "blog/posts/post2/index.html#mahalanobis-distance",
    "title": "Multivariate Outlier Detection in High-Dimensional Spectral Data",
    "section": "",
    "text": "Standard and widely used distance-based methods consist of computing the Mahalanobis distance. This class of methods only uses distance space to flag outlier observations. The Mahalanobis distance (MD) for the \\(i\\)-th observation is given by:\n\\[\n\\text{MD}_i = \\sqrt{(x_i-\\bar{x})^T C^{-1} (x_i-\\bar{x})} \\quad \\text{with} \\quad C = \\frac{1}{n-1}X^TX\n\\]\n\\(X\\) is the data matrix of size \\(n \\times p\\), where \\(p\\)is the number of variables and n is the number of observations. \\(x_i\\) is an observation (a row of \\(X\\)), \\(\\bar{x}\\) is the mean vector, \\(C\\) is the sample covariance matrix which gives information about the covariance structure of the data — i.e. the shape of the ellipsoid specified by the covariance matrix.\n\n\n\nFIG. 1: Robust Mahalanobis distance versus the sample (observation) number.\n\n\nTo identify outlier candidates, MD² is computed and compared to a cut-off value equal to the 0.975 quantile of the Chi-Square distributionwith m degrees of freedom, m being the number of variables. This comes from the fact that MD² of multivariate normal data follows a Chi-Square distribution. Albeit, Hardin and Rocke (2005) have reported that a better approximation is found using an F-distribution, especially for small sample sizes. Nonetheless, Chi-Square distribution is still reasonably good for approximating the squared Mahalanobis distance. Hence, an observation is considered as an outlier candidate if:\n\\[\n\\text{MD}^2 = \\chi^2_{m,0.975}\n\\]\nHowever, this approach has two major issues: (1) the arithmetic mean and the sample covariance matrix are sensitive to outliers and (2) the covariance matrix \\(X^TX\\) must be invertible — more formally non singular. The former is solved by applying robust statistics, whilst the latter is obviously a severe limitation for high-dimensional data often encountered in chemometrics where \\(p \\ggg n\\). Indeed, the same limitation holds even when one chooses to use the robust version of MD, which is derived from the Minimum Covariance Determinant (MCD) estimator, instead of the classical MD."
  },
  {
    "objectID": "blog/posts/post2/index.html#robust-principal-component-analysis",
    "href": "blog/posts/post2/index.html#robust-principal-component-analysis",
    "title": "Multivariate Outlier Detection in High-Dimensional Spectral Data",
    "section": "",
    "text": "In chemometrics, Principal Component Analysis (PCA) is widely used for exploratory analysis and for dimensionality reduction and can be used as outlier detection method.\nIndeed, PCA score is often used in conjunction with the Mahalanobis distance (or Hotelling’s \\(T^2\\) at 95% confidence level) to determine how far an observation is from the centroid of the elliptical region that encompasses most of the observations.\n\n\n\nFIG. 2: PCA score for the LIBS spectra of plant samples with the 95% confidence limit for Hotelling’s \\(T^2\\). The transparency level of the data points shows the contribution of each sample to the first component. The darkest samples are those that contribute most.\n\n\nHowever, this approach is not robust since classical PCA relies on the sample covariance matrix C, and therefore suffers from being extremely sensitive to outliers observations, as can be seen in the above figure. The consequences of this are twofold: on one hand, the variance explained by the principal components is inflated towards the outliers direction and therefore masking observations that deviate from the regular observations — the so-called masking effect. On the other hand, regular observations might be falsely flagged as outliers — the so-called swamping effect.\n\n\n\nFIG. 3: Same PCA score but with the addition of two colors: (red) with and (blue) without measurements error. The former (70 samples) are indistinguishable from error-free measurements (369 samples), although they seem to cluster in the upper right corner which may indicate a similar error.\n\n\nHence, to avoid these effects, a number of robustified versions of PCA based upon a robust estimate of the covariance matrix (each characterized by their breakdown point), by means of S-estimator, MM-estimator, (Fast)MCD-estimator or, the re-weighted MCD- (RMCD) estimator have been proposed over the past decades. However, these methods are limited to small or moderate dimensions since the robust estimates are only applicable when the number of observations is at least twice the number of variables \\((n﹥2p)\\).\n\n\n\nFIG. 4: Robust PCA (Fast-MCD) score for the plant samples. Prior to perform robust PCA, feature selection was carried out to reduce the dimensionality. Here, we can better differentiate the measurements with (red) and without (blue) error. It turns out that the former contribute the most (dark red) to the first component.\n\n\n\n\n\nFIG. 5: 3D score plot for the plant samples. Now we can see that the red samples form two separate clusters. The first is clearly separated from error-free measurements, while the second is still difficult to discern but clusters above the regular values.\n\n\nThere is another group of robust PCA methods that have been developed and that are better suited for handling high-dimensional data in the situation where the sample size is lower than the dimension \\((p \\ggg n)\\). These methods are:\n\nRobust PCA by projection-pursuit (PP-PCA)\nSpherical PCA (SPCA)\nRobust PCA (ROBPCA)\nRobust Sparse PCA (ROSPCA)\n\nThe projection-pursuit approach to robust PCA has been initially introduced by Li and Chen (1985) and is based on finding the directions that maximize a projection index. PP-PCA uses the median absolute deviation (MAD) or Qn-estimator as projection index instead of the variance. SPCA was derived by Locantore et al. (1999) and aims to project data onto the surface of a unit sphere to alleviate outliers effect. Like PP-PCA, SPCA uses MAD or Qn-estimator as a robust estimate of spread.\nMore recently, ROBPCA and ROSPCA have been introduced by Hubert et al. 2005 and by Hubert et al. 2015. ROBPCA combines the projection pursuit approach with the robust covariance estimation based on the RMCD in a low dimensional space. Interestingly, ROBPCA is much faster than the aforementioned methods and has the advantage to be applicable to both symmetrically distributed data and skewed data (Hubert et al. 2009). On the other hand, ROSPCA is derived from ROBPCA, but uses Sparse PCA (Zou et al. 2006) instead of PCA."
  },
  {
    "objectID": "blog/posts/post2/index.html#different-types-of-outliers",
    "href": "blog/posts/post2/index.html#different-types-of-outliers",
    "title": "Multivariate Outlier Detection in High-Dimensional Spectral Data",
    "section": "",
    "text": "After having briefly reviewed outlier detection methods based upon robust PCA, it is worth mentioning that outliers can be classified into two categories: Leverage points and Orthogonal outliers. As the figure below illustrates, the leverage points category can be split into Good leverage points and Bad leverage points. As their name suggests, one can have a positive effect while the other has a negative effect.\n\n\n\nFIG. 6: Illustration of the different types of outliers on the 2-dimensional PCA subspace. Image credit: Christian Goueguel.\n\n\nLeverage points and orthogonal outliers are differentiated by their respective scores and orthogonal distances. These distances tell us how far an observation is from the center of the ellipse defined by the regular observations (score = 0). The score distance (SD) is a measure of the distance between an observation belonging to the \\(k\\)-dimensional PCA subspace and the origin of that subspace. The orthogonal distance (OD) measures the deviation — i.e. lack of fit — of an observation from the \\(k\\)-dimensional PCA subspace.\n\n\n\nFIG. 7: Illustration of the score and orthogonal distances. Image credit: Christian Goueguel.\n\n\nThus, leverage points are characterized by a high score distance, while orthogonal outliers are characterized by a high orthogonal distance. Likewise, good and bad leverage points are differentiated by their respective orthogonal distance. Bad leverage points display higher orthogonal distance than good leverage points.\nThe score distance for the \\(i\\)-th observation on the \\(k\\)-dimensional PCA subspace is given by:\n\\[\n\\text{SD}^2_i = \\sum^k_{j=1}{\\frac{\\text{t}^2_{ij}}{l_j}}\n\\]\nwhere \\(l\\) are the eigenvalues of the MCD scatter matrix and t are the robust scores, for each \\(j = 1, \\cdots, k\\). Like the Mahalanobis distance, the cut-off values for outlying the squared score distances are obtained from the 0.975 quantile of the Chi-Square distributionwith \\(k\\)degrees of freedom.\n\\[\n\\text{SD}^2 &gt;  \\chi^2_{k,0.975}\n\\]\nThe corresponding orthogonal distance is given by:\n\\[\n\\text{OD}_i = \\| x_i-\\hat{\\mu}_x - P\\times t_i \\|\n\\]\nwhere \\(P\\) is the loading matrix, ûₓ is the robust estimate of center. The cut-off values for the orthogonal distances are obtained using the Wilson-Hilferty approximation for a Chi-Squared distribution. As a result, the orthogonal distances, raised to the power 2/3, are approximately normally distributed. Thus, the cut-off values for outliers observations are given by:\n\\[\n\\text{OD} &gt; \\left[ \\text{median}\\left(\\text{OD}^{2/3} \\right) + Z_{0.975}\\text{MAD}\\left(\\text{OD}^{2/3} \\right)  \\right]^{3/2}\n\\]\nwhere \\(Z_{0.975}\\) is the 0.975 quantile of the Standard Normal distribution."
  },
  {
    "objectID": "blog/posts/post2/index.html#outlier-map",
    "href": "blog/posts/post2/index.html#outlier-map",
    "title": "Multivariate Outlier Detection in High-Dimensional Spectral Data",
    "section": "",
    "text": "Outlier map provides a powerful graphical tool for visually identifying the different types of outliers.\n\n\n\nFIG. 8: Illustration of outlier map.\n\n\nFor each observation in the dataset, the score distance and the orthogonal distance are plotted on the x- and y-axis. Their respective cut-off values divide the map into four quadrants:\n\nRegular observations are in the bottom left corner of the map,\nOrthogonal outliers are in the upper left corner of the map,\nLeverage points are on the right side, with good leverage points at the bottom and bad leverage points at the top.\n\n\n\n\nFIG. 9: Outlier map obtained from ROBPCA using LIBS spectra of plant samples.\n\n\nThe above figure was obtained with ROBPCA. It took 15.86 seconds of execution time (laptop, 2.80GHz) to process the data matrix of size 439×7153. We can see that most of the measurements are in the quadrant of regular observations while some are flagged as orthogonal outliers and bad leverage points. Some of them are good leverage points. On the other hand, the measurements with error are mostly orthogonal outliers, although some are considered regular observations."
  },
  {
    "objectID": "blog/index.html",
    "href": "blog/index.html",
    "title": "Christian L. Goueguel",
    "section": "",
    "text": "Order By\n       Default\n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Title\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nAn Overview of Orthogonal Partial Least Squares\n\n\n\n\n\nHave you ever heard of Orthogonal Partial Least-Squares (OPLS)? This article aims to give you a clear and concise overview of OPLS and its advantages in the development of more efficient predictive models.\n\n\n\n\n\nDec 20, 2019\n\n\nChristian L. Goueguel\n\n\n6 min\n\n\n\n\n\n\n\n\n\n\n\n\nBeyond Standard Boxplot: The Adjusted and Generalized Boxplots\n\n\n\n\n\nBoxplots, also known as box-and-whisker plots, have been a cornerstone of data visualization since their introduction by John Tukey in the late 1970s. Despite their enduring utility, boxplot assumes a symmetrical mesokurtic distribution and might misrepresent datasets with skewness or heavy tails. Alternative approaches have been proposed to address these limitations.\n\n\n\n\n\nDec 10, 2024\n\n\nChristian L. Goueguel\n\n\n16 min\n\n\n\n\n\n\n\n\n\n\n\n\nChemometric Modeling with Tidymodels: A Tutorial for Spectroscopic Data\n\n\n\n\n\nIn this post, we demonstrate how to build robust chemometric models for spectroscopic data using the Tidymodels framework in R. This workflow is designed to cater to beginners and advanced practitioners alike, offering an end-to-end guide from data preprocessing to model evaluation and interpretation.\n\n\n\n\n\nApr 17, 2022\n\n\nChristian L. Goueguel\n\n\n10 min\n\n\n\n\n\n\n\n\n\n\n\n\nExploring Three Orthogonal Signal Correction (OSC) Algorithms\n\n\n\n\n\nOrthogonal signal correction (OSC) is a powerful preprocessing technique frequently used to remove variation in spectral data that is orthogonal to the property of interest. Over the years, several implementations of OSC have emerged, with the most notable being those by Wold et al., Sjöblom et al., and Fearn. This post compares these three methods, exploring their algorithmic approaches and practical implications.\n\n\n\n\n\nMay 25, 2023\n\n\nChristian L. Goueguel\n\n\n19 min\n\n\n\n\n\n\n\n\n\n\n\n\nMultivariate Outlier Detection in High-Dimensional Spectral Data\n\n\n\n\n\nHigh-dimensional data are particularly challenging for outlier detection. Robust PCA methods have been developed to build models that are unaffected by outliers in high dimensions. These outliers are generally characterized by their deviation from the PCA subspace.\n\n\n\n\n\nJan 7, 2020\n\n\nChristian L. Goueguel\n\n\n9 min\n\n\n\n\n\n\n\n\n\n\n\n\nOptical Breakdown Threshold in Water\n\n\n\n\n\nCascade (or avalanche) ionization and multiphoton ionization are the two primary mechanisms responsible for laser-induced plasma (LIP) formation in water. These absorption processes are influenced by the intensity of the laser pulse and the physical and chemical properties of the water itself. This post focuses on providing a concise overview of these key mechanisms.\n\n\n\n\n\nSep 7, 2022\n\n\nChristian L. Goueguel\n\n\n8 min\n\n\n\n\n\n\n\n\n\n\n\n\nThe Pseudo-Voigt Function\n\n\n\n\n\nIn spectroscopy, especially laser spectroscopy, accurate modeling of spectral line shapes is essential for analyzing the physical and chemical properties of matter. A commonly used approximation is the pseudo-Voigt function, which serves as a simplified representation of the Voigt profile. The Voigt profile, defined as the convolution of a Gaussian function and a Lorentzian function, accurately describes the line shapes, but its calculation is often time consuming.\n\n\n\n\n\nApr 5, 2019\n\n\nChristian L. Goueguel\n\n\n14 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "cv.html",
    "href": "cv.html",
    "title": "Curriculum Vitae",
    "section": "",
    "text": ":::"
  },
  {
    "objectID": "talks.html#oral-presentations",
    "href": "talks.html#oral-presentations",
    "title": "Christian L. Goueguel",
    "section": "Oral Presentations",
    "text": "Oral Presentations\n\nC. L. Goueguel, Direct determination of soil texture using laser-induced breakdown spectroscopy and multivariate linear regressions. Annual meetings of the North American Society for Laser-Induced Breakdown Spectroscopy (NASLIBS) hosted by FACSS SciX Conference 2019 (Palm Springs, CA, USA)\nC. L. Goueguel, Using LIBS analyzer for laser-based soil analysis for precision agriculture. Invited speaker at the Canadian Society for Analytical Sciences and Spectroscopy, Jan. 1, 2018 (Toronto, ON, Canada)\nJ. C. Jain, C. L. Goueguel, D. L. McIntyre, H. M. Edenborn, Dissolution of mineral carbonates with increasing CO2 pressure and the implications for carbon sequestration leak detection. GSA annual meeting, 2016 (Denver, CO, USA)\nC. G. Carson, C. Goueguel, J. Jain, D. McIntyre, Development of laser induced breakdown spectroscopy sensor to assess groundwater quality impacts resulting from geologic carbon sequestration. SPIE Micro- and Nanotechnology Sensors, Systems, and Applications VII, 2015 (Baltimore, MD, USA)\nC. L. Goueguel, J. C. Jain, D. L. McIntyre, A. K. Karamalidis, Application of laser induced breakdown spectroscopy (LIBS) to analyze CO2-bearing solutions in high pressure high temperature environment. ACS 248th - National Meeting, 2014 (San Francisco, CA, USA)\nC. L. Goueguel, J. C. Jain, A. K. Karamalidis, D. L. McIntyre, J. P. Singh, Laser-induced breakdown spectroscopy of high-pressure carbonated brine solutions. 2014 PITTCON Conference & Expo, 2014 (Chicago, IL, USA)\nJ. P. Singh, C L. Goueguel, D. L. McIntyre, J. C. Jain, A. K. Karamalidis, Laser-induced breakdown spectroscopy for elemental analysis of brines. 7th International Conference on Laser-Induced Breakdown Spectroscopy, 2012 (Luxor, Egypt)\nC. Goueguel, Laser Induced Breakdown Spectroscopy (LIBS): From Mars exploration to GCS monitoring. Postdoctoral Seminar at the Department of Civil and Environmental Engineering, Carnegie Mellon University, 2012 (Pittsburgh, PA, USA)\nC. Goueguel, S. Laville, F. Vidal, M. Sabsabi, Chaker, Investigation of resonance-enhanced laser-induced breakdown spectroscopy (RELIBS) for the analysis of aluminum alloys, PITTCON Conference & Expo, 2010 (Orlando, FL, USA)\nC. Goueguel, S. Laville, H. Loudyi, F. Vidal, M. Sabsabi, M. Chaker, Investigation of resonance-enhanced laser-induced breakdown spectroscopy for the analysis of aluminum alloys. 2nd North American Symposium on Laser-Induced Breakdown Spectroscopy, 2009 (New Orleans, LA, USA)\nC. Goueguel, H. Loudyi, S. Laville, M. Chaker, M. Sabsabi, F. Vidal, Excitation spectrale sélective d’un plasma d’aluminium par une seconde impulsion laser: Optimisation des paramètres expérimentaux. Colloque Plasma- Québec, 2009 (Montreal, QC, Canada)\nS. Laville, K. Rifai, H. Loudyi, M. Chaker, M. Sabsabi, F. Vidal, C. Goueguel, Analysis of trace elements in liquids using LIBS combined with laser-induced fluorescence. 5th International Conference on Laser-Induced Breakdown Spectroscopy, 2008 (Berlin, Germany)\nF. Vidal, M. Chaker, C. Goueguel, S. Laville, H. Loudyi, K. Rifai, M. Sabsabi, Enhanced laser-induced breakdown spectroscopy by second pulse selective wavelength excitation. Laser and plasma applications in materials science: 1st International Conference on Laser Plasma Applications in Materials Science, LAPAMS, 2008 (Alger, Algeria)\nC. Goueguel, S. Laville, H. Loudyi, M. Chaker, M. Sabsabi, F. Vidal. Detection of lead in brass by Laser-Induced Breakdown Spectroscopy combined with Laser-Induced Fluorescence. Photonics North, 2008 (Montreal, QC, Canada)\nH. Loudyi, C. Goueguel, K. Rifai, S. Laville, M. Sabsabi, M. Chaker, F. Vidal, Enhancing the sensitivity of the laser-Induced Breakdown Spectroscopy technique: spectrally selective excitation of specific elements in laser produced plasma. Canadian Association of Physicists Congress, 2008 (Laval, QC, Canada)\nC. Goueguel, H. Loudyi, S. Laville, M. Chaker, M. Sabsabi, F. Vidal, Analyse des matériaux solides par LIBS: Effet d’une deuxième impulsion laser basée sur une excitation spectrale sélective. Colloque Plasma-Québec, 2008 (Montreal, QC, Canada)\nF. Vidal, S. Laville, C. Goueguel, H. Loudyi, M. Chaker, M. Sabsabi, Investigation of laser-induced breakdown spectroscopy combined with laser- induced fluorescence. 4th Euro-Mediterranean Symposium on Laser-Induced Breakdown Spectroscopy, 2007 (Paris, France)"
  },
  {
    "objectID": "talks.html#poster-presentations",
    "href": "talks.html#poster-presentations",
    "title": "Christian L. Goueguel",
    "section": "Poster Presentations",
    "text": "Poster Presentations\n\nC. L. Goueguel, D. L. McIntyre, J. C. Jain, J. P. Singh, A. K. Karamalidis, Analysis of calcium in CO2-laden brine (NaCl-CaCl2) by Laser-induced breakdown spectroscopy (LIBS). Annual meetings of the North American Society for Laser-Induced Breakdown Spectroscopy (NASLIBS) hosted by FACSS SciX Conference 2013 (Milwaukee, WI, USA)\nC. Goueguel, S. Laville, F. Vidal, M. Sabsabi, M. Chaker, Investigation of resonance-enhanced laser-induced breakdown spectroscopy (RELIBS) for the analysis of aluminum alloys. 6th International Conference on Laser- Induced Breakdown Spectroscopy, 2010 (Memphis, TN, USA)"
  },
  {
    "objectID": "blog/posts/post4/index.html",
    "href": "blog/posts/post4/index.html",
    "title": "Exploring Three Orthogonal Signal Correction (OSC) Algorithms",
    "section": "",
    "text": "Photo by Jonatan Pie.\n\n\n\n\nWold’s method was the first formal OSC algorithm. It operates iteratively to identify orthogonal components unrelated to the dependent variable \\(Y\\). The method leverages a combination of principal component analysis (PCA) and partial least squares (PLS). Sjöblom’s approach builds on Wold’s by introducing a direct orthogonalization step. The algorithm emphasizes calibration transfer, making it especially useful for standardizing spectral datasets across instruments or conditions. Whereas, Fearn proposed a mathematically elegant version of OSC, simplifying the computation by leveraging matrix operations. The method directly orthogonalizes \\(X\\) using a singular value decomposition (SVD) of a residual matrix.\n\n\nThe Wold algorithm is like a precise sculptor of spectroscopic data. It uses Partial Least Squares (PLS) regression to systematically remove spectral variations that are unrelated to the target variable. The key steps involve:\nInitialize \\(t\\), the first score vector (e.g., using PCA on \\(X\\)).\n\nDeflate \\(t\\) using \\(Y\\): \\(t_{\\text{new}} = t - Y(Y^\\top Y)^{-1}Y^\\top t\\)\nCalculate a loading vector \\(p\\) from \\(t_{\\text{new}}\\) to model \\(X\\): \\(p = \\frac{X^\\top t_{\\text{new}}}{t_{\\text{new}}^\\top t_{\\text{new}}}\\)\nDeflate \\(X\\): \\(X_{\\text{new}} = X - t_{\\text{new}} p^\\top\\)\nRepeat until \\(n_{\\text{comp}}\\).\n\n\n\nShow the code\nwold_osc &lt;- function(X, Y, ncomp = 1, tol = 1e-6, max.iter = 100) {\n  # Ensure X and Y are matrices\n  X &lt;- as.matrix(X)\n  Y &lt;- as.matrix(Y)\n\n  # Store the original X matrix\n  X_original &lt;- X\n\n  # Initialize lists to store components\n  scores &lt;- list()\n  loadings &lt;- list()\n  weights &lt;- list()\n\n  for (comp in seq_len(ncomp)) {\n    # Step 1: Initial PCA on X to get the first principal component score vector (t)\n    t &lt;- svd(X, nu = 1, nv = 0)$u * svd(X, nu = 1, nv = 0)$d[1]\n\n    # Step 2: Orthogonalize t with respect to Y to obtain t*\n    t_star &lt;- t - Y %*% MASS::ginv(crossprod(Y, Y)) %*% crossprod(Y, t)\n\n    iter &lt;- 0\n    diff &lt;- tol + 1\n\n    while (diff &gt; tol && iter &lt; max.iter) {\n      iter &lt;- iter + 1\n\n      # Step 3: Compute weights (w) to make Xw as close as possible to t*\n      w &lt;- crossprod(X, t_star) / sum(t_star^2)\n      w &lt;- w / sqrt(sum(w^2))  # Normalize the weights\n\n      # Step 4: Update t as Xw\n      t_new &lt;- X %*% w\n\n      # Step 5: Orthogonalize t_new with respect to Y\n      t_star &lt;- t_new - Y %*% MASS::ginv(crossprod(Y, Y)) %*% crossprod(Y, t_new)\n\n      # Compute convergence criterion\n      diff &lt;- sqrt(sum((t_star - t)^2)) / sqrt(sum(t_star^2))\n\n      # Update t for the next iteration\n      t &lt;- t_star\n    }\n\n    if (iter == max.iter) {\n      warning(\"Iteration limit reached without convergence.\")\n    }\n\n    # Step 6: Compute the loading vector (p)\n    p &lt;- crossprod(X, t_star) / sum(t_star^2)\n\n    # Step 7: Deflate X\n    X &lt;- X - t_star %*% t(p)\n\n    # Store results\n    scores[[comp]] &lt;- t_star\n    loadings[[comp]] &lt;- p\n    weights[[comp]] &lt;- w\n  }\n\n  # Combine components into matrices\n  T_star &lt;- do.call(cbind, scores)\n  P &lt;- do.call(cbind, loadings)\n  W &lt;- do.call(cbind, weights)\n\n  # Calculate the filtered X matrix\n  X_filtered &lt;- X_original - T_star %*% t(P)\n\n  # Return results as a list\n  return(list(\n    scores = T_star,\n    loadings = P,\n    weights = W,\n    X_filtered = X_filtered\n  ))\n}\n\n\n\n\n\nSjöblom’s approach is the pragmatic cousin of the Wold method. It uses similar steps but simplifies certain iterative aspects, focusing on the orthogonal direction more explicitly.\n\nIdentify a direction vector \\(w\\) from \\(X\\) and \\(t\\), the orthogonal scores \\(w = \\frac{X^\\top t}{t^\\top t}\\)​\nNormalize \\(w\\): \\(w = \\frac{w}{\\|w\\|}\\)​\nDeflate \\(t\\) from \\(Y\\) as in Wold’s method.\nRemove the orthogonal variation from \\(X\\): \\(X_{\\text{new}} = X - t p^\\top\\)\nIterate for each component.\n\n\n\nShow the code\nsjoblom_osc &lt;- function(x, y, ncomp, tol, max.iter) {\n  x_original &lt;- x\n  ps &lt;- ws &lt;- ts &lt;- vector(\"list\", ncomp)\n  for (i in seq_len(ncomp)) {\n    pc &lt;- stats::prcomp(x, center = FALSE)\n    t &lt;- pc$x[, 1]\n    .diff &lt;- 1\n    .iter &lt;- 0\n    while (.diff &gt; tol && .iter &lt; max.iter) {\n      .iter &lt;- .iter + 1\n      t_new &lt;- t - y %*% MASS::ginv(crossprod(y, y)) %*% crossprod(y, t)\n      w &lt;- crossprod(x, t_new) %*% MASS::ginv(crossprod(t_new, t_new))\n      w &lt;- w / sqrt(sum(w^2))\n      t_new &lt;- x %*% w\n      .diff &lt;- sqrt(sum((t_new - t)^2) / sum(t_new^2))\n      t &lt;- t_new\n    }\n    plsFit &lt;- pls::simpls.fit(x, t, ncomp)\n    w &lt;- plsFit$coefficients[ , , ncomp]\n    t &lt;- x %*% w\n    t &lt;- t - y %*% MASS::ginv(crossprod(y, y)) %*% crossprod(y, t)\n    p &lt;- crossprod(x, t) %*% MASS::ginv(crossprod(t, t))\n    x &lt;- x - tcrossprod(t, p)\n    ws[[i]] &lt;- w\n    ps[[i]] &lt;- p\n    ts[[i]] &lt;- t\n  }\n  w_ortho &lt;- do.call(cbind, ws)\n  p_ortho &lt;- do.call(cbind, ps)\n  t_ortho &lt;- do.call(cbind, ts)\n  x_osc &lt;- x_original - x_original %*% tcrossprod(w_ortho, p_ortho)\n\n  R2 &lt;- sum(x_osc^2) / sum(x_original^2) * 100\n  angle &lt;- crossprod(t_ortho, y)\n  norm &lt;- MASS::ginv(sqrt(apply(t_ortho^2, 2, sum) * sum(y^2)))\n  angle &lt;- t(angle) %*% t(norm)\n  angle &lt;- mean(acos(angle) * 180 / pi)\n\n  res &lt;- list(\n    \"correction\" = tibble::as_tibble(x_osc),\n    \"weights\" = tibble::as_tibble(w_ortho),\n    \"scores\" = tibble::as_tibble(t_ortho),\n    \"loadings\" = tibble::as_tibble(p_ortho),\n    \"angle\" = angle,\n    \"R2\" = R2\n  )\n  return(res)\n}\n\n\n\n\n\nFearn’s method stands out by using Singular Value Decomposition (SVD) as its foundation. Its characteristics include:\n\nCompute the residual matrix \\(Z\\): \\(Z = X - Y (Y^\\top Y)^{-1} Y^\\top X\\)\nPerform SVD on \\(Z\\): \\(Z = U S V^\\top\\)\nExtract the first \\(n_{\\text{comp}}\\) components from \\(V\\) and reconstruct the orthogonal scores \\(t\\) and loadings \\(p\\): \\(t = Z V_{:, i}, \\quad p = \\frac{X^\\top t}{t^\\top t}\\)\nDeflate \\(X\\): \\(X_{\\text{new}} = X - t p^\\top\\)\n\n\n\nShow the code\nfearn_osc &lt;- function(x, y, ncomp, tol, max.iter) {\n  x_original &lt;- x\n  ps &lt;- ws &lt;- ts &lt;- vector(\"list\", ncomp)\n  m &lt;- diag(row(x)) - crossprod(x, y) %*% MASS::ginv(crossprod(y, x) %*% crossprod(x, y)) %*% crossprod(y, x)\n  z &lt;- x %*% m\n  decomp &lt;- svd(t(z))\n  u &lt;- decomp$u\n  s &lt;- decomp$d\n  v &lt;- decomp$v\n  g &lt;- diag(s[1:ncomp])\n  c &lt;- v[, 1:ncomp, drop = FALSE]\n\n  for (i in seq_len(ncomp)) {\n    w_old &lt;- rep(0, ncol(x))\n    w_new &lt;- rep(1, ncol(x))\n    dif &lt;- 1\n    iter &lt;- 0\n    while (dif &gt; tol && iter &lt; max.iter) {\n      iter &lt;- iter + 1\n      w_old &lt;- w_new\n      t_new &lt;- c[, i] %*% g[i, i]\n      p_new &lt;- tcrossprod(x, t_new) / tcrossprod(t_new, t_new)\n      w_new &lt;- m %*% tcrossprod(x, p_new)\n      dif &lt;- sqrt(sum((w_new - w_old)^2) / sum(w_new^2))\n    }\n    ws[[i]] &lt;- w_new\n    ts[[i]] &lt;- c[, i] %*% g[i, i]\n    ps[[i]] &lt;- tcrossprod(x, t[[i]]) / tcrossprod(t[[i]], t[[i]])\n  }\n  w_ortho &lt;- do.call(cbind, ws)\n  t_ortho &lt;- do.call(cbind, ts)\n  p_ortho &lt;- do.call(cbind, ps)\n  x_osc &lt;- x - tcrossprod(t_ortho, p_ortho)\n\n  R2 &lt;- sum(x_osc^2) / sum(x_original^2) * 100\n  angle &lt;- crossprod(t_ortho, y)\n  norm &lt;- MASS::ginv(sqrt(apply(t_ortho^2, 2, sum) * sum(y^2)))\n  angle &lt;- t(angle) %*% t(norm)\n  angle &lt;- mean(acos(angle) * 180 / pi)\n\n  res &lt;- list(\n    \"correction\" = tibble::as_tibble(x_osc),\n    \"weights\" = tibble::as_tibble(w_ortho),\n    \"scores\" = tibble::as_tibble(t_ortho),\n    \"loadings\" = tibble::as_tibble(p_ortho),\n    \"angle\" = angle,\n    \"R2\" = R2\n  )\n  return(res)\n}\n\n\n\n\n\n\nWe begin by implementing these algorithms, creating functions named wold_osc, sjoblom_osc, and fearn_osc. Each function takes five key parameters. The first parameter, x, represents the input data matrix, which typically contains spectral or chemical measurements. The second parameter, y, corresponds to the target variable or response vector. The ncomp parameter specifies the number of orthogonal components to extract, while tol sets the tolerance level for convergence, determining the stopping criterion for iterations. Finally, max.iter establishes the maximum number of iterations allowed during the optimization process. The function definitions for these algorithms follow this structure:\nwold_osc &lt;- function(x, y, ncomp, tol, max.iter)\nsjoblom_osc &lt;- function(x, y, ncomp, tol, max.iter)\nfearn_osc &lt;- function(x, y, ncomp, tol, max.iter)\nTo begin, the original data matrix x is stored, and empty lists are initialized to hold the extracted principal components, weights, and scores for each component. This step ensures that the algorithm’s outputs are organized for further processing or analysis:\nx_original &lt;- x\nps &lt;- ws &lt;- ts &lt;- vector(\"list\", ncomp)\nThe algorithm proceeds with a loop to extract the specified number of orthogonal components. For each iteration, Principal Component Analysis (PCA) is performed on the current x matrix without centering, using the stats::prcomp function. The initial score vector t is derived from the first principal component. Variables .iter and .diff are initialized to track the number of iterations and the difference between successive score vectors, which serves as the convergence criterion.\nWithin the loop, the orthogonalization process begins. Variation correlated with the response variable y is iteratively removed from the score vector t, refining its orthogonality. Weights, representing the relationship between the input matrix x and the score vector, are calculated and normalized to unit length. A new score vector is then computed, and the convergence check compares the difference between successive score vectors (.diff) to the tolerance level (tol). The loop continues until the difference falls below the specified tolerance or the maximum number of iterations is reached.\nwhile (.diff &gt; tol && .iter &lt; max.iter) {\n  .iter &lt;- .iter + 1\n  t_new &lt;- t - y %*% MASS::ginv(crossprod(y, y)) %*% crossprod(y, t)\n  w &lt;- crossprod(x, t_new) %*% MASS::ginv(crossprod(t_new, t_new))\n  w &lt;- w / sqrt(sum(w^2))\n  t_new &lt;- x %*% w\n  .diff &lt;- sqrt(sum((t_new - t)^2) / sum(t_new^2))\n  t &lt;- t_new\n}\nAfter achieving convergence, a Partial Least Squares (PLS) model is fitted to the data using the extracted scores. The weights and scores are updated, and the loadings are computed. At this stage, y-correlated variation is removed, and the input matrix x is deflated by subtracting the modeled variation. This step prepares the matrix for the next orthogonal component extraction.\nThe extracted weights, loadings, and scores for each orthogonal component are stored in their respective lists:\nws[[i]] &lt;- w\nps[[i]] &lt;- p\nts[[i]] &lt;- t\nOnce all components are extracted, the results are combined to construct the orthogonal components matrix. The orthogonally corrected matrix x_osc is then computed by removing the contributions of the orthogonal components from the original data matrix:\nx_osc &lt;- x_original - x_original %*% tcrossprod(w_ortho, p_ortho)\nFinally, to evaluate the algorithm’s performance, two metrics are computed. The percentage of variation removed R2 quantifies how effectively the algorithm deflates the input matrix, while the angle between the orthogonal scores and the target variable y provides insight into the degree of orthogonality achieved. These metrics allow us to assess the quality and effectiveness of the orthogonal signal correction methods.\n\n\n\nWe will use the beer dataset introduced in our previous post. The dataset consists of Near-Infrared Spectroscopy (NIRS) spectra collected from 80 beer samples, x_matrix. The target variable of interest is the Original Gravity (OG), also known as the original extract, y_target. This parameter measures the concentration of dissolved solids in the wort before fermentation begins, providing a crucial indicator of the brewing process.\n\n\nShow the code\nbeer &lt;- readr::read_csv(\"beer.csv\", show_col_types = FALSE) |&gt; dplyr::rename(extract = originalGravity)\nx_matrix &lt;- beer |&gt; dplyr::select(-extract) |&gt; as.matrix() |&gt; scale(scale = FALSE) \ny_target &lt;- beer |&gt; dplyr::pull(extract)\n\n\nNext, we will perform standard PCA and PLS on the NIRS spectra of the beer dataset to assess how applying orthogonal correction modifies the data structure.\n\n\nShow the code\nset.seed(123)\nn_samples &lt;- as.integer(nrow(x_matrix))\n# PCA\npca_result &lt;- stats::prcomp(x_matrix, center = FALSE, scale = FALSE)\npca_scores &lt;- pca_result$x\npca_df &lt;- data.frame(\n  comp1 = pca_scores[, 1],\n  comp2 = pca_scores[, 2],\n  extract = y_target,\n  Sample = factor(n_samples)\n)\n\n# PLS\npls_result &lt;- pls::plsr(y_target ~ x_matrix, ncomp = 10, validation = \"none\")\npls_scores &lt;- pls::scores(pls_result)[, 1:10]\npls_df &lt;- data.frame(\n  comp1 = pls_scores[, 1],\n  comp2 = pls_scores[, 2],\n  extract = y_target,\n  Sample = factor(n_samples)\n)\n\n# Hotelling's T-squared ellipse\npca_T2ellipse &lt;- HotellingEllipse::ellipseParam(pca_scores, k = 2, pcx = 1, pcy = 2)\npls_T2ellipse &lt;- HotellingEllipse::ellipseParam(pls_scores, k = 2, pcx = 1, pcy = 2)\n\n\n\n\n\n\n\n\n\n\n\nNow, we can apply the OSC algorithms to the dataset and compare their impact. Specifically, we’ll examine how the OSC filtering affects the distribution of samples in the reduced-dimensionality space and whether the variation captured aligns better with the response variable.\n\n\nShow the code\nwold_filter &lt;- wold_osc(x_matrix, y_target, ncomp = 10, tol = 1e10, max.iter = 10)\nsjoblom_filter &lt;- sjoblom_osc(x_matrix, y_target, ncomp = 10, tol = 1, max.iter = 10)\n\nwold_scores &lt;- wold_filter %&gt;%\n  pluck(\"scores\") %&gt;%\n  as_tibble() %&gt;%\n  mutate(extract = y_target)\n\nsjoblom_scores &lt;- sjoblom_filter %&gt;%\n  pluck(\"scores\") %&gt;%\n  as_tibble() %&gt;%\n  mutate(extract = y_target)\n\n\n\n\n\n\n\n\n\n\n\nThe following plot compares the results of OSC, PCA, and PLS modeling by overlaying their respective loadings on a single graph. By presenting their loadings together, we can clearly observe the differences in how each method captures and prioritizes spectral features and filter noise, highlighting their unique contributions and areas of overlap.\n\n\nShow the code\npca_loadings &lt;- pca_result %&gt;% \n  pluck(\"rotation\") %&gt;%\n  as_tibble() %&gt;%\n  mutate(wavelength = rep(seq(1100, 2250, 2), times = 1))\n\npls_loadings &lt;- loadings(pls_result)[, 1:10] %&gt;%\n  as_tibble() %&gt;%\n  janitor::clean_names() %&gt;%\n  mutate(wavelength = rep(seq(1100, 2250, 2), times = 1))\n\nwold_loadings &lt;- wold_filter %&gt;% \n  pluck(\"loadings\") %&gt;%\n  as_tibble() %&gt;%\n  mutate(wavelength = rep(seq(1100, 2250, 2), times = 1))\n  \nsjoblom_loadings &lt;- sjoblom_filter %&gt;% \n  pluck(\"loadings\") %&gt;%\n  mutate(wavelength = rep(seq(1100, 2250, 2), times = 1))\n\n\n\n\n\n\n\n\n\n\n\nUnlike PCA (in red) and PLS (in dark green), which show substantial variability, particularly in the region above 1350 nm, OSC (in blue) effectively filters the noise, reducing it to near-zero levels. This smoothing demonstrates OSC’s ability to isolate and preserve only the information strongly correlated with the target variable while systematically discarding irrelevant or orthogonal components. However, its aggressive filtering comes at the expense of potentially reducing some useful signal.\nThe PCA and PLS loadings, on the other hand, display pronounced fluctuations, reflecting their sensitivity to variance within the dataset. PCA, focusing solely on maximizing variance without considering the target variable, captures not only relevant features but also substantial noise. PLS, while more targeted as it incorporates the correlation with the target variable, still exhibits residual noise, especially in the higher wavelengths, indicating its partial retention of irrelevant variance."
  },
  {
    "objectID": "blog/posts/post4/index.html#wolds-osc-algorithm",
    "href": "blog/posts/post4/index.html#wolds-osc-algorithm",
    "title": "A Comparative Exploration of Orthogonal Signal Correction Methods",
    "section": "",
    "text": "The Wold algorithm is like a precise sculptor of spectroscopic data. It uses Partial Least Squares (PLS) regression to systematically remove spectral variations that are unrelated to the target variable. The key steps involve:\nInitialize \\(t\\), the first score vector (e.g., using PCA on \\(X\\)).\n\nDeflate \\(t\\) using \\(Y\\): \\(t_{\\text{new}} = t - Y(Y^\\top Y)^{-1}Y^\\top t\\)\nCalculate a loading vector \\(p\\) from \\(t_{\\text{new}}\\) to model \\(X\\): \\(p = \\frac{X^\\top t_{\\text{new}}}{t_{\\text{new}}^\\top t_{\\text{new}}}\\)\nDeflate \\(X\\): \\(X_{\\text{new}} = X - t_{\\text{new}} p^\\top\\)\nRepeat until \\(n_{\\text{comp}}\\)."
  },
  {
    "objectID": "blog/posts/post4/index.html#sjöbloms-osc-algorithm",
    "href": "blog/posts/post4/index.html#sjöbloms-osc-algorithm",
    "title": "A Comparative Exploration of Orthogonal Signal Correction Methods",
    "section": "",
    "text": "Sjöblom’s approach is the pragmatic cousin of the Wold method. It uses similar steps but simplifies certain iterative aspects, focusing on the orthogonal direction more explicitly.\n\nIdentify a direction vector \\(w\\) from \\(X\\) and \\(t\\), the orthogonal scores \\(w = \\frac{X^\\top t}{t^\\top t}\\)​\nNormalize \\(w\\): \\(w = \\frac{w}{\\|w\\|}\\)​\nDeflate \\(t\\) from \\(Y\\) as in Wold’s method.\nRemove the orthogonal variation from \\(X\\): \\(X_{\\text{new}} = X - t p^\\top\\)\nIterate for each component."
  },
  {
    "objectID": "blog/posts/post4/index.html#fearns-osc-algorithm",
    "href": "blog/posts/post4/index.html#fearns-osc-algorithm",
    "title": "A Comparative Exploration of Orthogonal Signal Correction Methods",
    "section": "",
    "text": "Fearn’s method stands out by using Singular Value Decomposition (SVD) as its foundation. Its characteristics include:\n\nCompute the residual matrix \\(Z\\): \\(Z = X - Y (Y^\\top Y)^{-1} Y^\\top X\\)\nPerform SVD on \\(Z\\): \\(Z = U S V^\\top\\)\nExtract the first \\(n_{\\text{comp}}\\) components from \\(V\\) and reconstruct the orthogonal scores \\(t\\) and loadings \\(p\\): \\(t = Z V_{:, i}, \\quad p = \\frac{X^\\top t}{t^\\top t}\\)\nDeflate \\(X\\): \\(X_{\\text{new}} = X - t p^\\top\\)\n\nWe begin by implementing these algorithms, creating functions named wold_osc, sjoblom_osc, and fearn_osc. Each function takes five key parameters. The first parameter, x, represents the input data matrix, which typically contains spectral or chemical measurements. The second parameter, y, corresponds to the target variable or response vector. The ncomp parameter specifies the number of orthogonal components to extract, while tol sets the tolerance level for convergence, determining the stopping criterion for iterations. Finally, max.iter establishes the maximum number of iterations allowed during the optimization process. The function definitions for these algorithms follow this structure:\nwold_osc &lt;- function(x, y, ncomp, tol, max.iter)\nsjoblom_osc &lt;- function(x, y, ncomp, tol, max.iter)\nfearn_osc &lt;- function(x, y, ncomp, tol, max.iter)\nTo begin, the original data matrix x is stored, and empty lists are initialized to hold the extracted principal components, weights, and scores for each component. This step ensures that the algorithm’s outputs are organized for further processing or analysis:\nx_original &lt;- x\nps &lt;- ws &lt;- ts &lt;- vector(\"list\", ncomp)\nThe algorithm proceeds with a loop to extract the specified number of orthogonal components. For each iteration, Principal Component Analysis (PCA) is performed on the current x matrix without centering, using the stats::prcomp function. The initial score vector t is derived from the first principal component. Variables .iter and .diff are initialized to track the number of iterations and the difference between successive score vectors, which serves as the convergence criterion.\nWithin the loop, the orthogonalization process begins. Variation correlated with the response variable y is iteratively removed from the score vector t, refining its orthogonality. Weights, representing the relationship between the input matrix x and the score vector, are calculated and normalized to unit length. A new score vector is then computed, and the convergence check compares the difference between successive score vectors (.diff) to the tolerance level (tol). The loop continues until the difference falls below the specified tolerance or the maximum number of iterations is reached.\nwhile (.diff &gt; tol && .iter &lt; max.iter) {\n  .iter &lt;- .iter + 1\n  t_new &lt;- t - y %*% MASS::ginv(crossprod(y, y)) %*% crossprod(y, t)\n  w &lt;- crossprod(x, t_new) %*% MASS::ginv(crossprod(t_new, t_new))\n  w &lt;- w / sqrt(sum(w^2))\n  t_new &lt;- x %*% w\n  .diff &lt;- sqrt(sum((t_new - t)^2) / sum(t_new^2))\n  t &lt;- t_new\n}\nAfter achieving convergence, a Partial Least Squares (PLS) model is fitted to the data using the extracted scores. The weights and scores are updated, and the loadings are computed. At this stage, y-correlated variation is removed, and the input matrix x is deflated by subtracting the modeled variation. This step prepares the matrix for the next orthogonal component extraction.\nThe extracted weights, loadings, and scores for each orthogonal component are stored in their respective lists:\nws[[i]] &lt;- w\nps[[i]] &lt;- p\nts[[i]] &lt;- t\nOnce all components are extracted, the results are combined to construct the orthogonal components matrix. The orthogonally corrected matrix x_osc is then computed by removing the contributions of the orthogonal components from the original data matrix:\nx_osc &lt;- x_original - x_original %*% tcrossprod(w_ortho, p_ortho)\nFinally, to evaluate the algorithm’s performance, two metrics are computed. The percentage of variation removed R2 quantifies how effectively the algorithm deflates the input matrix, while the angle between the orthogonal scores and the target variable y provides insight into the degree of orthogonality achieved. These metrics allow us to assess the quality and effectiveness of the orthogonal signal correction methods."
  },
  {
    "objectID": "blog/posts/post4/index.html#key-differences-among-the-methods",
    "href": "blog/posts/post4/index.html#key-differences-among-the-methods",
    "title": "A Comparative Exploration of Orthogonal Signal Correction Methods",
    "section": "",
    "text": "Aspect\nWold’s OSC\nSjöblom’s OSC\nFearn’s OSC\n\n\n\n\nComputation\nIterative\nuses PLS.\nSimplified iterative approach.\nSVD-based; avoids iteration.\n\n\nFocus\nGeneral orthogonal correction.\nOrthogonal deflation with efficiency.\nDirect analysis of residuals.\n\n\nComplexity\nModerate\nrequires tuning.\nLower than Wold’s.\nSimplest algorithmically.\n\n\nNoise Robustness\nSensitive to noise.\nSimilar to Wold’s.\nMore robust to noise.\n\n\nEase of Use\nEstablished but iterative.\nComputationally efficient.\nSimple and deterministic."
  },
  {
    "objectID": "blog/posts/post4/index.html#introduction",
    "href": "blog/posts/post4/index.html#introduction",
    "title": "Exploring Three Orthogonal Signal Correction (OSC) Algorithms",
    "section": "",
    "text": "Wold’s method was the first formal OSC algorithm. It operates iteratively to identify orthogonal components unrelated to the dependent variable \\(Y\\). The method leverages a combination of principal component analysis (PCA) and partial least squares (PLS). Sjöblom’s approach builds on Wold’s by introducing a direct orthogonalization step. The algorithm emphasizes calibration transfer, making it especially useful for standardizing spectral datasets across instruments or conditions. Whereas, Fearn proposed a mathematically elegant version of OSC, simplifying the computation by leveraging matrix operations. The method directly orthogonalizes \\(X\\) using a singular value decomposition (SVD) of a residual matrix.\n\n\nThe Wold algorithm is like a precise sculptor of spectroscopic data. It uses Partial Least Squares (PLS) regression to systematically remove spectral variations that are unrelated to the target variable. The key steps involve:\nInitialize \\(t\\), the first score vector (e.g., using PCA on \\(X\\)).\n\nDeflate \\(t\\) using \\(Y\\): \\(t_{\\text{new}} = t - Y(Y^\\top Y)^{-1}Y^\\top t\\)\nCalculate a loading vector \\(p\\) from \\(t_{\\text{new}}\\) to model \\(X\\): \\(p = \\frac{X^\\top t_{\\text{new}}}{t_{\\text{new}}^\\top t_{\\text{new}}}\\)\nDeflate \\(X\\): \\(X_{\\text{new}} = X - t_{\\text{new}} p^\\top\\)\nRepeat until \\(n_{\\text{comp}}\\).\n\n\n\nShow the code\nwold_osc &lt;- function(X, Y, ncomp = 1, tol = 1e-6, max.iter = 100) {\n  # Ensure X and Y are matrices\n  X &lt;- as.matrix(X)\n  Y &lt;- as.matrix(Y)\n\n  # Store the original X matrix\n  X_original &lt;- X\n\n  # Initialize lists to store components\n  scores &lt;- list()\n  loadings &lt;- list()\n  weights &lt;- list()\n\n  for (comp in seq_len(ncomp)) {\n    # Step 1: Initial PCA on X to get the first principal component score vector (t)\n    t &lt;- svd(X, nu = 1, nv = 0)$u * svd(X, nu = 1, nv = 0)$d[1]\n\n    # Step 2: Orthogonalize t with respect to Y to obtain t*\n    t_star &lt;- t - Y %*% MASS::ginv(crossprod(Y, Y)) %*% crossprod(Y, t)\n\n    iter &lt;- 0\n    diff &lt;- tol + 1\n\n    while (diff &gt; tol && iter &lt; max.iter) {\n      iter &lt;- iter + 1\n\n      # Step 3: Compute weights (w) to make Xw as close as possible to t*\n      w &lt;- crossprod(X, t_star) / sum(t_star^2)\n      w &lt;- w / sqrt(sum(w^2))  # Normalize the weights\n\n      # Step 4: Update t as Xw\n      t_new &lt;- X %*% w\n\n      # Step 5: Orthogonalize t_new with respect to Y\n      t_star &lt;- t_new - Y %*% MASS::ginv(crossprod(Y, Y)) %*% crossprod(Y, t_new)\n\n      # Compute convergence criterion\n      diff &lt;- sqrt(sum((t_star - t)^2)) / sqrt(sum(t_star^2))\n\n      # Update t for the next iteration\n      t &lt;- t_star\n    }\n\n    if (iter == max.iter) {\n      warning(\"Iteration limit reached without convergence.\")\n    }\n\n    # Step 6: Compute the loading vector (p)\n    p &lt;- crossprod(X, t_star) / sum(t_star^2)\n\n    # Step 7: Deflate X\n    X &lt;- X - t_star %*% t(p)\n\n    # Store results\n    scores[[comp]] &lt;- t_star\n    loadings[[comp]] &lt;- p\n    weights[[comp]] &lt;- w\n  }\n\n  # Combine components into matrices\n  T_star &lt;- do.call(cbind, scores)\n  P &lt;- do.call(cbind, loadings)\n  W &lt;- do.call(cbind, weights)\n\n  # Calculate the filtered X matrix\n  X_filtered &lt;- X_original - T_star %*% t(P)\n\n  # Return results as a list\n  return(list(\n    scores = T_star,\n    loadings = P,\n    weights = W,\n    X_filtered = X_filtered\n  ))\n}\n\n\n\n\n\nSjöblom’s approach is the pragmatic cousin of the Wold method. It uses similar steps but simplifies certain iterative aspects, focusing on the orthogonal direction more explicitly.\n\nIdentify a direction vector \\(w\\) from \\(X\\) and \\(t\\), the orthogonal scores \\(w = \\frac{X^\\top t}{t^\\top t}\\)​\nNormalize \\(w\\): \\(w = \\frac{w}{\\|w\\|}\\)​\nDeflate \\(t\\) from \\(Y\\) as in Wold’s method.\nRemove the orthogonal variation from \\(X\\): \\(X_{\\text{new}} = X - t p^\\top\\)\nIterate for each component.\n\n\n\nShow the code\nsjoblom_osc &lt;- function(x, y, ncomp, tol, max.iter) {\n  x_original &lt;- x\n  ps &lt;- ws &lt;- ts &lt;- vector(\"list\", ncomp)\n  for (i in seq_len(ncomp)) {\n    pc &lt;- stats::prcomp(x, center = FALSE)\n    t &lt;- pc$x[, 1]\n    .diff &lt;- 1\n    .iter &lt;- 0\n    while (.diff &gt; tol && .iter &lt; max.iter) {\n      .iter &lt;- .iter + 1\n      t_new &lt;- t - y %*% MASS::ginv(crossprod(y, y)) %*% crossprod(y, t)\n      w &lt;- crossprod(x, t_new) %*% MASS::ginv(crossprod(t_new, t_new))\n      w &lt;- w / sqrt(sum(w^2))\n      t_new &lt;- x %*% w\n      .diff &lt;- sqrt(sum((t_new - t)^2) / sum(t_new^2))\n      t &lt;- t_new\n    }\n    plsFit &lt;- pls::simpls.fit(x, t, ncomp)\n    w &lt;- plsFit$coefficients[ , , ncomp]\n    t &lt;- x %*% w\n    t &lt;- t - y %*% MASS::ginv(crossprod(y, y)) %*% crossprod(y, t)\n    p &lt;- crossprod(x, t) %*% MASS::ginv(crossprod(t, t))\n    x &lt;- x - tcrossprod(t, p)\n    ws[[i]] &lt;- w\n    ps[[i]] &lt;- p\n    ts[[i]] &lt;- t\n  }\n  w_ortho &lt;- do.call(cbind, ws)\n  p_ortho &lt;- do.call(cbind, ps)\n  t_ortho &lt;- do.call(cbind, ts)\n  x_osc &lt;- x_original - x_original %*% tcrossprod(w_ortho, p_ortho)\n\n  R2 &lt;- sum(x_osc^2) / sum(x_original^2) * 100\n  angle &lt;- crossprod(t_ortho, y)\n  norm &lt;- MASS::ginv(sqrt(apply(t_ortho^2, 2, sum) * sum(y^2)))\n  angle &lt;- t(angle) %*% t(norm)\n  angle &lt;- mean(acos(angle) * 180 / pi)\n\n  res &lt;- list(\n    \"correction\" = tibble::as_tibble(x_osc),\n    \"weights\" = tibble::as_tibble(w_ortho),\n    \"scores\" = tibble::as_tibble(t_ortho),\n    \"loadings\" = tibble::as_tibble(p_ortho),\n    \"angle\" = angle,\n    \"R2\" = R2\n  )\n  return(res)\n}\n\n\n\n\n\nFearn’s method stands out by using Singular Value Decomposition (SVD) as its foundation. Its characteristics include:\n\nCompute the residual matrix \\(Z\\): \\(Z = X - Y (Y^\\top Y)^{-1} Y^\\top X\\)\nPerform SVD on \\(Z\\): \\(Z = U S V^\\top\\)\nExtract the first \\(n_{\\text{comp}}\\) components from \\(V\\) and reconstruct the orthogonal scores \\(t\\) and loadings \\(p\\): \\(t = Z V_{:, i}, \\quad p = \\frac{X^\\top t}{t^\\top t}\\)\nDeflate \\(X\\): \\(X_{\\text{new}} = X - t p^\\top\\)\n\n\n\nShow the code\nfearn_osc &lt;- function(x, y, ncomp, tol, max.iter) {\n  x_original &lt;- x\n  ps &lt;- ws &lt;- ts &lt;- vector(\"list\", ncomp)\n  m &lt;- diag(row(x)) - crossprod(x, y) %*% MASS::ginv(crossprod(y, x) %*% crossprod(x, y)) %*% crossprod(y, x)\n  z &lt;- x %*% m\n  decomp &lt;- svd(t(z))\n  u &lt;- decomp$u\n  s &lt;- decomp$d\n  v &lt;- decomp$v\n  g &lt;- diag(s[1:ncomp])\n  c &lt;- v[, 1:ncomp, drop = FALSE]\n\n  for (i in seq_len(ncomp)) {\n    w_old &lt;- rep(0, ncol(x))\n    w_new &lt;- rep(1, ncol(x))\n    dif &lt;- 1\n    iter &lt;- 0\n    while (dif &gt; tol && iter &lt; max.iter) {\n      iter &lt;- iter + 1\n      w_old &lt;- w_new\n      t_new &lt;- c[, i] %*% g[i, i]\n      p_new &lt;- tcrossprod(x, t_new) / tcrossprod(t_new, t_new)\n      w_new &lt;- m %*% tcrossprod(x, p_new)\n      dif &lt;- sqrt(sum((w_new - w_old)^2) / sum(w_new^2))\n    }\n    ws[[i]] &lt;- w_new\n    ts[[i]] &lt;- c[, i] %*% g[i, i]\n    ps[[i]] &lt;- tcrossprod(x, t[[i]]) / tcrossprod(t[[i]], t[[i]])\n  }\n  w_ortho &lt;- do.call(cbind, ws)\n  t_ortho &lt;- do.call(cbind, ts)\n  p_ortho &lt;- do.call(cbind, ps)\n  x_osc &lt;- x - tcrossprod(t_ortho, p_ortho)\n\n  R2 &lt;- sum(x_osc^2) / sum(x_original^2) * 100\n  angle &lt;- crossprod(t_ortho, y)\n  norm &lt;- MASS::ginv(sqrt(apply(t_ortho^2, 2, sum) * sum(y^2)))\n  angle &lt;- t(angle) %*% t(norm)\n  angle &lt;- mean(acos(angle) * 180 / pi)\n\n  res &lt;- list(\n    \"correction\" = tibble::as_tibble(x_osc),\n    \"weights\" = tibble::as_tibble(w_ortho),\n    \"scores\" = tibble::as_tibble(t_ortho),\n    \"loadings\" = tibble::as_tibble(p_ortho),\n    \"angle\" = angle,\n    \"R2\" = R2\n  )\n  return(res)\n}"
  },
  {
    "objectID": "blog/posts/post4/index.html#implementation",
    "href": "blog/posts/post4/index.html#implementation",
    "title": "Exploring Three Orthogonal Signal Correction (OSC) Algorithms",
    "section": "",
    "text": "We begin by implementing these algorithms, creating functions named wold_osc, sjoblom_osc, and fearn_osc. Each function takes five key parameters. The first parameter, x, represents the input data matrix, which typically contains spectral or chemical measurements. The second parameter, y, corresponds to the target variable or response vector. The ncomp parameter specifies the number of orthogonal components to extract, while tol sets the tolerance level for convergence, determining the stopping criterion for iterations. Finally, max.iter establishes the maximum number of iterations allowed during the optimization process. The function definitions for these algorithms follow this structure:\nwold_osc &lt;- function(x, y, ncomp, tol, max.iter)\nsjoblom_osc &lt;- function(x, y, ncomp, tol, max.iter)\nfearn_osc &lt;- function(x, y, ncomp, tol, max.iter)\nTo begin, the original data matrix x is stored, and empty lists are initialized to hold the extracted principal components, weights, and scores for each component. This step ensures that the algorithm’s outputs are organized for further processing or analysis:\nx_original &lt;- x\nps &lt;- ws &lt;- ts &lt;- vector(\"list\", ncomp)\nThe algorithm proceeds with a loop to extract the specified number of orthogonal components. For each iteration, Principal Component Analysis (PCA) is performed on the current x matrix without centering, using the stats::prcomp function. The initial score vector t is derived from the first principal component. Variables .iter and .diff are initialized to track the number of iterations and the difference between successive score vectors, which serves as the convergence criterion.\nWithin the loop, the orthogonalization process begins. Variation correlated with the response variable y is iteratively removed from the score vector t, refining its orthogonality. Weights, representing the relationship between the input matrix x and the score vector, are calculated and normalized to unit length. A new score vector is then computed, and the convergence check compares the difference between successive score vectors (.diff) to the tolerance level (tol). The loop continues until the difference falls below the specified tolerance or the maximum number of iterations is reached.\nwhile (.diff &gt; tol && .iter &lt; max.iter) {\n  .iter &lt;- .iter + 1\n  t_new &lt;- t - y %*% MASS::ginv(crossprod(y, y)) %*% crossprod(y, t)\n  w &lt;- crossprod(x, t_new) %*% MASS::ginv(crossprod(t_new, t_new))\n  w &lt;- w / sqrt(sum(w^2))\n  t_new &lt;- x %*% w\n  .diff &lt;- sqrt(sum((t_new - t)^2) / sum(t_new^2))\n  t &lt;- t_new\n}\nAfter achieving convergence, a Partial Least Squares (PLS) model is fitted to the data using the extracted scores. The weights and scores are updated, and the loadings are computed. At this stage, y-correlated variation is removed, and the input matrix x is deflated by subtracting the modeled variation. This step prepares the matrix for the next orthogonal component extraction.\nThe extracted weights, loadings, and scores for each orthogonal component are stored in their respective lists:\nws[[i]] &lt;- w\nps[[i]] &lt;- p\nts[[i]] &lt;- t\nOnce all components are extracted, the results are combined to construct the orthogonal components matrix. The orthogonally corrected matrix x_osc is then computed by removing the contributions of the orthogonal components from the original data matrix:\nx_osc &lt;- x_original - x_original %*% tcrossprod(w_ortho, p_ortho)\nFinally, to evaluate the algorithm’s performance, two metrics are computed. The percentage of variation removed R2 quantifies how effectively the algorithm deflates the input matrix, while the angle between the orthogonal scores and the target variable y provides insight into the degree of orthogonality achieved. These metrics allow us to assess the quality and effectiveness of the orthogonal signal correction methods."
  },
  {
    "objectID": "blog/posts/post4/index.html#exemple",
    "href": "blog/posts/post4/index.html#exemple",
    "title": "Exploring Three Orthogonal Signal Correction (OSC) Algorithms",
    "section": "",
    "text": "We will use the beer dataset introduced in our previous post. The dataset consists of Near-Infrared Spectroscopy (NIRS) spectra collected from 80 beer samples, x_matrix. The target variable of interest is the Original Gravity (OG), also known as the original extract, y_target. This parameter measures the concentration of dissolved solids in the wort before fermentation begins, providing a crucial indicator of the brewing process.\n\n\nShow the code\nbeer &lt;- readr::read_csv(\"beer.csv\", show_col_types = FALSE) |&gt; dplyr::rename(extract = originalGravity)\nx_matrix &lt;- beer |&gt; dplyr::select(-extract) |&gt; as.matrix() |&gt; scale(scale = FALSE) \ny_target &lt;- beer |&gt; dplyr::pull(extract)\n\n\nNext, we will perform standard PCA and PLS on the NIRS spectra of the beer dataset to assess how applying orthogonal correction modifies the data structure.\n\n\nShow the code\nset.seed(123)\nn_samples &lt;- as.integer(nrow(x_matrix))\n# PCA\npca_result &lt;- stats::prcomp(x_matrix, center = FALSE, scale = FALSE)\npca_scores &lt;- pca_result$x\npca_df &lt;- data.frame(\n  comp1 = pca_scores[, 1],\n  comp2 = pca_scores[, 2],\n  extract = y_target,\n  Sample = factor(n_samples)\n)\n\n# PLS\npls_result &lt;- pls::plsr(y_target ~ x_matrix, ncomp = 10, validation = \"none\")\npls_scores &lt;- pls::scores(pls_result)[, 1:10]\npls_df &lt;- data.frame(\n  comp1 = pls_scores[, 1],\n  comp2 = pls_scores[, 2],\n  extract = y_target,\n  Sample = factor(n_samples)\n)\n\n# Hotelling's T-squared ellipse\npca_T2ellipse &lt;- HotellingEllipse::ellipseParam(pca_scores, k = 2, pcx = 1, pcy = 2)\npls_T2ellipse &lt;- HotellingEllipse::ellipseParam(pls_scores, k = 2, pcx = 1, pcy = 2)\n\n\n\n\n\n\n\n\n\n\n\nNow, we can apply the OSC algorithms to the dataset and compare their impact. Specifically, we’ll examine how the OSC filtering affects the distribution of samples in the reduced-dimensionality space and whether the variation captured aligns better with the response variable.\n\n\nShow the code\nwold_filter &lt;- wold_osc(x_matrix, y_target, ncomp = 10, tol = 1e10, max.iter = 10)\nsjoblom_filter &lt;- sjoblom_osc(x_matrix, y_target, ncomp = 10, tol = 1, max.iter = 10)\n\nwold_scores &lt;- wold_filter %&gt;%\n  pluck(\"scores\") %&gt;%\n  as_tibble() %&gt;%\n  mutate(extract = y_target)\n\nsjoblom_scores &lt;- sjoblom_filter %&gt;%\n  pluck(\"scores\") %&gt;%\n  as_tibble() %&gt;%\n  mutate(extract = y_target)\n\n\n\n\n\n\n\n\n\n\n\nThe following plot compares the results of OSC, PCA, and PLS modeling by overlaying their respective loadings on a single graph. By presenting their loadings together, we can clearly observe the differences in how each method captures and prioritizes spectral features and filter noise, highlighting their unique contributions and areas of overlap.\n\n\nShow the code\npca_loadings &lt;- pca_result %&gt;% \n  pluck(\"rotation\") %&gt;%\n  as_tibble() %&gt;%\n  mutate(wavelength = rep(seq(1100, 2250, 2), times = 1))\n\npls_loadings &lt;- loadings(pls_result)[, 1:10] %&gt;%\n  as_tibble() %&gt;%\n  janitor::clean_names() %&gt;%\n  mutate(wavelength = rep(seq(1100, 2250, 2), times = 1))\n\nwold_loadings &lt;- wold_filter %&gt;% \n  pluck(\"loadings\") %&gt;%\n  as_tibble() %&gt;%\n  mutate(wavelength = rep(seq(1100, 2250, 2), times = 1))\n  \nsjoblom_loadings &lt;- sjoblom_filter %&gt;% \n  pluck(\"loadings\") %&gt;%\n  mutate(wavelength = rep(seq(1100, 2250, 2), times = 1))\n\n\n\n\n\n\n\n\n\n\n\nUnlike PCA (in red) and PLS (in dark green), which show substantial variability, particularly in the region above 1350 nm, OSC (in blue) effectively filters the noise, reducing it to near-zero levels. This smoothing demonstrates OSC’s ability to isolate and preserve only the information strongly correlated with the target variable while systematically discarding irrelevant or orthogonal components. However, its aggressive filtering comes at the expense of potentially reducing some useful signal.\nThe PCA and PLS loadings, on the other hand, display pronounced fluctuations, reflecting their sensitivity to variance within the dataset. PCA, focusing solely on maximizing variance without considering the target variable, captures not only relevant features but also substantial noise. PLS, while more targeted as it incorporates the correlation with the target variable, still exhibits residual noise, especially in the higher wavelengths, indicating its partial retention of irrelevant variance."
  },
  {
    "objectID": "talks.html#conference-presentations",
    "href": "talks.html#conference-presentations",
    "title": "Christian L. Goueguel",
    "section": "Conference Presentations",
    "text": "Conference Presentations\n\nC. L. Goueguel, Direct determination of soil texture using laser-induced breakdown spectroscopy and multivariate linear regressions. Annual meetings of the North American Society for Laser-Induced Breakdown Spectroscopy (NASLIBS) hosted by FACSS SciX Conference 2019 (Palm Springs, CA, USA)\nC. L. Goueguel, Using LIBS analyzer for laser-based soil analysis for precision agriculture. Invited speaker at the Canadian Society for Analytical Sciences and Spectroscopy, Jan. 1, 2018 (Toronto, ON, Canada)\nJ. C. Jain, C. L. Goueguel, D. L. McIntyre, H. M. Edenborn, Dissolution of mineral carbonates with increasing CO2 pressure and the implications for carbon sequestration leak detection. GSA annual meeting, 2016 (Denver, CO, USA)\nC. G. Carson, C. Goueguel, J. Jain, D. McIntyre, Development of laser induced breakdown spectroscopy sensor to assess groundwater quality impacts resulting from geologic carbon sequestration. SPIE Micro- and Nanotechnology Sensors, Systems, and Applications VII, 2015 (Baltimore, MD, USA)\nC. L. Goueguel, J. C. Jain, D. L. McIntyre, A. K. Karamalidis, Application of laser induced breakdown spectroscopy (LIBS) to analyze CO2-bearing solutions in high pressure high temperature environment. ACS 248th - National Meeting, 2014 (San Francisco, CA, USA)\nC. L. Goueguel, J. C. Jain, A. K. Karamalidis, D. L. McIntyre, J. P. Singh, Laser-induced breakdown spectroscopy of high-pressure carbonated brine solutions. 2014 PITTCON Conference & Expo, 2014 (Chicago, IL, USA)\nC. L. Goueguel, D. L. McIntyre, J. C. Jain, J. P. Singh, A. K. Karamalidis, Analysis of calcium in CO2-laden brine (NaCl-CaCl2) by Laser-induced breakdown spectroscopy (LIBS). Annual meetings of the North American Society for Laser-Induced Breakdown Spectroscopy (NASLIBS) hosted by FACSS SciX Conference 2013 (Milwaukee, WI, USA)\nJ. P. Singh, C L. Goueguel, D. L. McIntyre, J. C. Jain, A. K. Karamalidis, Laser-induced breakdown spectroscopy for elemental analysis of brines. 7th International Conference on Laser-Induced Breakdown Spectroscopy, 2012 (Luxor, Egypt)\nC. Goueguel, Laser Induced Breakdown Spectroscopy (LIBS): From Mars exploration to GCS monitoring. Postdoctoral Seminar at the Department of Civil and Environmental Engineering, Carnegie Mellon University, 2012 (Pittsburgh, PA, USA)\nC. Goueguel, S. Laville, F. Vidal, M. Sabsabi, M. Chaker, Investigation of resonance-enhanced laser-induced breakdown spectroscopy (RELIBS) for the analysis of aluminum alloys. 6th International Conference on Laser- Induced Breakdown Spectroscopy, 2010 (Memphis, TN, USA)\nC. Goueguel, S. Laville, H. Loudyi, F. Vidal, M. Sabsabi, M. Chaker, Investigation of resonance-enhanced laser-induced breakdown spectroscopy for the analysis of aluminum alloys. 2nd North American Symposium on Laser-Induced Breakdown Spectroscopy, 2009 (New Orleans, LA, USA)\nC. Goueguel, H. Loudyi, S. Laville, M. Chaker, M. Sabsabi, F. Vidal, Excitation spectrale sélective d’un plasma d’aluminium par une seconde impulsion laser: Optimisation des paramètres expérimentaux. Colloque Plasma- Québec, 2009 (Montreal, QC, Canada)\nS. Laville, K. Rifai, H. Loudyi, M. Chaker, M. Sabsabi, F. Vidal, C. Goueguel, Analysis of trace elements in liquids using LIBS combined with laser-induced fluorescence. 5th International Conference on Laser-Induced Breakdown Spectroscopy, 2008 (Berlin, Germany)\nF. Vidal, M. Chaker, C. Goueguel, S. Laville, H. Loudyi, K. Rifai, M. Sabsabi, Enhanced laser-induced breakdown spectroscopy by second pulse selective wavelength excitation. Laser and plasma applications in materials science: 1st International Conference on Laser Plasma Applications in Materials Science, LAPAMS, 2008 (Alger, Algeria)\nC. Goueguel, S. Laville, H. Loudyi, M. Chaker, M. Sabsabi, F. Vidal. Detection of lead in brass by Laser-Induced Breakdown Spectroscopy combined with Laser-Induced Fluorescence. Photonics North, 2008 (Montreal, QC, Canada)\nH. Loudyi, C. Goueguel, K. Rifai, S. Laville, M. Sabsabi, M. Chaker, F. Vidal, Enhancing the sensitivity of the laser-Induced Breakdown Spectroscopy technique: spectrally selective excitation of specific elements in laser produced plasma. Canadian Association of Physicists Congress, 2008 (Laval, QC, Canada)\nC. Goueguel, H. Loudyi, S. Laville, M. Chaker, M. Sabsabi, F. Vidal, Analyse des matériaux solides par LIBS: Effet d’une deuxième impulsion laser basée sur une excitation spectrale sélective. Colloque Plasma-Québec, 2008 (Montreal, QC, Canada)\nF. Vidal, S. Laville, C. Goueguel, H. Loudyi, M. Chaker, M. Sabsabi, Investigation of laser-induced breakdown spectroscopy combined with laser- induced fluorescence. 4th Euro-Mediterranean Symposium on Laser-Induced Breakdown Spectroscopy, 2007 (Paris, France)"
  },
  {
    "objectID": "blog/blog.html",
    "href": "blog/blog.html",
    "title": "Christian L. Goueguel",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nA Comparative Exploration of Orthogonal Signal Correction Methods\n\n\n\n\n\nOrthogonal signal correction (OSC) is a powerful preprocessing technique frequently used to remove variation in spectral data that is orthogonal to the property of interest. Over the years, several implementations of OSC have emerged, with the most notable being those by Wold et al., Sjöblom et al., and Fearn. This post compares these three methods, exploring their algorithmic approaches and practical implications.\n\n\n\n\n\nMay 25, 2023\n\n\nChristian L. Goueguel\n\n\n13 min\n\n\n\n\n\n\n\n\n\n\n\n\nAn Overview of Orthogonal Partial Least Squares\n\n\n\n\n\nHave you ever heard of Orthogonal Partial Least-Squares (OPLS)? This article aims to give you a clear and concise overview of OPLS and its advantages in the development of more efficient predictive models.\n\n\n\n\n\nDec 20, 2019\n\n\nChristian L. Goueguel\n\n\n6 min\n\n\n\n\n\n\n\n\n\n\n\n\nChemometric Modeling with Tidymodels: A Tutorial for Spectroscopic Data\n\n\n\n\n\nIn this post, we demonstrate how to build robust chemometric models for spectroscopic data using the Tidymodels framework in R. This workflow is designed to cater to beginners and advanced practitioners alike, offering an end-to-end guide from data preprocessing to model evaluation and interpretation.\n\n\n\n\n\nApr 17, 2022\n\n\nChristian L. Goueguel\n\n\n10 min\n\n\n\n\n\n\n\n\n\n\n\n\nMultivariate Outlier Detection in High-Dimensional Spectral Data\n\n\n\n\n\nHigh-dimensional data are particularly challenging for outlier detection. Robust PCA methods have been developed to build models that are unaffected by outliers in high dimensions. These outliers are generally characterized by their deviation from the PCA subspace.\n\n\n\n\n\nJan 7, 2020\n\n\nChristian L. Goueguel\n\n\n9 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blog/posts/post5/index.html",
    "href": "blog/posts/post5/index.html",
    "title": "Optical Breakdown Threshold in Water",
    "section": "",
    "text": "Photo by Vinicius Amano.\n\n\n\n\n\n\nMultiphoton ionization (MPI) is a key first step in laser-induced plasma (LIP) [1-4]. This process occurs when a valence electron absorbs multiple photons simultaneously, leading to ionization. The probability of ionization via MPI is proportional to \\(I^{k}\\) (where \\(I\\) is the laser intensity while \\(k\\) is the minimum number of photons required whose total energy meets or exceeds the ionization energy of the valence electron. For example, consider the MPI process for sodium (Na). The first ionization energy \\((\\text{E}_{ion})\\) of Na is ~ 5.14 eV. To ionize a valence electron from the ground state \\((\\text{E}_{0})\\) to the free state, the simultaneous absorption of five photons from a 1064-nm laser pulse is necessary.\nMPI is a nonlinear optical process that becomes prominent at extremely high laser intensities, resulting in a higher breakdown threshold compared to the cascade ionization (CI) mechanism. Additionally, MPI dominates in the femtosecond regime, where the ultra-short laser pulse duration is insufficient to sustain the cascading free electron generation necessary for CI. Consequently, MPI is the primary absorption mechanism under such conditions.\n\n\n\nIn pure water, seed electrons are primarily generated through the multiphoton ionization (MPI) of water molecules. In contrast, in water containing impurities, seed electrons are more likely produced through the ionization of impurities by thermal excitation, which establishes an initial density of free electrons within the laser’s focal volume. Notably, achieving multiphoton initiation of cascade ionization (CI) in pure water requires significantly higher laser intensities. However, the presence of impurities can drastically lower the breakdown threshold and facilitate plasma formation by providing additional seed electrons. For instance, saline water demonstrates this phenomenon effectively. In such cases, seed electrons originate from the ionization of easily ionizable elements (EIE), such as sodium. These free electrons, liberated from the outermost shells of Na atoms, acquire sufficient kinetic energy through inverse Bremsstrahlung (IB) absorption—a process in which seed electrons absorb laser photons during collisions with heavy particles like molecules or ions. This energy gain enables the electrons to produce a cascade of additional free electrons, ultimately resulting in water breakdown.\nFor breakdown to occur, the rate of electron energy gain via IB absorption must exceed the rate of energy loss due to inelastic collisions. Similarly, the ionization rate must surpass the loss of free electrons through electron-ion recombination and diffusion out of the focal volume. Consequently, the laser intensity within the focal volume must be sufficiently high to drive these energy gains and maintain a net increase in free electrons. Finally, CI is the dominant breakdown mechanism in most scenarios, especially when long-duration laser pulses (in the nanosecond regime) are employed.\n\n\n\n\nIn this section, we summarize some published theoretical modeling results and experimental measurements of breakdown thresholds in water. Before going any further, it is important to remember that for aqueous solutions, in the nanosecond regime, the breakdown threshold is significantly higher for MPI compared to CI. As the duration of the laser pulse decreases, MPI begins to dominate the IC mechanism, both for pure water and for water with impurities. In addition, the uncertainty associated with the search for seed electrons in the focal volume at a given time makes breakdown by CI a probabilistic process, which defined breakdown threshold in terms of breakdown probability.\n\n\nBreakdown threshold in water can be defined experimentally as the minimum laser intensity required for observing a spark, the signal emitted by plasma (which rather refers to the plasma emission threshold), the appearance of bubbles in water, the production of shock waves or the generation of acoustic sound. Breakdown defined as the appearance of a spark visible to the naked eye or detectable by an optical imaging system is the approach commonly used by many authors. In contrast, in theoretical modeling, the breakdown threshold is commonly defined as the minimum laser intensity required for the production of a sufficiently dense plasma to significantly absorb laser photons. The latter is typically defined by a critical free electron density \\(N_{cr}\\) of approximately \\(10^{19}-10^{20} \\text{cm}^{3}\\) in the focal volume, which has a cylindrical shape given by [1]:\n\\[V = (\\sqrt{2}-1)\\frac{πd^{2}}{2}\\frac{f^{2}\\theta}{D_{0}}\\] where \\(d\\) is the beam spot size, \\(f\\) is the focal length of the focusing lens, \\(\\theta\\) is the beam divergence and \\(D_{0}\\) is the unfocused beam diameter.\n\n\nThe rate equation of free electrons density under the MPI mechanism is expressed by [1]:\n\\[\\frac{\\text{d}}{\\text{dt}}(N_{mpi}) = \\frac{2 \\omega}{9 \\pi} \\left( \\frac{m^{'}}{ℏ} \\right )^{3/2} \\left( \\frac{e^{2}}{16nc\\epsilon_{0}m^{'} \\Delta E \\omega^{2}} \\right)^{k} I^{k}e^{(2kI)} \\Phi \\cdot \\sqrt{2 \\left( k-\\frac{\\Delta E}{ℏ\\omega} \\right)}\\] with, \\(\\Phi(x) = e^{(-x^{2})}∫_{0}^{x}e^{y^{2}}\\text{d}y\\), and \\(m^{'} = \\frac{m_{e}m_{ℏ}}{m_{e} + m_{ℏ}}\\), where \\(\\omega\\) is the angular frequency of laser light, \\(I\\) is the laser intensity, \\(m^{'}\\) is the reduced exciton mass, \\(e\\) is the electron charge, \\(n\\) is the refractive index of water, \\(c\\) is the velocity of light, \\(k\\) is degree of non-linearity of MPI (i.e. the smallest number of laser photons required for MPI), \\(ℏ\\) the reduced Planck constant, \\(\\epsilon_{0}\\) is permittivity of free space and \\(\\Delta E\\) is the ionization potential of water. While the rate equation under CI mechanism is expressed by [1]:\n\\[N_{ci} = \\frac{1}{\\omega^{2}\\tau_{m}^{2} + 1} \\left( \\frac{e^{2}\\tau_{m}}{cnm\\epsilon_{0} \\Delta E} I - \\frac{m \\omega^{2} \\tau_{m}}{M} \\right)\\] where \\(\\tau_{m}\\) is the mean free time between electron-heavy particle collisions, \\(M\\) is the mass of the liquid molecule, and \\(m\\) is the electron mass. Following the above equations, the rate equation under the combined effect of both MPI and CI mechanisms is expressed as [1]:\n\\[\\frac{\\text{d}}{\\text{dt}}(N_{e}) = \\frac{\\text{d}}{\\text{dt}}(N_{mpi}) + N_{ci}N_{e} - (RN_{e} + D)N_{e}\\] where \\(N_{e}\\) is the free electron density, \\(D\\) and \\(R\\) are losses by diffusion and recombination, respectively.\n\n\n\n\nThe laser intensity threshold required for the optical breakdown and plasma formation is a function of both the characteristics of the liquid (ionization energy, level of impurities) and the characteristics of the laser beam (wavelength, pulse duration, and spot size).\n\n\nThis graph illustrates the breakdown threshold (irradiance threshold, (\\(\\text{I}_{th}​\\)) as a function of the laser pulse width (pulse duration) for pure and impure water, with the laser operating at a wavelength of 1064 nm and a focal spot size of \\(d = 30 \\mu\\)m.\n\n\n\nFig.1: Breakdown threshold as a function of the laser pulse duration in pure and impure water. Laser spot size is fixed at 30 um and wavelength is at 1064 nm [4].\n\n\nFor ultrashort pulse durations in the femtosecond regime (leftmost part of the graph), the breakdown threshold is very high. This is because multiphoton ionization (MPI) dominates, requiring simultaneous absorption of multiple photons to ionize valence electrons. As the pulse width increases into the picosecond and nanosecond regimes (rightward on the graph), the breakdown threshold significantly decreases. In this range, cascade ionization becomes dominant, which involves free electron generation through seed electrons and their subsequent acceleration via inverse Bremsstrahlung absorption.\nThe difference between the dashed and solid lines in the nanosecond regime highlights the significant role impurities play in lowering the breakdown threshold through thermal excitation.\n\n\n\nThis graph shows the breakdown threshold (irradiance threshold) as a function of pulse duration for pure water at various focal spot sizes (10, 30, and 90 \\(\\mu\\)m) compared to impure water, using a laser wavelength of 1064 nm.\n\n\n\nFig.2: Breakdown threshold as a function of the laser pulse duration and spot size in pure and impure water. Laser wavelength is at 1064 nm [4].\n\n\nAs the pulse duration increases from the femtosecond to the nanosecond regime, the threshold decreases significantly. In pure water, the breakdown threshold is higher and depends strongly on the focal spot size, with smaller focal spots (e.g., 10 \\(\\mu\\)m) requiring higher irradiance to achieve breakdown.\n\n\n\nThis graph illustrates the dependence of the water breakdown threshold on laser wavelength as a function of pulse width. Notably, it reveals a significant decrease in the breakdown threshold at a laser wavelength of 532 nm compared to 1064 nm, highlighting the wavelength’s impact on the process.\n\n\n\nFig.3: Breakdown threshold as a function of the laser pulse duration and wavelength in pure and impure water. Laser spot size is set at 30 um [4].\n\n\n\n\n\n\n\nThis post highlights the strong dependence of the breakdown threshold on the duration of the laser pulse. When using ultrashort pulses in the femtosecond range, a significantly high-intensity laser source is required in the focal volume due to the dominance of multiphoton ionization as the primary absorption mechanism. This also explains why laser sources in the Visible or UV range, which provide higher-energy photons, are more effective for such applications than IR sources. In contrast, laser pulses in the nanosecond regime allow for a lower breakdown threshold by leveraging the thermal excitation of impurities present in the water. However, it is essential to conduct measurements well above the breakdown threshold, as the threshold itself is governed by probabilistic mechanisms, unlike the more deterministic breakdown behavior observed in the femtosecond regime.\n\n\n\n\nGaabour, L. H., Gamal, Y. E. E. D. & Abdellatif, G. Numerical Investigation of the Plasma Formation in Distilled Water by Nd-YAG Laser Pulses of Different Duration. J Mod Phys 03, 1683–1691 (2012).\nHammer, D. X. et al. Experimental investigation of ultrashort pulse laser-induced breakdown thresholds in aqueous media. IEEE J Quantum Elect 32, 670–678 (1996).\nVogel, A., Nahen, K., Theisen, D. & Noack, J. Plasma formation in water by picosecond and nanosecond Nd:YAG laser pulses. I. Optical breakdown at threshold and superthreshold irradiance. IEEE J Sel Top Quant 2, 847–860 (1996).\nKennedy, P. A first-order model for computation of laser-induced breakdown thresholds in ocular and aqueous media. I. Theory. Progress in Quantum Electronics (1995)."
  },
  {
    "objectID": "blog/posts/post5/index.html#absorption-mechanisms",
    "href": "blog/posts/post5/index.html#absorption-mechanisms",
    "title": "Optical Breakdown Threshold in Water",
    "section": "",
    "text": "Multiphoton ionization (MPI) is a key first step in laser-induced plasma (LIP) [1-4]. This process occurs when a valence electron absorbs multiple photons simultaneously, leading to ionization. The probability of ionization via MPI is proportional to \\(I^{k}\\) (where \\(I\\) is the laser intensity while \\(k\\) is the minimum number of photons required whose total energy meets or exceeds the ionization energy of the valence electron. For example, consider the MPI process for sodium (Na). The first ionization energy \\((\\text{E}_{ion})\\) of Na is ~ 5.14 eV. To ionize a valence electron from the ground state \\((\\text{E}_{0})\\) to the free state, the simultaneous absorption of five photons from a 1064-nm laser pulse is necessary.\nMPI is a nonlinear optical process that becomes prominent at extremely high laser intensities, resulting in a higher breakdown threshold compared to the cascade ionization (CI) mechanism. Additionally, MPI dominates in the femtosecond regime, where the ultra-short laser pulse duration is insufficient to sustain the cascading free electron generation necessary for CI. Consequently, MPI is the primary absorption mechanism under such conditions.\n\n\n\nIn pure water, seed electrons are primarily generated through the multiphoton ionization (MPI) of water molecules. In contrast, in water containing impurities, seed electrons are more likely produced through the ionization of impurities by thermal excitation, which establishes an initial density of free electrons within the laser’s focal volume. Notably, achieving multiphoton initiation of cascade ionization (CI) in pure water requires significantly higher laser intensities. However, the presence of impurities can drastically lower the breakdown threshold and facilitate plasma formation by providing additional seed electrons. For instance, saline water demonstrates this phenomenon effectively. In such cases, seed electrons originate from the ionization of easily ionizable elements (EIE), such as sodium. These free electrons, liberated from the outermost shells of Na atoms, acquire sufficient kinetic energy through inverse Bremsstrahlung (IB) absorption—a process in which seed electrons absorb laser photons during collisions with heavy particles like molecules or ions. This energy gain enables the electrons to produce a cascade of additional free electrons, ultimately resulting in water breakdown.\nFor breakdown to occur, the rate of electron energy gain via IB absorption must exceed the rate of energy loss due to inelastic collisions. Similarly, the ionization rate must surpass the loss of free electrons through electron-ion recombination and diffusion out of the focal volume. Consequently, the laser intensity within the focal volume must be sufficiently high to drive these energy gains and maintain a net increase in free electrons. Finally, CI is the dominant breakdown mechanism in most scenarios, especially when long-duration laser pulses (in the nanosecond regime) are employed."
  },
  {
    "objectID": "blog/posts/post5/index.html#breakdown-threshold",
    "href": "blog/posts/post5/index.html#breakdown-threshold",
    "title": "Optical Breakdown Threshold in Water",
    "section": "",
    "text": "In this section, we summarize some published theoretical modeling results and experimental measurements of breakdown thresholds in water. Before going any further, it is important to remember that for aqueous solutions, in the nanosecond regime, the breakdown threshold is significantly higher for MPI compared to CI. As the duration of the laser pulse decreases, MPI begins to dominate the IC mechanism, both for pure water and for water with impurities. In addition, the uncertainty associated with the search for seed electrons in the focal volume at a given time makes breakdown by CI a probabilistic process, which defined breakdown threshold in terms of breakdown probability.\n\n\nBreakdown threshold in water can be defined experimentally as the minimum laser intensity required for observing a spark, the signal emitted by plasma (which rather refers to the plasma emission threshold), the appearance of bubbles in water, the production of shock waves or the generation of acoustic sound. Breakdown defined as the appearance of a spark visible to the naked eye or detectable by an optical imaging system is the approach commonly used by many authors. In contrast, in theoretical modeling, the breakdown threshold is commonly defined as the minimum laser intensity required for the production of a sufficiently dense plasma to significantly absorb laser photons. The latter is typically defined by a critical free electron density \\(N_{cr}\\) of approximately \\(10^{19}-10^{20} \\text{cm}^{3}\\) in the focal volume, which has a cylindrical shape given by [1]:\n\\[V = (\\sqrt{2}-1)\\frac{πd^{2}}{2}\\frac{f^{2}\\theta}{D_{0}}\\] where \\(d\\) is the beam spot size, \\(f\\) is the focal length of the focusing lens, \\(\\theta\\) is the beam divergence and \\(D_{0}\\) is the unfocused beam diameter.\n\n\nThe rate equation of free electrons density under the MPI mechanism is expressed by [1]:\n\\[\\frac{\\text{d}}{\\text{dt}}(N_{mpi}) = \\frac{2 \\omega}{9 \\pi} \\left( \\frac{m^{'}}{ℏ} \\right )^{3/2} \\left( \\frac{e^{2}}{16nc\\epsilon_{0}m^{'} \\Delta E \\omega^{2}} \\right)^{k} I^{k}e^{(2kI)} \\Phi \\cdot \\sqrt{2 \\left( k-\\frac{\\Delta E}{ℏ\\omega} \\right)}\\] with, \\(\\Phi(x) = e^{(-x^{2})}∫_{0}^{x}e^{y^{2}}\\text{d}y\\), and \\(m^{'} = \\frac{m_{e}m_{ℏ}}{m_{e} + m_{ℏ}}\\), where \\(\\omega\\) is the angular frequency of laser light, \\(I\\) is the laser intensity, \\(m^{'}\\) is the reduced exciton mass, \\(e\\) is the electron charge, \\(n\\) is the refractive index of water, \\(c\\) is the velocity of light, \\(k\\) is degree of non-linearity of MPI (i.e. the smallest number of laser photons required for MPI), \\(ℏ\\) the reduced Planck constant, \\(\\epsilon_{0}\\) is permittivity of free space and \\(\\Delta E\\) is the ionization potential of water. While the rate equation under CI mechanism is expressed by [1]:\n\\[N_{ci} = \\frac{1}{\\omega^{2}\\tau_{m}^{2} + 1} \\left( \\frac{e^{2}\\tau_{m}}{cnm\\epsilon_{0} \\Delta E} I - \\frac{m \\omega^{2} \\tau_{m}}{M} \\right)\\] where \\(\\tau_{m}\\) is the mean free time between electron-heavy particle collisions, \\(M\\) is the mass of the liquid molecule, and \\(m\\) is the electron mass. Following the above equations, the rate equation under the combined effect of both MPI and CI mechanisms is expressed as [1]:\n\\[\\frac{\\text{d}}{\\text{dt}}(N_{e}) = \\frac{\\text{d}}{\\text{dt}}(N_{mpi}) + N_{ci}N_{e} - (RN_{e} + D)N_{e}\\] where \\(N_{e}\\) is the free electron density, \\(D\\) and \\(R\\) are losses by diffusion and recombination, respectively.\n\n\n\n\nThe laser intensity threshold required for the optical breakdown and plasma formation is a function of both the characteristics of the liquid (ionization energy, level of impurities) and the characteristics of the laser beam (wavelength, pulse duration, and spot size).\n\n\nThis graph illustrates the breakdown threshold (irradiance threshold, (\\(\\text{I}_{th}​\\)) as a function of the laser pulse width (pulse duration) for pure and impure water, with the laser operating at a wavelength of 1064 nm and a focal spot size of \\(d = 30 \\mu\\)m.\n\n\n\nFig.1: Breakdown threshold as a function of the laser pulse duration in pure and impure water. Laser spot size is fixed at 30 um and wavelength is at 1064 nm [4].\n\n\nFor ultrashort pulse durations in the femtosecond regime (leftmost part of the graph), the breakdown threshold is very high. This is because multiphoton ionization (MPI) dominates, requiring simultaneous absorption of multiple photons to ionize valence electrons. As the pulse width increases into the picosecond and nanosecond regimes (rightward on the graph), the breakdown threshold significantly decreases. In this range, cascade ionization becomes dominant, which involves free electron generation through seed electrons and their subsequent acceleration via inverse Bremsstrahlung absorption.\nThe difference between the dashed and solid lines in the nanosecond regime highlights the significant role impurities play in lowering the breakdown threshold through thermal excitation.\n\n\n\nThis graph shows the breakdown threshold (irradiance threshold) as a function of pulse duration for pure water at various focal spot sizes (10, 30, and 90 \\(\\mu\\)m) compared to impure water, using a laser wavelength of 1064 nm.\n\n\n\nFig.2: Breakdown threshold as a function of the laser pulse duration and spot size in pure and impure water. Laser wavelength is at 1064 nm [4].\n\n\nAs the pulse duration increases from the femtosecond to the nanosecond regime, the threshold decreases significantly. In pure water, the breakdown threshold is higher and depends strongly on the focal spot size, with smaller focal spots (e.g., 10 \\(\\mu\\)m) requiring higher irradiance to achieve breakdown.\n\n\n\nThis graph illustrates the dependence of the water breakdown threshold on laser wavelength as a function of pulse width. Notably, it reveals a significant decrease in the breakdown threshold at a laser wavelength of 532 nm compared to 1064 nm, highlighting the wavelength’s impact on the process.\n\n\n\nFig.3: Breakdown threshold as a function of the laser pulse duration and wavelength in pure and impure water. Laser spot size is set at 30 um [4]."
  },
  {
    "objectID": "blog/posts/post5/index.html#summary",
    "href": "blog/posts/post5/index.html#summary",
    "title": "Optical Breakdown Threshold in Water",
    "section": "",
    "text": "This post highlights the strong dependence of the breakdown threshold on the duration of the laser pulse. When using ultrashort pulses in the femtosecond range, a significantly high-intensity laser source is required in the focal volume due to the dominance of multiphoton ionization as the primary absorption mechanism. This also explains why laser sources in the Visible or UV range, which provide higher-energy photons, are more effective for such applications than IR sources. In contrast, laser pulses in the nanosecond regime allow for a lower breakdown threshold by leveraging the thermal excitation of impurities present in the water. However, it is essential to conduct measurements well above the breakdown threshold, as the threshold itself is governed by probabilistic mechanisms, unlike the more deterministic breakdown behavior observed in the femtosecond regime."
  },
  {
    "objectID": "blog/posts/post5/index.html#references",
    "href": "blog/posts/post5/index.html#references",
    "title": "Optical Breakdown Threshold in Water",
    "section": "",
    "text": "Gaabour, L. H., Gamal, Y. E. E. D. & Abdellatif, G. Numerical Investigation of the Plasma Formation in Distilled Water by Nd-YAG Laser Pulses of Different Duration. J Mod Phys 03, 1683–1691 (2012).\nHammer, D. X. et al. Experimental investigation of ultrashort pulse laser-induced breakdown thresholds in aqueous media. IEEE J Quantum Elect 32, 670–678 (1996).\nVogel, A., Nahen, K., Theisen, D. & Noack, J. Plasma formation in water by picosecond and nanosecond Nd:YAG laser pulses. I. Optical breakdown at threshold and superthreshold irradiance. IEEE J Sel Top Quant 2, 847–860 (1996).\nKennedy, P. A first-order model for computation of laser-induced breakdown thresholds in ocular and aqueous media. I. Theory. Progress in Quantum Electronics (1995)."
  },
  {
    "objectID": "blog/posts/post6/index.html",
    "href": "blog/posts/post6/index.html",
    "title": "The Pseudo-Voigt Function",
    "section": "",
    "text": "The pseudo-Voigt function represents a significant advancement in spectroscopy, emerging from the long-standing challenge of accurately modeling spectral line shapes. While Gaussian and Lorentzian profiles have been fundamental tools in spectral data analysis for decades, they often fell short in capturing the complexity of spectral signal. In the mid-20th century, researchers began to recognize the inherent limitations of relying solely on Gaussian (normal distribution) and Lorentzian (Cauchy distribution) profiles. Complex spectral lines often result from multiple broadening mechanisms, including:\n\nDoppler broadening, which occurs due to thermal motion of particles\nCollisional broadening, resulting from interactions between particles\nInstrumental broadening introduced by measurement apparatus\n\nWhen Doppler broadening dominates, the shape of a spectral line is best modeled by a Gaussian profile. In contrast, collisional broadening typically leads to a Lorentzian line shape. However, in many cases, the observed line shape results from the interplay of multiple broadening mechanisms. As a result, the pseudo-Voigt function often provides a more accurate representation of the line shape. Indeed, the pseudo-Voigt function can account for both collisional and Doppler broadening effects. Moreover, natural broadening resulting from the finite lifetime of excited states can also be modeled using the pseudo-Voigt function. Therefore, the pseudo-Voigt function emerged as an elegant solution to these challenges. By combining the characteristics of Gaussian and Lorentzian profiles, it offers a more nuanced representation of spectral line shapes.\nThe pseudo-Voigt function was first derived by Thompson, Cox, and Hastings in 1987 [1], in their study describing the application of the Rietveld refinement technique to synchrotron X-ray data collected from a capillary sample of Al₂O₃ using Debye–Scherrer geometry at the Cornell High Energy Synchrotron Source (CHESS). Their analysis showed that individual peak shapes are accurately modeled by a pseudo-Voigt function, in which the Gaussian and Lorentzian half-widths vary, respectively, with the Bragg angle due to instrumental resolution and particle-size broadening.\n\n\n\nFundamentally, the pseudo-Voigt function is an approximation of the more precise Voigt profile (named after German physicist Woldemar Voigt). The Voigt profile (\\(V\\)) represents the exact convolution of Gaussian and Lorentzian profiles, providing a more accurate description of spectral line shapes, particularly when both Doppler and Lorentzian broadening are significant.\n\\[\nV(x;\\sigma,\\gamma) = \\int_{-\\infty}^{\\infty} G(x';\\sigma) L(x - x';\\gamma) \\, dx'\n\\]\nThe Voigt profile can also be expressed using the Faddeeva function \\(\\omega(z)\\), given by:\n\\[\nw(z) = e^{-z^2} \\left( 1 + \\frac{2i}{\\sqrt{\\pi}} \\int_0^z e^{t^2} \\, dt \\right)\n\\] where \\(z\\) is a complex number. Using the Faddeeva function, the Voigt profile is: \\[\nV(x; \\sigma, \\gamma) = \\frac{1}{\\sigma\\sqrt{2\\pi}}\\Re\\left[w\\left( \\frac{x + i\\gamma}{\\sigma\\sqrt{2}} \\right)\\right]\n\\]\nwhere \\(\\Re[w(z)]\\) denotes the real part of the Faddeeva function.\n\n\n\n\n\n\n\n\n\nHowever, the Voigt profile is computationally intensive, requiring efficient numerical methods to accurately evaluate the convolution integral. This complexity has led to the development of approximations, such as the pseudo-Voigt function, which strike a balance between computational efficiency and maintaining acceptable levels of accuracy in the spectral line shape.\n\n\n\nThe pseudo-Voigt function (\\(pV\\)) is a linear combination of a Gaussian (\\(G\\)) and a Lorentzian (\\(L\\)) function, and is defined as:\n\\[\npV(x;\\eta) = ηG(x;\\sigma) + (1-η)L(x;\\gamma)\n\\]\n\\[\nG(x;\\sigma) = \\frac{1}{\\sigma \\sqrt{2\\pi}} e^{-\\frac{x^2}{2\\sigma^2}}\n\\]\n\\[\nL(x; \\gamma) = \\frac{\\gamma}{\\pi (x^2 + \\gamma^2)}\n\\]\nwhere, \\(\\eta\\) is a mixing parameter that determines the relative contribution of the Lorentzian (\\(\\eta = 0\\)) and Gaussian (\\(\\eta = 1\\)) components. Here, the parameters \\(\\sigma\\) and \\(\\gamma\\) represent, respectively, the standard deviation of the Gaussian component, related to the full width at half maximum (FWHM) of the Gaussian by \\(w_G = 2\\sqrt{2\\ln 2} \\, \\sigma\\), and the half-width at half maximum (HWHM) of the Lorentzian component, related to the Lorentzian FWHM by \\(w_L = 2\\gamma\\).\n\n\n\n\n\n\n\n\n\nThe following animation illustrates how the pseudo-Voigt function varies with the mixing parameter \\(\\eta\\). As \\(\\eta\\) increases from 0 to 1, the contribution of the Gaussian component becomes more significant. Each case has a FWHM of \\(w_G = w_L = 1\\).\n\n\nShow the code\nlibrary(ggplot2)\nlibrary(gganimate)\n\ngaussian &lt;- function(x, x_c, w_G, A, y_0) {\n  y_0 + A / (w_G * sqrt(pi / (4 * log(2)))) * exp(-4 * log(2) * (x - x_c)^2 / w_G^2)\n}\nlorentzian &lt;- function(x, x_c, w_L, A, y_0) {\n  y_0 + A / pi * w_L / ((x - x_c)^2 + w_L^2)\n}\npseudo_voigt &lt;- function(x, x_c, w_L, w_G, A, y_0, eta) {\n  y_0 + A * (eta * gaussian(x, x_c, w_G, A, y_0)  + (1 - eta) * lorentzian(x, x_c, w_L, A, y_0))\n}\n\nx_c &lt;- y_0 &lt;- 0; w_L &lt;- w_G &lt;- A &lt;- 1\neta_values &lt;- seq(0, 1, by = 0.05)\ndf &lt;- data.frame(x = seq(-5, 5, by = 0.1))\nfor (eta in eta_values) {\n  df[[paste0(\"η = \", eta)]] &lt;- pseudo_voigt(df$x, x_c, w_L, w_G, A, y_0, eta)\n}\ndf_long &lt;- tidyr::pivot_longer(df, cols = -x, names_to = \"eta\", values_to = \"y\")\n\np &lt;- df_long |&gt; \n  ggplot(aes(x = x, y = y, colour = eta)) +\n  geom_line(linewidth = 1) +\n  scale_y_continuous(limits = c(0,1)) +\n  geom_point(size = 2) +\n  theme_bw(base_size = 15) +\n  labs(\n    x = \"x\", \n    y = \"y\", \n    title = \"The Pseudo-Voigt Function\",\n    subtitle = paste0(\"Varying η from Lorentzian (η = 0) to Gaussian (η = 1): \", \"{closest_state}\"),\n    caption = \"The mixing parameter η determines the relative contribution of the Lorentzian and Gaussian components.\") +\n  theme(\n    legend.position = \"none\",\n    plot.title = element_text(hjust = 0, size = 18, face = \"bold\"),\n    plot.subtitle = element_text(hjust = 0),\n    panel.background = element_rect(colour = \"black\", linewidth = 1)\n    )\n\nanim &lt;- p +\n  transition_states(eta, transition_length = 2, state_length = 1) +\n  ease_aes('exponential-in-out') +\n  shadow_mark(alpha = 2/10, size = 0.1)\n\nanimate(\n  anim, \n  nframes = 100, \n  fps = 10, \n  width = 600, \n  height = 400, \n  renderer = gifski_renderer()\n  )\n\n\n\n\n\n\n\n\n\n\nIn the early 2000s, Ida et al. [2] refined the pseudo-Voigt function by introducing an extended formula designed to more accurately approximate the Voigt profile. This formula is given by:\n\\[\nepV(x;\\eta_L, \\eta_I, \\eta_P) = (1 - \\eta_L - \\eta_I - \\eta_P)G(x;\\sigma) + \\eta_L L(x; \\gamma) + \\eta_I F_I(x;\\gamma_I) + \\eta_P F_P(x;\\gamma_P)\n\\]\nwhere \\(F_I\\) and \\(F_P\\) are intermediate functions that represent the transition between the Lorentzian and Gaussian profiles, respectively. \\(F_I\\) is an irrational function involving a square root, while \\(F_P\\) is the squared hyperbolic secant function. These functions are defined as follows:\n\\[\nF_I(x;\\gamma_I) = \\frac{1}{2\\gamma_I}\\left( 1 + \\left(\\frac{x}{\\gamma_I}\\right)^2 \\right)^{-3/2}\n\\]\n\\[\nF_P(x;\\gamma_P) = \\frac{1}{2\\gamma_P}\\text{sech}^2\\left(\\frac{x}{\\gamma_P}\\right)\n\\]\nThe FWHMs are given by \\(w_I = 2 \\gamma_I \\sqrt{(2^{2/3} - 1)}\\), and \\(w_P = 2 \\gamma_P \\ln{(\\sqrt{2} + 1)}\\). A Gaussian profile is obtained when the mixing parameters are set to \\(\\eta_L = \\eta_I = \\eta_P = 0\\), while the Lorentzian contribution is governed by the parameter \\(\\eta_L\\). Specifically, a pure Lorentzian profile arises when \\(\\eta_L = 1\\). These parameters are constrained to the range from 0 to 1 and must satisfy the condition \\(\\eta_L + \\eta_I + \\eta_P = 1\\). This constraint defines a 2D simplex within a plane in 3D space, representing all possible combinations of the mixing parameters.\n\n\n\n\n\n\n\n\n\nThe animation below is a visual representation of the normalized extended pseudo-Voigt profiles, illustrating the transition from Gaussian to Lorentzian shapes as the mixing parameters \\(\\eta_L, \\eta_I\\), and \\(\\eta_P\\), are varied simultaneously.\n\n\n\n\n\n\n\n\n\nP. Thompson, D.E. Cox, and J.B. Hastings, Rietveld refinement of Debye–Scherrer synchrotron X-ray data from Al2O3. J. Appl. Cryst. 20, 79-83, 1987.\nT. Ida, M. Ando, H. Toraya, Extended pseudo-Voigt function for approximating the Voigt profile. J. Appl. Cryst. 33, 1311-1316, 2000."
  },
  {
    "objectID": "blog/posts/post6/index.html#background",
    "href": "blog/posts/post6/index.html#background",
    "title": "The Pseudo-Voigt Function",
    "section": "",
    "text": "The pseudo-Voigt function represents a significant advancement in spectroscopy, emerging from the long-standing challenge of accurately modeling spectral line shapes. While Gaussian and Lorentzian profiles have been fundamental tools in spectral data analysis for decades, they often fell short in capturing the complexity of spectral signal. In the mid-20th century, researchers began to recognize the inherent limitations of relying solely on Gaussian (normal distribution) and Lorentzian (Cauchy distribution) profiles. Complex spectral lines often result from multiple broadening mechanisms, including:\n\nDoppler broadening, which occurs due to thermal motion of particles\nCollisional broadening, resulting from interactions between particles\nInstrumental broadening introduced by measurement apparatus\n\nWhen Doppler broadening dominates, the shape of a spectral line is best modeled by a Gaussian profile. In contrast, collisional broadening typically leads to a Lorentzian line shape. However, in many cases, the observed line shape results from the interplay of multiple broadening mechanisms. As a result, the pseudo-Voigt function often provides a more accurate representation of the line shape. Indeed, the pseudo-Voigt function can account for both collisional and Doppler broadening effects. Moreover, natural broadening resulting from the finite lifetime of excited states can also be modeled using the pseudo-Voigt function. Therefore, the pseudo-Voigt function emerged as an elegant solution to these challenges. By combining the characteristics of Gaussian and Lorentzian profiles, it offers a more nuanced representation of spectral line shapes.\nThe pseudo-Voigt function was first derived by Thompson, Cox, and Hastings in 1987 [1], in their study describing the application of the Rietveld refinement technique to synchrotron X-ray data collected from a capillary sample of Al₂O₃ using Debye–Scherrer geometry at the Cornell High Energy Synchrotron Source (CHESS). Their analysis showed that individual peak shapes are accurately modeled by a pseudo-Voigt function, in which the Gaussian and Lorentzian half-widths vary, respectively, with the Bragg angle due to instrumental resolution and particle-size broadening."
  },
  {
    "objectID": "blog/posts/post6/index.html#visualizing-the-pseudo-voigt-function",
    "href": "blog/posts/post6/index.html#visualizing-the-pseudo-voigt-function",
    "title": "The Pseudo-Voigt Function",
    "section": "",
    "text": "The following animation illustrates how the Pseudo-Voigt function varies with the mixing parameter \\(\\eta\\). As \\(\\eta\\) increases from 0 to 1, the contribution of the Gaussian component becomes more significant.\n\n\nShow the code\nlibrary(ggplot2)\nlibrary(gganimate)\n\ngaussian &lt;- function(x, x_c, w_G, A, y_0) {\n  y_0 + A / (w_G * sqrt(pi / (4 * log(2)))) * exp(-4 * log(2) * (x - x_c)^2 / w_G^2)\n}\nlorentzian &lt;- function(x, x_c, w_L, A, y_0) {\n  y_0 + A / pi * w_L / ((x - x_c)^2 + w_L^2)\n}\npseudo_voigt &lt;- function(x, x_c, w_L, w_G, A, y_0, eta) {\n  y_0 + A * (eta * lorentzian(x, x_c, w_L, A, y_0) + (1 - eta) * gaussian(x, x_c, w_G, A, y_0))\n}\nx_c &lt;- y_0 &lt;- 0; w_L &lt;- w_G &lt;- A &lt;- 1\neta_values &lt;- seq(0, 1, by = 0.05)\ndf &lt;- data.frame(x = seq(-5, 5, by = 0.1))\nfor (eta in eta_values) {\n  df[[paste0(\"eta = \", eta)]] &lt;- pseudo_voigt(df$x, x_c, w_L, w_G, A, y_0, eta)\n}\ndf_long &lt;- tidyr::pivot_longer(df, cols = -x, names_to = \"eta\", values_to = \"y\")\n\np &lt;- df_long |&gt; \n  ggplot(aes(x = x, y = y, colour = eta)) +\n  geom_line(linewidth = 1) +\n  geom_point(size = 2) +\n  theme_bw(base_size = 15) +\n  labs(x = \"x\", y = \"y\", title = \"The Pseudo-Voigt Function\",\n    subtitle = paste0(\"Varying η (eta) from Lorentzian (η = 0) to Gaussian (η = 1): \", \"{closest_state}\")) +\n  theme(\n    #panel.grid = element_blank(), \n    legend.position = \"none\",\n    plot.title = element_text(hjust = 0, size = 18, face = \"bold\"),\n    plot.subtitle = element_text(hjust = 0),\n    panel.background = element_rect(colour = \"black\", linewidth = 1)\n    )\n\nanim &lt;- p +\n  transition_states(eta, transition_length = 2, state_length = 1) +\n  ease_aes('exponential-in-out') +\n  shadow_mark(alpha = 0.3, size = 0.1)\n\nanimate(\n  anim, \n  nframes = 100, \n  fps = 10, \n  width = 600, \n  height = 400, \n  renderer = gifski_renderer()\n  )\nanim_save(\"voigt_animated.gif\")"
  },
  {
    "objectID": "blog/posts/post6/index.html#visualizing-the-pseudo-voigt-functions",
    "href": "blog/posts/post6/index.html#visualizing-the-pseudo-voigt-functions",
    "title": "The Pseudo-Voigt Function",
    "section": "",
    "text": "The following animation illustrates how the pseudo-Voigt function varies with the mixing parameter \\(\\eta\\). As \\(\\eta\\) increases from 0 to 1, the contribution of the Gaussian component becomes more significant.\n\n\nShow the code\nlibrary(ggplot2)\nlibrary(gganimate)\n\ngaussian &lt;- function(x, x_c, w_G, A, y_0) {\n  y_0 + A / (w_G * sqrt(pi / (4 * log(2)))) * exp(-4 * log(2) * (x - x_c)^2 / w_G^2)\n}\nlorentzian &lt;- function(x, x_c, w_L, A, y_0) {\n  y_0 + A / pi * w_L / ((x - x_c)^2 + w_L^2)\n}\npseudo_voigt &lt;- function(x, x_c, w_L, w_G, A, y_0, eta) {\n  y_0 + A * (eta * gaussian(x, x_c, w_G, A, y_0)  + (1 - eta) * lorentzian(x, x_c, w_L, A, y_0))\n}\n\nx_c &lt;- y_0 &lt;- 0; w_L &lt;- w_G &lt;- A &lt;- 1\neta_values &lt;- seq(0, 1, by = 0.05)\ndf &lt;- data.frame(x = seq(-5, 5, by = 0.1))\nfor (eta in eta_values) {\n  df[[paste0(\"η = \", eta)]] &lt;- pseudo_voigt(df$x, x_c, w_L, w_G, A, y_0, eta)\n}\ndf_long &lt;- tidyr::pivot_longer(df, cols = -x, names_to = \"eta\", values_to = \"y\")\n\np &lt;- df_long |&gt; \n  ggplot(aes(x = x, y = y, colour = eta)) +\n  geom_line(linewidth = 1) +\n  scale_y_continuous(limits = c(0,1)) +\n  geom_point(size = 2) +\n  theme_bw(base_size = 15) +\n  labs(\n    x = \"x\", \n    y = \"y\", \n    title = \"The Pseudo-Voigt Function\",\n    subtitle = paste0(\"Varying η from Lorentzian (η = 0) to Gaussian (η = 1): \", \"{closest_state}\"),\n    caption = \"The mixing parameter η determines the relative contribution of the Lorentzian and Gaussian components.\") +\n  theme(\n    legend.position = \"none\",\n    plot.title = element_text(hjust = 0, size = 18, face = \"bold\"),\n    plot.subtitle = element_text(hjust = 0),\n    panel.background = element_rect(colour = \"black\", linewidth = 1)\n    )\n\nanim &lt;- p +\n  transition_states(eta, transition_length = 2, state_length = 1) +\n  ease_aes('exponential-in-out') +\n  shadow_mark(alpha = 2/10, size = 0.1)\n\nanimate(\n  anim, \n  nframes = 100, \n  fps = 10, \n  width = 600, \n  height = 400, \n  renderer = gifski_renderer()\n  )\nanim_save(\"pv_animation.gif\")\n\n\n\n\n\n\n\nNext, we visualize the normalized extended pseudo-Voigt profiles, illustrating the transition from Gaussian to Lorentzian shapes as the mixing parameters \\(\\eta_L, \\eta_I\\), and \\(\\eta_P\\), are varied simultaneously."
  },
  {
    "objectID": "blog/posts/post6/index.html#references",
    "href": "blog/posts/post6/index.html#references",
    "title": "The Pseudo-Voigt Function",
    "section": "",
    "text": "P. Thompson, D.E. Cox, and J.B. Hastings, Rietveld refinement of Debye–Scherrer synchrotron X-ray data from Al2O3. J. Appl. Cryst. 20, 79-83, 1987.\nT. Ida, M. Ando, H. Toraya, Extended pseudo-Voigt function for approximating the Voigt profile. J. Appl. Cryst. 33, 1311-1316, 2000."
  },
  {
    "objectID": "blog/posts/post7/index.html",
    "href": "blog/posts/post7/index.html",
    "title": "Beyond Standard Boxplot: The Adjusted and Generalized Boxplots",
    "section": "",
    "text": "Photo by Dana Katharina.\n\n\n\n\nProposed by Tukey in 1977 [1], a boxplot is constructed using five key summary statistics: the minimum, first quartile (Q1), median, third quartile (Q3), and maximum. These elements are complemented by whiskers, which typically extend to 1.5 times the interquartile range (IQR = Q3 - Q1) beyond Q1 and Q3, respectively. Data points outside this range are flagged as potential outliers.\n\\[\n\\begin{aligned}\n    \\text{Lower Fence} &= Q_1 - 1.5 \\cdot \\text{IQR} \\\\\n    \\text{Upper Fence} &= Q_3 + 1.5 \\cdot \\text{IQR}\n\\end{aligned}\n\\]\nThe ends of the whiskers are called adjacent values. They are the smallest and largest values not flagged outliers. Moreover, since IQR has a breakdown point of 0.25, it takes more than 25% of the data to be contaminated by outliers for the masking effect to occur. For example, in the Figure below, we show how the quartiles and therefore IQR are significantly influenced by the outliers, highlighting the breakdown point. After replacing 25% of the data with extreme outliers, the IQR increased drastically, from 13 (clean data) to 136 (contaminated data). This demonstrates that with 25% contamination, the IQR becomes unreliable as a measure of spread.\n\n\n\n\n\n\n\n\n\nBoxplots are a widely used tool for visualizing data distributions, particularly when the data exhibit a symmetrical mesokurtic distribution. However, its effectiveness diminishes when dealing with skewed data. In such cases, the boxplot can misrepresent the dataset’s nuances—most notably by classifying an excessive number of larger values as outliers when the distribution is skewed. Recognizing this limitation, Hubert and Vandervieren [2] introduced a refined version of Tukey’s boxplot rule. Their approach incorporates a robust measure of skewness, allowing for a more accurate depiction of asymmetrical distributions while preserving the simplicity and interpretability of the Tukey’s boxplot.\n\n\n\nIn 2004, Brys et al. [3] introduced the medcouple (MC), which is a robust statistic used to measure the skewness of a distribution. Unlike traditional measures of skewness that are sensitive to outliers, MC focuses on the median and the IQR, making it less affected by extreme values. It is based on the difference between left and right data spreads relative to the median. MC is bounded between −1 and 1. A value of 0 indicates a perfectly symmetric distribution, while positive values signify a right-tailed (positively skewed) distribution, and negative values correspond to a left-tailed (negatively skewed) distribution. Importantly, this measure is most effective for moderately skewed distributions, particularly when the absolute value of MC is less than or equal to 0.6 (\\(|\\text{MC}| \\leq 0.6\\)). MC is given by:\n\\[\n\\text{MC} = \\text{med} \\left\\{ \\frac{(x_i - m) - (m - x_j)}{x_i - x_j} \\, \\middle| \\, x_i \\geq m \\geq x_j \\right\\}\n\\]\nwhere, \\(x_i\\) and \\(x_j\\) are data points from the sample, \\(m\\) is the median of the sample, and \\(\\text{med}\\{\\cdot\\}\\) denotes the median operator.\nTo address the limitation of the Tukey’s boxplot in handling skewed data, Hubert and Vandervieren [2] introduced the adjusted boxplot for skewed distribution. This method modifies the whisker length based on the MC. Specifically, the upper whisker extends further for right-skewed data, and the lower whisker extends further for left-skewed data. This new approch modifies the traditional boxplot fences to account for skewness in the data using the exponential function of MC as follows:\nWhen \\(\\text{MC} \\geq 0\\):\n\\[\n\\begin{aligned}\n    \\text{Lower Fence} &= Q_1 - k \\cdot e^{-4\\text{MC}} \\cdot \\text{IQR} \\\\\n    \\text{Upper Fence} &= Q_3 + k \\cdot e^{3\\text{MC}} \\cdot \\text{IQR}\n\\end{aligned}\n\\]\nWhen \\(\\text{MC} \\leq 0\\):\n\\[\n\\begin{aligned}\n    \\text{Lower Fence} &= Q_1 - k \\cdot e^{-3\\text{MC}} \\cdot \\text{IQR} \\\\\n    \\text{Upper Fence} &= Q_3 + k \\cdot e^{4\\text{MC}} \\cdot \\text{IQR}\n\\end{aligned}\n\\]\nwhere, \\(k\\) is the fence factor. Hubert and Vandervieren [2] have optimized the adjusted boxplot for \\(k = 1.5\\). In the next Figure, we compare Tukey’s boxplot (left) with the adjusted boxplot (right), applied to a skewed data. The adjusted boxplot shows fewer outliers for the skewed data, reflecting its ability to handle skewness better.\n\n\n\n\n\n\n\n\n\nThe adjusted boxplot has garnered attention for its ability to handle skewed data, even earning a mention on platforms like Wikipedia. However, it is not without its critiques.\nFor example, Bruffaerts et al. [4] pointed out that the whiskers in the adjusted boxplot are designed to achieve a fixed outlier detection rate of 0.7%. While this threshold may be suitable for some applications, the process becomes cumbersome if one wishes to adopt a different detection rate. It requires re-running the simulations to determine a new tuning constant or fence factor (\\(k\\)). Moreover, for datasets with heavy-tailed distributions, the adjusted boxplot may still misclassify observations, as the whisker adjustment relies solely on skewness and not on tail behavior. Another significant issue lies in its dependence on MC. While MC is effective for large samples, its estimation can be imprecise for small sample sizes.\nThe Figure below illustrates the behavior of MC estimates for small and large sample sizes using density plots. To achieve this, we generated multiple random samples from a skewed Gamma distribution and examined the variability of the MC estimates across these sample sizes. For small sample sizes, the MC estimates exhibit significant variability, reflecting its reduced robustness in such cases. In contrast, as the sample size increases, the MC estimates stabilize and converge to a consistent value, demonstrating its improved precision and reliability with larger data.\n\n\n\n\n\n\n\n\n\nIn response to these shortcomings, Bruffaerts et al. [4] proposed an alternative method to address skewness and heavy-tailed distributions. Their approach introduces a rank-preserving transformation that allows the data to conform to a Tukey \\(g\\)-\\(h\\) distribution.\n\n\n\nIn 1977, Tukey introduced a family of distributions through two nonlinear transformations, giving rise to what is now known as the Tukey \\(g\\)-\\(h\\) distributions [5,6]. These distributions are widely used in robust statistics and flexible modeling due to their ability to accommodate a range of skewness and kurtosis levels. The Tukey \\(g\\)-\\(h\\) distribution is defined by applying two transformations, \\(g\\) (skewness) and \\(h\\) (kurtosis), to a standard normal random variable \\(Z \\sim \\mathcal{N}(0, 1)\\). The resulting transformation is expressed by:\n\\[\nT_{g,h}(Z) = \\frac{e^{gZ}-1}{g} \\cdot e^{h\\frac{Z^2}{2}} \\quad \\text{with} \\quad g \\neq 0, h \\in \\mathbb{R}\n\\]\nBy adjusting \\(g\\) and \\(h\\), the Tukey \\(g\\)-\\(h\\) distribution can model a wide variety of distribution, from symmetric to skewed, and from light-tailed to heavy-tailed distributions.\nThe constants \\(g\\) and \\(h\\), can be estimated from the empirical quantiles as follows:\n\\[\n\\hat{g} = \\frac{1}{z_p} \\text{ln}\\left(-\\frac{Q_p(\\{x_j\\})}{Q_{1-p}(\\{x_j\\})}\\right),\n\\quad\n\\hat{h} = \\frac{2}{z_p^2} \\text{ln}\\left(-\\hat{g}\\frac{Q_p(\\{x_j\\}) \\cdot Q_{1-p}(\\{x_j\\})}{Q_p(\\{x_j\\}) + Q_{1-p}(\\{x_j\\})}\\right)\n\\]where \\(z_p\\) is the quantile of order \\(p\\) of the standard normal distribution. \\(Q_p\\) and \\(Q_{1-p}\\) are the empirical quantiles of order \\(p\\) (\\(0.5&lt;p&lt;1\\)) and \\(1-p\\) of the univariate data \\(X = \\{x_j\\} = \\{x_1, \\cdots, x_n\\}\\).\nThe generalized boxplot as proposed by Bruffaerts et al. [4] begins by applying a rank-preserving transformation to the data. This transformation maps the original observations onto the unit interval (0, 1), maintaining the order of the data points while capturing key distributional features like skewness and tail behavior. Then, an inverse normal transformation is applied, similar to rank-based approaches. This transformed distribution can be fine-tuned using the Tukey \\(g\\)-\\(h\\) distribution, whose quantiles are used to set the boxplot whiskers.\nThe Figures below illustrate a comparison of Tukey’s boxplot, the adjusted boxplot, and the generalized boxplot across three distinct types of data distributions: (i) a normal distribution, (ii) a right-skewed distribution, and (iii) a heavy-tailed distribution. The Figures show how each method performs and adapts to the specific characteristics of these distributions, effectively balancing sensitivity to outliers without over-identifying them.\nFor a symmetric distribution, all three boxplots produce similar results as expected.\n\n\n\n\n\n\n\n\n\nFor a right-skewed distribution, the Tukey’s boxplot overestimates the number of outliers, while the adjusted and generalized boxplots provide a better representation of the underlying data distribution. The generalized boxplot, in particular, offers the most flexible approach by accommodating skewness.\n\n\n\n\n\n\n\n\n\nFor a heavy-tailed distribution, the Tukey’s boxplot inaccurately identifies an excessive number of outliers due to its assumption of symmetry and limited adaptability to extreme tails. The adjusted boxplot improved upon Tukey’s boxplot by accounting for skewness in the data, yet it still struggles to effectively handle the variability and extreme values inherent in heavy-tailed distributions. In contrast, the generalized boxplot proves to be a more robust and versatile method. Its whiskers and overall box structure more accurately reflect the underlying data distribution, demonstrating its ability to adapt to both skewness and tail heaviness more effectively than the other two methods.\n\n\n\n\n\n\n\n\n\n\n\n\nBy comparing the three boxplot approaches, we show the limitations of Tukey’s boxplot when applied to skewed or heavy-tailed data. While Tukey’s method assumes symmetry and often flags excessive outliers, the adjusted boxplot improves this by incorporating a skewness measure (medcouple). The generalized boxplot provides the most robust solution by flexibly adapting to both skewness and tail heaviness through the Tukey \\(g\\)-\\(h\\) distribution. Overall, the adjusted and generalized boxplots are better suited for non-symmetric data, with the generalized boxplot offering the most robust approach.\n\n\n\n\nTukey, J.W., (1977). Exploratory Data Analysis, Pearson, 1st edition, page 39.\nHubert, M., Vandervieren, E., (2008). An adjusted boxplot for skewed distributions. Computational Statistics and Data Analysis, 52, 5186-5201.\nBrys, G., Hubert, M., Struyf, A., (2004). A robust measure of skewness. Journal of Computational and Graphical Statistics, 13, 996-1017.\nBruffaerts, C., Verardi, V., Vermandele, C., (2014). A generalized boxplot for skewed and heavy-tailed distributions. Statistics & Probability Letters, 95, 110–117.\nTukey, J.W., (1977). Modern techniques in data analysis. NSF‐sponsored regional research conference at Southeastern Massachusetts University, North Dartmouth, MA.\nMartinez, J., Iglewicz, B., (1984). Some properties of the Tukey g and h family of distributions. Communications in Statistics: Theory and Methods, 13, 353–369."
  },
  {
    "objectID": "blog/posts/post7/index.html#tukeys-boxplot",
    "href": "blog/posts/post7/index.html#tukeys-boxplot",
    "title": "Beyond Standard Boxplot: The Adjusted and Generalized Boxplots",
    "section": "",
    "text": "Proposed by Tukey in 1977 [1], a boxplot is constructed using five key summary statistics: the minimum, first quartile (Q1), median, third quartile (Q3), and maximum. These elements are complemented by whiskers, which typically extend to 1.5 times the interquartile range (IQR = Q3 - Q1) beyond Q1 and Q3, respectively. Data points outside this range are flagged as potential outliers.\n\\[\n\\begin{aligned}\n    \\text{Lower Fence} &= Q_1 - 1.5 \\cdot \\text{IQR} \\\\\n    \\text{Upper Fence} &= Q_3 + 1.5 \\cdot \\text{IQR}\n\\end{aligned}\n\\]\nThe ends of the whiskers are called adjacent values. They are the smallest and largest values not flagged outliers. Moreover, since IQR has a breakdown point of 0.25, it takes more than 25% of the data to be contaminated by outliers for the masking effect to occur. For example, in the Figure below, we show how the quartiles and therefore IQR are significantly influenced by the outliers, highlighting the breakdown point. After replacing 25% of the data with extreme outliers, the IQR increased drastically, from 13 (clean data) to 136 (contaminated data). This demonstrates that with 25% contamination, the IQR becomes unreliable as a measure of spread.\n\n\n\n\n\n\n\n\n\nBoxplots are a widely used tool for visualizing data distributions, particularly when the data exhibit a symmetrical mesokurtic distribution. However, its effectiveness diminishes when dealing with skewed data. In such cases, the boxplot can misrepresent the dataset’s nuances—most notably by classifying an excessive number of larger values as outliers when the distribution is skewed. Recognizing this limitation, Hubert and Vandervieren [2] introduced a refined version of Tukey’s boxplot rule. Their approach incorporates a robust measure of skewness, allowing for a more accurate depiction of asymmetrical distributions while preserving the simplicity and interpretability of the Tukey’s boxplot."
  },
  {
    "objectID": "blog/posts/post7/index.html#the-skew-adjusted-boxplot",
    "href": "blog/posts/post7/index.html#the-skew-adjusted-boxplot",
    "title": "An Overview of Skew-Adjusted and Generalized Boxplots",
    "section": "",
    "text": "To address the limitations of the standard boxplot in handling skewed data, Hubert and Vandervieren (2008) introduced the skew-adjusted boxplot. This method modifies the whisker length based on the medcouple, a robust skewness statistic. Specifically, the upper whisker extends further for right-skewed data, and the lower whisker extends further for left-skewed data. This adjustment ensures a more accurate representation of the data’s spread while retaining the robustness of the standard boxplot."
  },
  {
    "objectID": "blog/posts/post7/index.html#the-generalized-boxplot",
    "href": "blog/posts/post7/index.html#the-generalized-boxplot",
    "title": "Beyond Standard Boxplot: The Adjusted and Generalized Boxplots",
    "section": "",
    "text": "In 1977, Tukey introduced a family of distributions through two nonlinear transformations, giving rise to what is now known as the Tukey \\(g\\)-\\(h\\) distributions [5,6]. These distributions are widely used in robust statistics and flexible modeling due to their ability to accommodate a range of skewness and kurtosis levels. The Tukey \\(g\\)-\\(h\\) distribution is defined by applying two transformations, \\(g\\) (skewness) and \\(h\\) (kurtosis), to a standard normal random variable \\(Z \\sim \\mathcal{N}(0, 1)\\). The resulting transformation is expressed by:\n\\[\nT_{g,h}(Z) = \\frac{e^{gZ}-1}{g} \\cdot e^{h\\frac{Z^2}{2}} \\quad \\text{with} \\quad g \\neq 0, h \\in \\mathbb{R}\n\\]\nBy adjusting \\(g\\) and \\(h\\), the Tukey \\(g\\)-\\(h\\) distribution can model a wide variety of distribution, from symmetric to skewed, and from light-tailed to heavy-tailed distributions.\nThe constants \\(g\\) and \\(h\\), can be estimated from the empirical quantiles as follows:\n\\[\n\\hat{g} = \\frac{1}{z_p} \\text{ln}\\left(-\\frac{Q_p(\\{x_j\\})}{Q_{1-p}(\\{x_j\\})}\\right),\n\\quad\n\\hat{h} = \\frac{2}{z_p^2} \\text{ln}\\left(-\\hat{g}\\frac{Q_p(\\{x_j\\}) \\cdot Q_{1-p}(\\{x_j\\})}{Q_p(\\{x_j\\}) + Q_{1-p}(\\{x_j\\})}\\right)\n\\]where \\(z_p\\) is the quantile of order \\(p\\) of the standard normal distribution. \\(Q_p\\) and \\(Q_{1-p}\\) are the empirical quantiles of order \\(p\\) (\\(0.5&lt;p&lt;1\\)) and \\(1-p\\) of the univariate data \\(X = \\{x_j\\} = \\{x_1, \\cdots, x_n\\}\\).\nThe generalized boxplot as proposed by Bruffaerts et al. [4] begins by applying a rank-preserving transformation to the data. This transformation maps the original observations onto the unit interval (0, 1), maintaining the order of the data points while capturing key distributional features like skewness and tail behavior. Then, an inverse normal transformation is applied, similar to rank-based approaches. This transformed distribution can be fine-tuned using the Tukey \\(g\\)-\\(h\\) distribution, whose quantiles are used to set the boxplot whiskers.\nThe Figures below illustrate a comparison of Tukey’s boxplot, the adjusted boxplot, and the generalized boxplot across three distinct types of data distributions: (i) a normal distribution, (ii) a right-skewed distribution, and (iii) a heavy-tailed distribution. The Figures show how each method performs and adapts to the specific characteristics of these distributions, effectively balancing sensitivity to outliers without over-identifying them.\nFor a symmetric distribution, all three boxplots produce similar results as expected.\n\n\n\n\n\n\n\n\n\nFor a right-skewed distribution, the Tukey’s boxplot overestimates the number of outliers, while the adjusted and generalized boxplots provide a better representation of the underlying data distribution. The generalized boxplot, in particular, offers the most flexible approach by accommodating skewness.\n\n\n\n\n\n\n\n\n\nFor a heavy-tailed distribution, the Tukey’s boxplot inaccurately identifies an excessive number of outliers due to its assumption of symmetry and limited adaptability to extreme tails. The adjusted boxplot improved upon Tukey’s boxplot by accounting for skewness in the data, yet it still struggles to effectively handle the variability and extreme values inherent in heavy-tailed distributions. In contrast, the generalized boxplot proves to be a more robust and versatile method. Its whiskers and overall box structure more accurately reflect the underlying data distribution, demonstrating its ability to adapt to both skewness and tail heaviness more effectively than the other two methods."
  },
  {
    "objectID": "blog/posts/post7/index.html#the-adjusted-boxplot",
    "href": "blog/posts/post7/index.html#the-adjusted-boxplot",
    "title": "Beyond Standard Boxplot: The Adjusted and Generalized Boxplots",
    "section": "",
    "text": "In 2004, Brys et al. [3] introduced the medcouple (MC), which is a robust statistic used to measure the skewness of a distribution. Unlike traditional measures of skewness that are sensitive to outliers, MC focuses on the median and the IQR, making it less affected by extreme values. It is based on the difference between left and right data spreads relative to the median. MC is bounded between −1 and 1. A value of 0 indicates a perfectly symmetric distribution, while positive values signify a right-tailed (positively skewed) distribution, and negative values correspond to a left-tailed (negatively skewed) distribution. Importantly, this measure is most effective for moderately skewed distributions, particularly when the absolute value of MC is less than or equal to 0.6 (\\(|\\text{MC}| \\leq 0.6\\)). MC is given by:\n\\[\n\\text{MC} = \\text{med} \\left\\{ \\frac{(x_i - m) - (m - x_j)}{x_i - x_j} \\, \\middle| \\, x_i \\geq m \\geq x_j \\right\\}\n\\]\nwhere, \\(x_i\\) and \\(x_j\\) are data points from the sample, \\(m\\) is the median of the sample, and \\(\\text{med}\\{\\cdot\\}\\) denotes the median operator.\nTo address the limitation of the Tukey’s boxplot in handling skewed data, Hubert and Vandervieren [2] introduced the adjusted boxplot for skewed distribution. This method modifies the whisker length based on the MC. Specifically, the upper whisker extends further for right-skewed data, and the lower whisker extends further for left-skewed data. This new approch modifies the traditional boxplot fences to account for skewness in the data using the exponential function of MC as follows:\nWhen \\(\\text{MC} \\geq 0\\):\n\\[\n\\begin{aligned}\n    \\text{Lower Fence} &= Q_1 - k \\cdot e^{-4\\text{MC}} \\cdot \\text{IQR} \\\\\n    \\text{Upper Fence} &= Q_3 + k \\cdot e^{3\\text{MC}} \\cdot \\text{IQR}\n\\end{aligned}\n\\]\nWhen \\(\\text{MC} \\leq 0\\):\n\\[\n\\begin{aligned}\n    \\text{Lower Fence} &= Q_1 - k \\cdot e^{-3\\text{MC}} \\cdot \\text{IQR} \\\\\n    \\text{Upper Fence} &= Q_3 + k \\cdot e^{4\\text{MC}} \\cdot \\text{IQR}\n\\end{aligned}\n\\]\nwhere, \\(k\\) is the fence factor. Hubert and Vandervieren [2] have optimized the adjusted boxplot for \\(k = 1.5\\). In the next Figure, we compare Tukey’s boxplot (left) with the adjusted boxplot (right), applied to a skewed data. The adjusted boxplot shows fewer outliers for the skewed data, reflecting its ability to handle skewness better.\n\n\n\n\n\n\n\n\n\nThe adjusted boxplot has garnered attention for its ability to handle skewed data, even earning a mention on platforms like Wikipedia. However, it is not without its critiques.\nFor example, Bruffaerts et al. [4] pointed out that the whiskers in the adjusted boxplot are designed to achieve a fixed outlier detection rate of 0.7%. While this threshold may be suitable for some applications, the process becomes cumbersome if one wishes to adopt a different detection rate. It requires re-running the simulations to determine a new tuning constant or fence factor (\\(k\\)). Moreover, for datasets with heavy-tailed distributions, the adjusted boxplot may still misclassify observations, as the whisker adjustment relies solely on skewness and not on tail behavior. Another significant issue lies in its dependence on MC. While MC is effective for large samples, its estimation can be imprecise for small sample sizes.\nThe Figure below illustrates the behavior of MC estimates for small and large sample sizes using density plots. To achieve this, we generated multiple random samples from a skewed Gamma distribution and examined the variability of the MC estimates across these sample sizes. For small sample sizes, the MC estimates exhibit significant variability, reflecting its reduced robustness in such cases. In contrast, as the sample size increases, the MC estimates stabilize and converge to a consistent value, demonstrating its improved precision and reliability with larger data.\n\n\n\n\n\n\n\n\n\nIn response to these shortcomings, Bruffaerts et al. [4] proposed an alternative method to address skewness and heavy-tailed distributions. Their approach introduces a rank-preserving transformation that allows the data to conform to a Tukey \\(g\\)-\\(h\\) distribution."
  },
  {
    "objectID": "blog/posts/post6/index.html#voigt-profile",
    "href": "blog/posts/post6/index.html#voigt-profile",
    "title": "The Pseudo-Voigt Function",
    "section": "",
    "text": "Fundamentally, the pseudo-Voigt function is an approximation of the more precise Voigt profile (named after German physicist Woldemar Voigt). The Voigt profile (\\(V\\)) represents the exact convolution of Gaussian and Lorentzian profiles, providing a more accurate description of spectral line shapes, particularly when both Doppler and Lorentzian broadening are significant.\n\\[\nV(x;\\sigma,\\gamma) = \\int_{-\\infty}^{\\infty} G(x';\\sigma) L(x - x';\\gamma) \\, dx'\n\\]\nThe Voigt profile can also be expressed using the Faddeeva function \\(\\omega(z)\\), given by:\n\\[\nw(z) = e^{-z^2} \\left( 1 + \\frac{2i}{\\sqrt{\\pi}} \\int_0^z e^{t^2} \\, dt \\right)\n\\] where \\(z\\) is a complex number. Using the Faddeeva function, the Voigt profile is: \\[\nV(x; \\sigma, \\gamma) = \\frac{1}{\\sigma\\sqrt{2\\pi}}\\Re\\left[w\\left( \\frac{x + i\\gamma}{\\sigma\\sqrt{2}} \\right)\\right]\n\\]\nwhere \\(\\Re[w(z)]\\) denotes the real part of the Faddeeva function.\n\n\n\n\n\n\n\n\n\nHowever, the Voigt profile is computationally intensive, requiring efficient numerical methods to accurately evaluate the convolution integral. This complexity has led to the development of approximations, such as the pseudo-Voigt function, which strike a balance between computational efficiency and maintaining acceptable levels of accuracy in the spectral line shape."
  },
  {
    "objectID": "blog/posts/post6/index.html#pseudo-voigt-function",
    "href": "blog/posts/post6/index.html#pseudo-voigt-function",
    "title": "The Pseudo-Voigt Function",
    "section": "",
    "text": "The pseudo-Voigt function (\\(pV\\)) is a linear combination of a Gaussian (\\(G\\)) and a Lorentzian (\\(L\\)) function, and is defined as:\n\\[\npV(x;\\eta) = ηG(x;\\sigma) + (1-η)L(x;\\gamma)\n\\]\n\\[\nG(x;\\sigma) = \\frac{1}{\\sigma \\sqrt{2\\pi}} e^{-\\frac{x^2}{2\\sigma^2}}\n\\]\n\\[\nL(x; \\gamma) = \\frac{\\gamma}{\\pi (x^2 + \\gamma^2)}\n\\]\nwhere, \\(\\eta\\) is a mixing parameter that determines the relative contribution of the Lorentzian (\\(\\eta = 0\\)) and Gaussian (\\(\\eta = 1\\)) components. Here, the parameters \\(\\sigma\\) and \\(\\gamma\\) represent, respectively, the standard deviation of the Gaussian component, related to the full width at half maximum (FWHM) of the Gaussian by \\(w_G = 2\\sqrt{2\\ln 2} \\, \\sigma\\), and the half-width at half maximum (HWHM) of the Lorentzian component, related to the Lorentzian FWHM by \\(w_L = 2\\gamma\\).\n\n\n\n\n\n\n\n\n\nThe following animation illustrates how the pseudo-Voigt function varies with the mixing parameter \\(\\eta\\). As \\(\\eta\\) increases from 0 to 1, the contribution of the Gaussian component becomes more significant. Each case has a FWHM of \\(w_G = w_L = 1\\).\n\n\nShow the code\nlibrary(ggplot2)\nlibrary(gganimate)\n\ngaussian &lt;- function(x, x_c, w_G, A, y_0) {\n  y_0 + A / (w_G * sqrt(pi / (4 * log(2)))) * exp(-4 * log(2) * (x - x_c)^2 / w_G^2)\n}\nlorentzian &lt;- function(x, x_c, w_L, A, y_0) {\n  y_0 + A / pi * w_L / ((x - x_c)^2 + w_L^2)\n}\npseudo_voigt &lt;- function(x, x_c, w_L, w_G, A, y_0, eta) {\n  y_0 + A * (eta * gaussian(x, x_c, w_G, A, y_0)  + (1 - eta) * lorentzian(x, x_c, w_L, A, y_0))\n}\n\nx_c &lt;- y_0 &lt;- 0; w_L &lt;- w_G &lt;- A &lt;- 1\neta_values &lt;- seq(0, 1, by = 0.05)\ndf &lt;- data.frame(x = seq(-5, 5, by = 0.1))\nfor (eta in eta_values) {\n  df[[paste0(\"η = \", eta)]] &lt;- pseudo_voigt(df$x, x_c, w_L, w_G, A, y_0, eta)\n}\ndf_long &lt;- tidyr::pivot_longer(df, cols = -x, names_to = \"eta\", values_to = \"y\")\n\np &lt;- df_long |&gt; \n  ggplot(aes(x = x, y = y, colour = eta)) +\n  geom_line(linewidth = 1) +\n  scale_y_continuous(limits = c(0,1)) +\n  geom_point(size = 2) +\n  theme_bw(base_size = 15) +\n  labs(\n    x = \"x\", \n    y = \"y\", \n    title = \"The Pseudo-Voigt Function\",\n    subtitle = paste0(\"Varying η from Lorentzian (η = 0) to Gaussian (η = 1): \", \"{closest_state}\"),\n    caption = \"The mixing parameter η determines the relative contribution of the Lorentzian and Gaussian components.\") +\n  theme(\n    legend.position = \"none\",\n    plot.title = element_text(hjust = 0, size = 18, face = \"bold\"),\n    plot.subtitle = element_text(hjust = 0),\n    panel.background = element_rect(colour = \"black\", linewidth = 1)\n    )\n\nanim &lt;- p +\n  transition_states(eta, transition_length = 2, state_length = 1) +\n  ease_aes('exponential-in-out') +\n  shadow_mark(alpha = 2/10, size = 0.1)\n\nanimate(\n  anim, \n  nframes = 100, \n  fps = 10, \n  width = 600, \n  height = 400, \n  renderer = gifski_renderer()\n  )"
  },
  {
    "objectID": "blog/posts/post6/index.html#extended-pseudo-voigt-function",
    "href": "blog/posts/post6/index.html#extended-pseudo-voigt-function",
    "title": "The Pseudo-Voigt Function",
    "section": "",
    "text": "In the early 2000s, Ida et al. [2] refined the pseudo-Voigt function by introducing an extended formula designed to more accurately approximate the Voigt profile. This formula is given by:\n\\[\nepV(x;\\eta_L, \\eta_I, \\eta_P) = (1 - \\eta_L - \\eta_I - \\eta_P)G(x;\\sigma) + \\eta_L L(x; \\gamma) + \\eta_I F_I(x;\\gamma_I) + \\eta_P F_P(x;\\gamma_P)\n\\]\nwhere \\(F_I\\) and \\(F_P\\) are intermediate functions that represent the transition between the Lorentzian and Gaussian profiles, respectively. \\(F_I\\) is an irrational function involving a square root, while \\(F_P\\) is the squared hyperbolic secant function. These functions are defined as follows:\n\\[\nF_I(x;\\gamma_I) = \\frac{1}{2\\gamma_I}\\left( 1 + \\left(\\frac{x}{\\gamma_I}\\right)^2 \\right)^{-3/2}\n\\]\n\\[\nF_P(x;\\gamma_P) = \\frac{1}{2\\gamma_P}\\text{sech}^2\\left(\\frac{x}{\\gamma_P}\\right)\n\\]\nThe FWHMs are given by \\(w_I = 2 \\gamma_I \\sqrt{(2^{2/3} - 1)}\\), and \\(w_P = 2 \\gamma_P \\ln{(\\sqrt{2} + 1)}\\). A Gaussian profile is obtained when the mixing parameters are set to \\(\\eta_L = \\eta_I = \\eta_P = 0\\), while the Lorentzian contribution is governed by the parameter \\(\\eta_L\\). Specifically, a pure Lorentzian profile arises when \\(\\eta_L = 1\\). These parameters are constrained to the range from 0 to 1 and must satisfy the condition \\(\\eta_L + \\eta_I + \\eta_P = 1\\). This constraint defines a 2D simplex within a plane in 3D space, representing all possible combinations of the mixing parameters.\n\n\n\n\n\n\n\n\n\nThe animation below is a visual representation of the normalized extended pseudo-Voigt profiles, illustrating the transition from Gaussian to Lorentzian shapes as the mixing parameters \\(\\eta_L, \\eta_I\\), and \\(\\eta_P\\), are varied simultaneously."
  },
  {
    "objectID": "blog/posts/post7/index.html#references",
    "href": "blog/posts/post7/index.html#references",
    "title": "Beyond Standard Boxplot: The Adjusted and Generalized Boxplots",
    "section": "",
    "text": "Tukey, J.W., (1977). Exploratory Data Analysis, Pearson, 1st edition, page 39.\nHubert, M., Vandervieren, E., (2008). An adjusted boxplot for skewed distributions. Computational Statistics and Data Analysis, 52, 5186-5201.\nBrys, G., Hubert, M., Struyf, A., (2004). A robust measure of skewness. Journal of Computational and Graphical Statistics, 13, 996-1017.\nBruffaerts, C., Verardi, V., Vermandele, C., (2014). A generalized boxplot for skewed and heavy-tailed distributions. Statistics & Probability Letters, 95, 110–117.\nTukey, J.W., (1977). Modern techniques in data analysis. NSF‐sponsored regional research conference at Southeastern Massachusetts University, North Dartmouth, MA.\nMartinez, J., Iglewicz, B., (1984). Some properties of the Tukey g and h family of distributions. Communications in Statistics: Theory and Methods, 13, 353–369."
  },
  {
    "objectID": "blog/posts/post7/index.html#conclusions",
    "href": "blog/posts/post7/index.html#conclusions",
    "title": "Beyond Standard Boxplot: The Adjusted and Generalized Boxplots",
    "section": "",
    "text": "By comparing the three boxplot approaches, we show the limitations of Tukey’s boxplot when applied to skewed or heavy-tailed data. While Tukey’s method assumes symmetry and often flags excessive outliers, the adjusted boxplot improves this by incorporating a skewness measure (medcouple). The generalized boxplot provides the most robust solution by flexibly adapting to both skewness and tail heaviness through the Tukey \\(g\\)-\\(h\\) distribution. Overall, the adjusted and generalized boxplots are better suited for non-symmetric data, with the generalized boxplot offering the most robust approach."
  }
]